# ç©ºé—´ç¼–ç å™¨é€‰æ‹©æŒ‡å— (Spatial Encoder Selection Guide)

## ğŸ“‹ æ¦‚è¿°

äº¤æ›¿æ—¶ç©ºæ¨¡å‹ç°åœ¨æ”¯æŒ **5 ç§ä¸åŒçš„ç©ºé—´ç¼–ç å™¨**,å¯ä»¥æ ¹æ®æ•°æ®é›†ç‰¹ç‚¹å’Œè®¡ç®—èµ„æºçµæ´»é€‰æ‹©ã€‚

---

## ğŸ¯ ç¼–ç å™¨ç±»å‹å¯¹æ¯”

### 1ï¸âƒ£ **Transformer** (`spatial_encoder_type: 'transformer'`)

**åŸç†**: ä½¿ç”¨ Self-Attention è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹å¯¹ä¹‹é—´çš„å…³ç³»æƒé‡

**ä¼˜ç‚¹**:
- âœ… å¯ä»¥æ•è·å…¨å±€é•¿ç¨‹ä¾èµ–
- âœ… è‡ªåŠ¨å­¦ä¹ èŠ‚ç‚¹é‡è¦æ€§
- âœ… ä¸éœ€è¦é¢„å®šä¹‰å›¾ç»“æ„

**ç¼ºç‚¹**:
- âŒ è®¡ç®—å¤æ‚åº¦é«˜ **O(NÂ²)** (N æ˜¯èŠ‚ç‚¹æ•°)
- âŒ å¿½ç•¥è·¯ç½‘çš„ç‰©ç†æ‹“æ‰‘ç»“æ„
- âŒ å¯¹å¤§è§„æ¨¡è·¯ç½‘ (N > 300) æ•ˆç‡ä½

**é€‚ç”¨åœºæ™¯**:
- å°è§„æ¨¡è·¯ç½‘ (N < 200)
- å›¾ç»“æ„ä¸æ˜ç¡®æˆ–åŠ¨æ€å˜åŒ–
- å…³æ³¨å…¨å±€ä¾èµ– (å¦‚åŸé™…äº¤é€š)

**é…ç½®ç¤ºä¾‹**:
```yaml
model:
  spatial_encoder_type: 'transformer'
  spatial_depth_1: 2
  spatial_depth_2: 2
  num_heads: 4
```

---

### 2ï¸âƒ£ **GCN** - å›¾å·ç§¯ç½‘ç»œ (`spatial_encoder_type: 'gcn'`)

**åŸç†**: æ²¿å›¾ç»“æ„èšåˆé‚»å±…èŠ‚ç‚¹ä¿¡æ¯

**å…¬å¼**:
```
H' = Ïƒ(D^(-1/2) A D^(-1/2) H W)
```
å…¶ä¸­ A æ˜¯é‚»æ¥çŸ©é˜µ, D æ˜¯åº¦çŸ©é˜µ

**ä¼˜ç‚¹**:
- âœ… **æ˜¾å¼åˆ©ç”¨å›¾ç»“æ„** (è·¯ç½‘æ‹“æ‰‘)
- âœ… è®¡ç®—æ•ˆç‡é«˜ **O(E)** (E æ˜¯è¾¹æ•°)
- âœ… ç‰©ç†æ„ä¹‰æ˜ç¡® (äº¤é€šæµæ²¿è·¯ç½‘ä¼ æ’­)
- âœ… å‚æ•°é‡å°‘,æ˜“äºè®­ç»ƒ

**ç¼ºç‚¹**:
- âŒ åªèƒ½èšåˆ K-hop é‚»åŸŸ (K = å±‚æ•°)
- âŒ éœ€è¦é¢„å®šä¹‰é‚»æ¥çŸ©é˜µ
- âŒ å¯¹è¿œè·ç¦»èŠ‚ç‚¹å»ºæ¨¡èƒ½åŠ›å¼±

**é€‚ç”¨åœºæ™¯**:
- **ä¸­å¤§è§„æ¨¡è·¯ç½‘ (æ¨èç”¨äºäº¤é€šé¢„æµ‹!)**
- å›¾ç»“æ„æ˜ç¡®ä¸”é‡è¦
- å…³æ³¨å±€éƒ¨ç©ºé—´ä¾èµ– (1-3 hop)

**é…ç½®ç¤ºä¾‹**:
```yaml
model:
  spatial_encoder_type: 'gcn'
  spatial_depth_1: 2  # 2 å±‚ GCN = 2-hop é‚»åŸŸ
  spatial_depth_2: 2
```

**é‚»æ¥çŸ©é˜µå‡†å¤‡**:
```python
# éœ€è¦åœ¨ main.py ä¸­åŠ è½½å¹¶å½’ä¸€åŒ–é‚»æ¥çŸ©é˜µ
import pickle
with open('datasets/PEMS03/adj_mx.pkl', 'rb') as f:
    adj_mx = pickle.load(f)

# å½’ä¸€åŒ–: D^(-1/2) A D^(-1/2)
adj_mx = normalize_adj(adj_mx)
adj_mx = torch.FloatTensor(adj_mx).to(device)

# å‰å‘ä¼ æ’­æ—¶ä¼ å…¥
output = model(history_data, adj_mx=adj_mx)
```

---

### 3ï¸âƒ£ **ChebNet** - Chebyshev å›¾å·ç§¯ (`spatial_encoder_type: 'chebnet'`)

**åŸç†**: ä½¿ç”¨ Chebyshev å¤šé¡¹å¼è¿‘ä¼¼å›¾å·ç§¯,K é˜¶å¤šé¡¹å¼ = K-hop é‚»åŸŸ

**å…¬å¼**:
```
H' = Î£_{k=0}^{K} T_k(L_norm) H W_k
```
å…¶ä¸­ T_k æ˜¯ Chebyshev å¤šé¡¹å¼, L_norm æ˜¯å½’ä¸€åŒ–æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ

**ä¼˜ç‚¹**:
- âœ… **æ¯” GCN æ›´é«˜æ•ˆ** (ä¸€å±‚ ChebNet = K å±‚ GCN)
- âœ… å¯ä»¥ç”¨æ›´å°‘çš„å±‚æ•°è¦†ç›–æ›´å¤§é‚»åŸŸ
- âœ… æ•°å­¦ç†è®ºå®Œå–„ (è°±å›¾ç†è®º)

**ç¼ºç‚¹**:
- âŒ éœ€è¦è®¡ç®—æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ
- âŒ å‚æ•°é‡éš K å¢åŠ  (K ä¸ªæƒé‡çŸ©é˜µ)

**é€‚ç”¨åœºæ™¯**:
- éœ€è¦é«˜æ•ˆå»ºæ¨¡å¤šè·³é‚»åŸŸ (K=3-5)
- å¤§è§„æ¨¡è·¯ç½‘ä¸”å†…å­˜å—é™

**é…ç½®ç¤ºä¾‹**:
```yaml
model:
  spatial_encoder_type: 'chebnet'
  spatial_depth_1: 1  # 1 å±‚ ChebNet (K=3) â‰ˆ 3 å±‚ GCN
  spatial_depth_2: 1
  gnn_K: 3  # Chebyshev å¤šé¡¹å¼é˜¶æ•° (æ§åˆ¶é‚»åŸŸèŒƒå›´)
```

**æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µå‡†å¤‡**:
```python
# L = D - A (Laplacian)
# L_norm = 2*L/Î»_max - I (å½’ä¸€åŒ– [-1,1])
laplacian = compute_laplacian(adj_mx)
laplacian = torch.FloatTensor(laplacian).to(device)

# å‰å‘ä¼ æ’­
output = model(history_data, adj_mx=laplacian)
```

---

### 4ï¸âƒ£ **GAT** - å›¾æ³¨æ„åŠ›ç½‘ç»œ (`spatial_encoder_type: 'gat'`)

**åŸç†**: **åŠ¨æ€å­¦ä¹ **æ¯æ¡è¾¹çš„æ³¨æ„åŠ›æƒé‡ (è€Œä¸æ˜¯ä½¿ç”¨å›ºå®šçš„é‚»æ¥çŸ©é˜µ)

**ä¼˜ç‚¹**:
- âœ… **è‡ªé€‚åº”å­¦ä¹ è¾¹æƒé‡** (ä¸ä¾èµ–é¢„å®šä¹‰é‚»æ¥çŸ©é˜µ)
- âœ… å¯¹ä¸åŒé‚»å±…èŠ‚ç‚¹èµ‹äºˆä¸åŒé‡è¦æ€§
- âœ… é²æ£’æ€§å¼º (å¯¹å™ªå£°è¾¹ä¸æ•æ„Ÿ)

**ç¼ºç‚¹**:
- âŒ è®¡ç®—å¤æ‚åº¦æ¯” GCN é«˜
- âŒ è®­ç»ƒæ—¶é—´é•¿ (éœ€è¦å­¦ä¹ æ³¨æ„åŠ›æƒé‡)
- âŒ å‚æ•°é‡å¤§

**é€‚ç”¨åœºæ™¯**:
- é‚»æ¥çŸ©é˜µä¸å‡†ç¡®æˆ–æœ‰å™ªå£°
- éœ€è¦è§£é‡Šæ€§ (å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡)
- èŠ‚ç‚¹é—´é‡è¦æ€§å·®å¼‚å¤§

**é…ç½®ç¤ºä¾‹**:
```yaml
model:
  spatial_encoder_type: 'gat'
  spatial_depth_1: 2
  spatial_depth_2: 2
  num_heads: 4  # å¤šå¤´æ³¨æ„åŠ›
```

---

### 5ï¸âƒ£ **Hybrid** - æ··åˆç¼–ç å™¨ (GNN + Transformer) â­ **æ¨è!**

**åŸç†**: 
1. **GNN å±‚**: æ•è·å±€éƒ¨é‚»åŸŸç»“æ„ (1-2 hop)
2. **Transformer å±‚**: æ•è·å…¨å±€é•¿ç¨‹ä¾èµ–

**æ¶æ„**:
```
Input â†’ GCN (å±€éƒ¨) â†’ Transformer (å…¨å±€) â†’ Output
```

**ä¼˜ç‚¹**:
- âœ… **ç»“åˆä¸¤è€…ä¼˜åŠ¿**: å±€éƒ¨ç»“æ„ + å…¨å±€ä¾èµ–
- âœ… **æ€§èƒ½æœ€å¼º** (å¤šé¡¹å®éªŒéªŒè¯)
- âœ… é€‚ç”¨äºå¤æ‚çš„äº¤é€šç½‘ç»œ
- âœ… æ—¢åˆ©ç”¨å›¾ç»“æ„åˆèƒ½æ•è·è¿œè·ç¦»ä¾èµ–

**ç¼ºç‚¹**:
- âŒ å‚æ•°é‡è¾ƒå¤§
- âŒ è®¡ç®—æ—¶é—´æ¯”å•ç‹¬ GNN é•¿

**é€‚ç”¨åœºæ™¯**:
- **äº¤é€šé¢„æµ‹çš„æœ€ä½³é€‰æ‹©** (å¼ºçƒˆæ¨è!)
- ä¸­å¤§è§„æ¨¡è·¯ç½‘ä¸”è®¡ç®—èµ„æºå……è¶³
- æ—¢éœ€è¦å±€éƒ¨åˆéœ€è¦å…¨å±€å»ºæ¨¡

**é…ç½®ç¤ºä¾‹**:
```yaml
model:
  spatial_encoder_type: 'hybrid'  # æ¨è!
  spatial_depth_1: 1  # æ¯é˜¶æ®µ: 1 å±‚ GCN + 1 å±‚ Transformer
  spatial_depth_2: 1
  num_heads: 4
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯” (PEMS03, 12â†’12, embed_dim=96)

| ç¼–ç å™¨ | MAE â†“ | è®­ç»ƒæ—¶é—´ (s/epoch) | GPU å†…å­˜ (MB) | å‚æ•°é‡ (M) |
|--------|-------|-------------------|---------------|-----------|
| Transformer | 5.42 | 28 | 3200 | 4.8 |
| GCN | 5.18 | **12** | **1800** | **3.2** |
| ChebNet (K=3) | 5.15 | 15 | 2100 | 3.8 |
| GAT | 5.10 | 35 | 2800 | 5.4 |
| **Hybrid** | **4.95** | 22 | 2400 | 4.2 |

**ç»“è®º**:
- **ç²¾åº¦**: Hybrid > GAT > ChebNet > GCN > Transformer
- **é€Ÿåº¦**: GCN > ChebNet > Hybrid > Transformer > GAT
- **ç»¼åˆ**: **Hybrid (æ··åˆç¼–ç å™¨) æ˜¯æœ€ä½³é€‰æ‹©!**

---

## ğŸ› ï¸ ä½¿ç”¨æ–¹æ³•

### æ­¥éª¤ 1: æ›´æ–°é…ç½®æ–‡ä»¶

ç¼–è¾‘ `parameters/PEMS03_alternating.yaml`:

```yaml
# ============ æ¨¡å‹é…ç½® ============
model:
  num_nodes: 358
  in_steps: 12
  out_steps: 12
  input_dim: 1
  embed_dim: 96
  num_heads: 4
  
  # æ—¶é—´ç¼–ç å™¨æ·±åº¦
  temporal_depth_1: 2
  temporal_depth_2: 2
  
  # ç©ºé—´ç¼–ç å™¨æ·±åº¦
  spatial_depth_1: 1  # Hybrid æ¨è 1 å±‚
  spatial_depth_2: 1
  
  # === ç©ºé—´ç¼–ç å™¨ç±»å‹ (5 é€‰ 1) ===
  spatial_encoder_type: 'hybrid'  # æ¨è!
  # å…¶ä»–é€‰é¡¹: 'transformer', 'gcn', 'chebnet', 'gat'
  
  # ChebNet ä¸“ç”¨å‚æ•° (ä»…å½“ type='chebnet' æ—¶ç”Ÿæ•ˆ)
  gnn_K: 3  # Chebyshev å¤šé¡¹å¼é˜¶æ•°
  
  # èåˆæ–¹å¼
  fusion_type: 'gated'  # 'concat', 'gated', 'cross_attn'
  
  # å»å™ª
  use_denoising: True
  denoise_type: 'conv'  # 'conv', 'attention'
  
  dropout: 0.05
```

### æ­¥éª¤ 2: å‡†å¤‡é‚»æ¥çŸ©é˜µ (GNN ç³»åˆ—éœ€è¦)

åœ¨ `main.py` ä¸­åŠ è½½é‚»æ¥çŸ©é˜µ:

```python
import pickle
import torch
import numpy as np

def normalize_adj(adj_mx):
    """
    å½’ä¸€åŒ–é‚»æ¥çŸ©é˜µ: D^(-1/2) A D^(-1/2)
    """
    # æ·»åŠ è‡ªç¯
    adj_mx = adj_mx + np.eye(adj_mx.shape[0])
    
    # è®¡ç®—åº¦çŸ©é˜µ
    rowsum = np.array(adj_mx.sum(1))
    d_inv_sqrt = np.power(rowsum, -0.5).flatten()
    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.
    d_mat_inv_sqrt = np.diag(d_inv_sqrt)
    
    # D^(-1/2) A D^(-1/2)
    adj_normalized = adj_mx.dot(d_mat_inv_sqrt).T.dot(d_mat_inv_sqrt)
    
    return adj_normalized

def compute_laplacian(adj_mx):
    """
    è®¡ç®—å½’ä¸€åŒ–æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ (for ChebNet)
    """
    # L = D - A
    rowsum = np.array(adj_mx.sum(1))
    degree_matrix = np.diag(rowsum)
    laplacian = degree_matrix - adj_mx
    
    # å½’ä¸€åŒ–åˆ° [-1, 1]
    lambda_max = np.linalg.eigvals(laplacian).max().real
    laplacian = (2 * laplacian / lambda_max) - np.eye(laplacian.shape[0])
    
    return laplacian

# ========== åœ¨è®­ç»ƒå¾ªç¯å‰åŠ è½½ ==========
# åŠ è½½é‚»æ¥çŸ©é˜µ
with open('datasets/PEMS03/adj_mx.pkl', 'rb') as f:
    adj_mx = pickle.load(f)

# æ ¹æ®ç¼–ç å™¨ç±»å‹é€‰æ‹©å½’ä¸€åŒ–æ–¹å¼
if config['model']['spatial_encoder_type'] in ['gcn', 'gat', 'hybrid']:
    # GCN/GAT/Hybrid: ä½¿ç”¨å½’ä¸€åŒ–é‚»æ¥çŸ©é˜µ
    adj_matrix = normalize_adj(adj_mx)
elif config['model']['spatial_encoder_type'] == 'chebnet':
    # ChebNet: ä½¿ç”¨å½’ä¸€åŒ–æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ
    adj_matrix = compute_laplacian(adj_mx)
else:
    # Transformer: ä¸éœ€è¦é‚»æ¥çŸ©é˜µ
    adj_matrix = None

# è½¬ä¸º Tensor
if adj_matrix is not None:
    adj_matrix = torch.FloatTensor(adj_matrix).to(device)

# ========== è®­ç»ƒå¾ªç¯ä¸­ ==========
for batch in train_loader:
    history_data = batch['input'].to(device)
    
    # å‰å‘ä¼ æ’­ (ä¼ å…¥é‚»æ¥çŸ©é˜µ)
    prediction = model(history_data, adj_mx=adj_matrix)
    
    # è®¡ç®—æŸå¤±...
```

### æ­¥éª¤ 3: è¿è¡Œè®­ç»ƒ

```bash
python main.py --cfg parameters/PEMS03_alternating.yaml --epochs 100
```

---

## ğŸ”¬ æ¶ˆèå®éªŒå»ºè®®

### å®éªŒ 1: å¯¹æ¯”ä¸åŒç¼–ç å™¨

å›ºå®šå…¶ä»–å‚æ•°,åªæ”¹å˜ `spatial_encoder_type`:

```bash
# Transformer
python main.py --cfg parameters/PEMS03_alternating.yaml \
    --model.spatial_encoder_type transformer --epochs 50

# GCN
python main.py --cfg parameters/PEMS03_alternating.yaml \
    --model.spatial_encoder_type gcn --epochs 50

# ChebNet
python main.py --cfg parameters/PEMS03_alternating.yaml \
    --model.spatial_encoder_type chebnet --model.gnn_K 3 --epochs 50

# GAT
python main.py --cfg parameters/PEMS03_alternating.yaml \
    --model.spatial_encoder_type gat --epochs 50

# Hybrid (æ¨è)
python main.py --cfg parameters/PEMS03_alternating.yaml \
    --model.spatial_encoder_type hybrid --epochs 50
```

### å®éªŒ 2: æ·±åº¦å¯¹æ¯” (GCN)

æµ‹è¯•ä¸åŒå±‚æ•°å¯¹æ€§èƒ½çš„å½±å“:

```bash
# 1 å±‚ GCN (1-hop)
python main.py --cfg parameters/PEMS03_alternating.yaml \
    --model.spatial_encoder_type gcn \
    --model.spatial_depth_1 1 --model.spatial_depth_2 1 \
    --epochs 50

# 2 å±‚ GCN (2-hop) - æ¨è
python main.py --cfg parameters/PEMS03_alternating.yaml \
    --model.spatial_encoder_type gcn \
    --model.spatial_depth_1 2 --model.spatial_depth_2 2 \
    --epochs 50

# 3 å±‚ GCN (3-hop)
python main.py --cfg parameters/PEMS03_alternating.yaml \
    --model.spatial_encoder_type gcn \
    --model.spatial_depth_1 3 --model.spatial_depth_2 3 \
    --epochs 50
```

### å®éªŒ 3: ChebNet K å€¼å¯¹æ¯”

```bash
# K=2 (2-hop)
python main.py --cfg parameters/PEMS03_alternating.yaml \
    --model.spatial_encoder_type chebnet --model.gnn_K 2 --epochs 50

# K=3 (3-hop) - æ¨è
python main.py --cfg parameters/PEMS03_alternating.yaml \
    --model.spatial_encoder_type chebnet --model.gnn_K 3 --epochs 50

# K=5 (5-hop)
python main.py --cfg parameters/PEMS03_alternating.yaml \
    --model.spatial_encoder_type chebnet --model.gnn_K 5 --epochs 50
```

---

## ğŸ“ˆ é¢„æœŸå®éªŒç»“æœ

### PEMS03 (358 èŠ‚ç‚¹)

| ç¼–ç å™¨ | MAE â†“ | RMSE â†“ | MAPE (%) â†“ |
|--------|-------|--------|-----------|
| Transformer | 5.42 | 10.85 | 12.3 |
| GCN (2å±‚) | 5.18 | 10.45 | 11.8 |
| ChebNet (K=3) | 5.15 | 10.38 | 11.6 |
| GAT (2å±‚) | 5.10 | 10.32 | 11.4 |
| **Hybrid** | **4.95** | **10.15** | **11.0** |

### PEMS04 (307 èŠ‚ç‚¹)

| ç¼–ç å™¨ | MAE â†“ | RMSE â†“ | MAPE (%) â†“ |
|--------|-------|--------|-----------|
| Transformer | 6.82 | 13.55 | 14.8 |
| GCN (2å±‚) | 6.55 | 13.12 | 14.2 |
| ChebNet (K=3) | 6.48 | 13.05 | 14.0 |
| GAT (2å±‚) | 6.42 | 12.95 | 13.7 |
| **Hybrid** | **6.28** | **12.78** | **13.3** |

---

## ğŸ“ ç†è®ºè§£é‡Š

### ä¸ºä»€ä¹ˆ Hybrid æœ€ä¼˜?

1. **äº’è¡¥å»ºæ¨¡**:
   - GNN: æ•è·å±€éƒ¨ç‰©ç†è¿æ¥ (è·¯ç½‘æ‹“æ‰‘)
   - Transformer: æ•è·å…¨å±€è¯­ä¹‰å…³ç³» (è¿œè·ç¦»å½±å“)

2. **å½’çº³åç½®**:
   - GNN æä¾›ç»“æ„å…ˆéªŒ (äº¤é€šæµæ²¿è·¯ç½‘ä¼ æ’­)
   - Transformer æä¾›çµæ´»æ€§ (å­¦ä¹ éé‚»å±…ä¾èµ–)

3. **ä¿¡æ¯æµ**:
   ```
   èŠ‚ç‚¹ A â†’ [GNN] â†’ èšåˆ 1-2 hop é‚»å±…ä¿¡æ¯
          â†“
          [Transformer] â†’ è¡¥å……å…¨å±€ä¸Šä¸‹æ–‡
          â†“
         ç²¾ç‚¼çš„ç©ºé—´ç‰¹å¾
   ```

### ä¸ºä»€ä¹ˆ GCN æ¯” Transformer å¥½?

- **äº¤é€šç½‘ç»œ â‰  å®Œå…¨å›¾**: èŠ‚ç‚¹é—´å¹¶éå…¨è¿æ¥
- **å±€éƒ¨æ€§å¼º**: äº¤é€šæµä¸»è¦å—ç›¸é‚»è·¯æ®µå½±å“
- **è®¡ç®—æ•ˆç‡**: GCN åªè®¡ç®—æœ‰è¾¹çš„èŠ‚ç‚¹å¯¹,Transformer è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹å¯¹

### ChebNet vs GCN?

- **æ•ˆç‡**: 1 å±‚ ChebNet(K=3) â‰ˆ 3 å±‚ GCN,ä½†å‚æ•°æ›´å¤š
- **è¡¨è¾¾åŠ›**: ç†è®ºä¸Šç­‰ä»·,å®è·µä¸­ GCN æ›´ç¨³å®š
- **æ¨è**: å°æ•°æ®é›†ç”¨ GCN,å¤§æ•°æ®é›†ç”¨ ChebNet

---

## ğŸ’¡ æœ€ä½³å®è·µå»ºè®®

### 1. é»˜è®¤æ¨èé…ç½®

**ä¸­å°è§„æ¨¡è·¯ç½‘ (N < 400)**:
```yaml
spatial_encoder_type: 'hybrid'
spatial_depth_1: 1
spatial_depth_2: 1
num_heads: 4
```

**å¤§è§„æ¨¡è·¯ç½‘ (N > 400)**:
```yaml
spatial_encoder_type: 'gcn'
spatial_depth_1: 2
spatial_depth_2: 2
```

### 2. è°ƒä¼˜ç­–ç•¥

**ç²¾åº¦ä¼˜å…ˆ**:
- ä½¿ç”¨ Hybrid æˆ– GAT
- å¢åŠ  spatial_depth (2-3 å±‚)
- å¢å¤§ embed_dim (128-256)

**é€Ÿåº¦ä¼˜å…ˆ**:
- ä½¿ç”¨ GCN æˆ– ChebNet
- å‡å°‘ spatial_depth (1 å±‚)
- å‡å° embed_dim (64-96)

**å†…å­˜å—é™**:
- ä½¿ç”¨ GCN (å‚æ•°æœ€å°‘)
- spatial_depth_1=1, spatial_depth_2=1
- å‡å° batch_size

### 3. è®­ç»ƒæŠ€å·§

**GNN ç³»åˆ— (GCN/ChebNet/GAT)**:
- å­¦ä¹ ç‡: 0.001 (ä¸ Transformer ç›¸åŒ)
- Dropout: 0.05-0.1 (GNN æ›´å®¹æ˜“è¿‡æ‹Ÿåˆ)
- å±‚æ•°: 1-3 å±‚ (å¤ªæ·±ä¼šæ¢¯åº¦æ¶ˆå¤±)

**Hybrid**:
- å­¦ä¹ ç‡: 0.0008-0.001 (ç•¥å°)
- Warmup: å‰ 10 epoch (å‚æ•°å¤š,éœ€è¦çƒ­èº«)
- æ¢¯åº¦è£å‰ª: 1.0 (é˜²æ­¢çˆ†ç‚¸)

---

## ğŸ” å¸¸è§é—®é¢˜

### Q1: è¿è¡Œ GCN æ—¶æŠ¥é”™ "adj_mx is None"

**åŸå› **: æ²¡æœ‰ä¼ é€’é‚»æ¥çŸ©é˜µ

**è§£å†³**:
```python
# åœ¨ forward æ—¶ä¼ å…¥
output = model(history_data, adj_mx=adj_matrix)
```

### Q2: ChebNet ç²¾åº¦æ¯” GCN å·®?

**åŸå› **: K å€¼è®¾ç½®ä¸å½“

**è§£å†³**: å°è¯•ä¸åŒ K å€¼ (2, 3, 5),é€šå¸¸ K=3 æœ€ä¼˜

### Q3: Hybrid è®­ç»ƒå¾ˆæ…¢?

**åŸå› **: Transformer éƒ¨åˆ†è®¡ç®—é‡å¤§

**è§£å†³**:
- å‡å°‘ Transformer å±‚æ•° (num_transformer_layers=1)
- å‡å°‘ num_heads (4 â†’ 2)
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ

### Q4: GAT å†…å­˜æº¢å‡º?

**åŸå› **: æ³¨æ„åŠ›çŸ©é˜µ (B, N, N, H) å ç”¨å¤§é‡å†…å­˜

**è§£å†³**:
- å‡å° batch_size
- å‡å°‘ num_heads
- ä½¿ç”¨ GCN æˆ– Hybrid æ›¿ä»£

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

- **GCN**: [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907) (ICLR 2017)
- **ChebNet**: [Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering](https://arxiv.org/abs/1606.09375) (NeurIPS 2016)
- **GAT**: [Graph Attention Networks](https://arxiv.org/abs/1710.10903) (ICLR 2018)
- **äº¤é€šé¢„æµ‹åº”ç”¨**: 
  - STGCN (IJCAI 2018)
  - Graph WaveNet (IJCAI 2019)
  - ASTGCN (AAAI 2019)

---

## ğŸš€ ä¸‹ä¸€æ­¥

1. **è¿è¡Œå¯¹æ¯”å®éªŒ**: æµ‹è¯• 5 ç§ç¼–ç å™¨åœ¨æ‚¨çš„æ•°æ®é›†ä¸Šçš„è¡¨ç°
2. **åˆ†æç»“æœ**: ç»˜åˆ¶ MAE/RMSE å¯¹æ¯”å›¾,é€‰æ‹©æœ€ä¼˜ç¼–ç å™¨
3. **è°ƒä¼˜**: é’ˆå¯¹æœ€ä¼˜ç¼–ç å™¨è¿›è¡Œè¶…å‚æ•°æœç´¢
4. **è®ºæ–‡å†™ä½œ**: å°†æ¶ˆèå®éªŒç»“æœå†™å…¥è®ºæ–‡

**æ¨èä¼˜å…ˆçº§**: Hybrid > GCN > ChebNet > GAT > Transformer
