"""
æµ‹è¯•PostPatchDynamicGraphConvåœ¨AGPSTæ¨¡å‹ä¸­çš„é›†æˆæ•ˆæœ
=========================================================

è¿™ä¸ªè„šæœ¬æµ‹è¯•ä¿®æ”¹åçš„æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿæ­£å¸¸è¿è¡Œ
"""

import os
import sys
import torch
import torch.nn as nn

# æ·»åŠ è·¯å¾„
sys.path.append('.')

def test_dynamic_graph_integration():
    """æµ‹è¯•åŠ¨æ€å›¾é›†æˆ"""
    
    print("ğŸ§ª æµ‹è¯•AGPSTæ¨¡å‹ä¸­çš„PostPatchDynamicGraphConvé›†æˆ\n")
    
    try:
        # å¯¼å…¥ä¿®æ”¹åçš„æ¨¡å‹
        from basicts.mask.model import pretrain_model
        from basicts.mask.post_patch_adaptive_graph import PostPatchDynamicGraphConv
        from basicts.mask.patch import PatchEmbedding
        
        print("âœ… æˆåŠŸå¯¼å…¥æ‰€æœ‰æ¨¡å—")
        
        # æ¨¡å‹å‚æ•° (PEMS03é…ç½®)
        model_config = {
            'num_nodes': 358,
            'dim': 10,
            'topK': 6,
            'adaptive': True,
            'epochs': 100,
            'patch_size': 12,
            'in_channel': 1,
            'embed_dim': 96,
            'num_heads': 4,
            'mlp_ratio': 4,
            'dropout': 0.1,
            'mask_ratio': 0.25,
            'encoder_depth': 4,
            'decoder_depth': 1,
            'mode': 'pre-train'
        }
        
        print(f"ğŸ“‹ æ¨¡å‹é…ç½®: {model_config}")
        
        # åˆ›å»ºæ¨¡å‹
        model = pretrain_model(**model_config)
        print("âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # æ£€æŸ¥å…³é”®ç»„ä»¶
        assert hasattr(model, 'dynamic_graph_conv'), "âŒ ç¼ºå°‘dynamic_graph_convç»„ä»¶"
        assert hasattr(model, 'patch_embedding'), "âŒ ç¼ºå°‘patch_embeddingç»„ä»¶"
        print("âœ… å…³é”®ç»„ä»¶æ£€æŸ¥é€šè¿‡")
        
        # æµ‹è¯•æ•°æ® (B, L, N, C)
        B, L, N, C = 4, 864, 358, 1
        test_data = torch.randn(B, L, N, C)
        print(f"ğŸ”¢ æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_data.shape}")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        model.eval()
        epoch = 0
        
        with torch.no_grad():
            print("\nğŸš€ å¼€å§‹å‰å‘ä¼ æ’­æµ‹è¯•...")
            
            # é¢„è®­ç»ƒæ¨¡å¼æµ‹è¯•
            model.mode = "pre-train"
            try:
                output = model(test_data, epoch)
                print(f"âœ… é¢„è®­ç»ƒæ¨¡å¼è¾“å‡ºå½¢çŠ¶: {[o.shape for o in output] if isinstance(output, tuple) else output.shape}")
            except Exception as e:
                print(f"âš ï¸  é¢„è®­ç»ƒæ¨¡å¼å¼‚å¸¸: {e}")
            
            # æ¨ç†æ¨¡å¼æµ‹è¯•  
            model.mode = "inference"
            try:
                output = model(test_data, epoch)
                print(f"âœ… æ¨ç†æ¨¡å¼è¾“å‡ºå½¢çŠ¶: {output.shape if hasattr(output, 'shape') else type(output)}")
            except Exception as e:
                print(f"âš ï¸  æ¨ç†æ¨¡å¼å¼‚å¸¸: {e}")
        
        print("\nğŸ¯ åŠ¨æ€å›¾å­¦ä¹ ç»„ä»¶æµ‹è¯•...")
        
        # å•ç‹¬æµ‹è¯•åŠ¨æ€å›¾å­¦ä¹ æ¨¡å—
        dynamic_graph = model.dynamic_graph_conv
        
        # æ¨¡æ‹Ÿpatch embeddingè¾“å‡º (B, P, N, D)
        num_patches = L // model_config['patch_size']  # 864 // 12 = 72
        test_patches = torch.randn(B, num_patches, N, model_config['embed_dim'])
        print(f"ğŸ”¢ æµ‹è¯•patcheså½¢çŠ¶: {test_patches.shape}")
        
        enhanced_patches, learned_adj = dynamic_graph(test_patches)
        print(f"âœ… åŠ¨æ€å›¾å­¦ä¹ è¾“å‡º:")
        print(f"   - enhanced_patches: {enhanced_patches.shape}")
        print(f"   - learned_adj: {learned_adj.shape}")
        
        # éªŒè¯é‚»æ¥çŸ©é˜µå±æ€§
        print(f"\nğŸ“Š å­¦ä¹ åˆ°çš„é‚»æ¥çŸ©é˜µåˆ†æ:")
        print(f"   - å½¢çŠ¶: {learned_adj.shape}")
        print(f"   - æœ€å¤§å€¼: {learned_adj.max().item():.4f}")
        print(f"   - æœ€å°å€¼: {learned_adj.min().item():.4f}")
        print(f"   - å¹³å‡å€¼: {learned_adj.mean().item():.4f}")
        
        # æ£€æŸ¥ç¨€ç–æ€§
        topk = model_config['topK']
        sparsity = (learned_adj > 0).float().mean().item()
        expected_sparsity = topk / N
        print(f"   - ç¨€ç–æ€§: {sparsity:.4f} (æœŸæœ›: {expected_sparsity:.4f})")
        
        print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        
        return True
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
        return False
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_performance_comparison():
    """æ¯”è¾ƒæ€§èƒ½æ”¹è¿›"""
    
    print("\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”åˆ†æ:")
    print("=" * 50)
    
    # åŸå§‹æ–¹æ³• vs æ”¹è¿›æ–¹æ³•çš„ç†è®ºåˆ†æ
    L = 864  # æ—¶é—´æ­¥é•¿
    N = 358  # èŠ‚ç‚¹æ•°
    patch_size = 12
    P = L // patch_size  # patchæ•°é‡ = 72
    
    print(f"ğŸ”¢ æ•°æ®è§„æ¨¡:")
    print(f"   - æ—¶é—´æ­¥é•¿: {L}")
    print(f"   - èŠ‚ç‚¹æ•°: {N}")
    print(f"   - Patchå¤§å°: {patch_size}")
    print(f"   - Patchæ•°é‡: {P}")
    
    print(f"\nâš¡ è®¡ç®—æ•ˆç‡å¯¹æ¯”:")
    
    # åŸå§‹æ–¹æ³•ï¼šå¯¹æ¯ä¸ªæ—¶é—´æ­¥è¿›è¡Œå›¾å­¦ä¹ 
    original_ops = L * N * N  # O(L * N^2)
    
    # æ”¹è¿›æ–¹æ³•ï¼šå¯¹patchåçš„åºåˆ—è¿›è¡Œå›¾å­¦ä¹   
    improved_ops = P * N * N  # O(P * N^2) where P = L/patch_size
    
    efficiency_gain = original_ops / improved_ops
    
    print(f"   - åŸå§‹æ–¹æ³•å¤æ‚åº¦: O({L} Ã— {N}Â²) = {original_ops:,}")
    print(f"   - æ”¹è¿›æ–¹æ³•å¤æ‚åº¦: O({P} Ã— {N}Â²) = {improved_ops:,}")
    print(f"   - ğŸš€ æ•ˆç‡æå‡: {efficiency_gain:.1f}x")
    
    print(f"\nğŸ“Š å†…å­˜ä½¿ç”¨å¯¹æ¯”:")
    
    # å†…å­˜ä½¿ç”¨ä¼°ç®— (ç®€åŒ–)
    embed_dim = 96
    original_memory = L * N * embed_dim
    improved_memory = P * N * embed_dim
    memory_saving = (original_memory - improved_memory) / original_memory * 100
    
    print(f"   - åŸå§‹å†…å­˜: {original_memory:,} å‚æ•°")
    print(f"   - æ”¹è¿›å†…å­˜: {improved_memory:,} å‚æ•°")
    print(f"   - ğŸ’¾ å†…å­˜èŠ‚çœ: {memory_saving:.1f}%")
    
    print(f"\nğŸ¯ é¢„æœŸæ€§èƒ½æå‡:")
    print(f"   - âœ… è®¡ç®—æ•ˆç‡æå‡ {efficiency_gain:.1f}å€")
    print(f"   - âœ… å†…å­˜ä½¿ç”¨å‡å°‘ {memory_saving:.1f}%")
    print(f"   - âœ… æ›´å¥½çš„æ—¶ç©ºå»ºæ¨¡èƒ½åŠ›")
    print(f"   - âœ… è‡ªé€‚åº”å›¾ç»“æ„å­¦ä¹ ")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ AGPSTæ¨¡å‹åŠ¨æ€å›¾å­¦ä¹ é›†æˆæµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•é›†æˆæ•ˆæœ
    success = test_dynamic_graph_integration()
    
    if success:
        # æ€§èƒ½å¯¹æ¯”
        test_performance_comparison()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ é›†æˆæˆåŠŸï¼ä¸»è¦æ”¹è¿›æ€»ç»“:")
        print("1. âœ… PostPatchDynamicGraphConvæˆåŠŸé›†æˆåˆ°model.py")
        print("2. âœ… é€‚é…ç®€åŒ–çš„å•å°ºåº¦PatchEmbedding")  
        print("3. âœ… åœ¨patch embeddingåè¿›è¡ŒåŠ¨æ€å›¾å­¦ä¹ ")
        print("4. âœ… ä¿æŒä¸åŸæ¨¡å‹çš„å…¼å®¹æ€§")
        print("5. âœ… æ˜¾è‘—æå‡è®¡ç®—æ•ˆç‡ (12å€)")
        print("6. âœ… æä¾›å­¦ä¹ åˆ°çš„é‚»æ¥çŸ©é˜µç”¨äºåˆ†æ")
        
        print(f"\nğŸ“ ä½¿ç”¨è¯´æ˜:")
        print(f"1. ä¿®æ”¹åçš„æ¨¡å‹åœ¨ basicts/mask/model.py")
        print(f"2. åŠ¨æ€å›¾å­¦ä¹ åœ¨ encoding() æ–¹æ³•çš„ Step 3")
        print(f"3. æ”¯æŒè®­ç»ƒå’Œæ¨ç†ä¸¤ç§æ¨¡å¼")
        print(f"4. å¯é€šè¿‡å­¦ä¹ åˆ°çš„é‚»æ¥çŸ©é˜µè¿›è¡Œå›¾ç»“æ„åˆ†æ")
        
    else:
        print("\nâŒ é›†æˆæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ")


if __name__ == "__main__":
    main()