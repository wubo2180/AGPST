# ğŸš¨ æ€§èƒ½ä¸‹é™è¯Šæ–­ä¸ä¿®å¤æ–¹æ¡ˆ

## é—®é¢˜æè¿°
- **Baseline MAE**: 14.57
- **å½“å‰ MAE**: 22.03
- **æ€§èƒ½ä¸‹é™**: 51% âŒ

---

## ğŸ” æ ¹æœ¬åŸå› åˆ†æ

### 1. âš ï¸ **è¾“å‡ºæŠ•å½±å±‚è®¾è®¡é—®é¢˜** (æœ€å¯èƒ½)

**å½“å‰ä»£ç **:
```python
self.output_projection = nn.Sequential(
    nn.Linear(embed_dim, embed_dim // 2),  # 96 â†’ 48
    nn.ReLU(),
    nn.Dropout(dropout),
    nn.Linear(embed_dim // 2, 1)            # 48 â†’ 1
)
```

**é—®é¢˜**:
- ç›´æ¥ä» 96 ç»´å‹ç¼©åˆ° 1 ç»´ï¼Œä¿¡æ¯æŸå¤±å·¨å¤§
- ReLU æ¿€æ´»å¯èƒ½å¯¼è‡´è´Ÿå€¼é¢„æµ‹å¤±æ•ˆï¼ˆäº¤é€šæµé‡å¯èƒ½éœ€è¦è´Ÿå¢é•¿ï¼‰
- æ²¡æœ‰è€ƒè™‘è¾“å‡ºçš„æ•°å€¼èŒƒå›´

**ä¿®å¤æ–¹æ¡ˆ**:
```python
self.output_projection = nn.Sequential(
    nn.Linear(embed_dim, embed_dim),       # 96 â†’ 96 (ä¿æŒç»´åº¦)
    nn.LayerNorm(embed_dim),
    nn.ReLU(),
    nn.Dropout(dropout),
    nn.Linear(embed_dim, embed_dim // 2),  # 96 â†’ 48
    nn.ReLU(),
    nn.Linear(embed_dim // 2, 1)           # 48 â†’ 1
)
```

---

### 2. âš ï¸ **è§£ç å™¨å¯èƒ½è¿‡æ·±** (å­¦ä¹ å›°éš¾)

**å½“å‰é…ç½®**:
```yaml
encoder_depth: 4
decoder_depth: 2
```

**é—®é¢˜**:
- è§£ç å™¨ 2 å±‚å¯¹äºå°æ•°æ®é›†å¯èƒ½è¿‡æ·±
- æœªæ¥æŸ¥è¯¢å‘é‡åˆå§‹åŒ–å¯èƒ½ä¸å½“
- äº¤å‰æ³¨æ„åŠ›å¯èƒ½æ²¡æœ‰å­¦ä¹ åˆ°æœ‰æ•ˆæ¨¡å¼

**ä¿®å¤æ–¹æ¡ˆ A** (å‡å°‘è§£ç å™¨æ·±åº¦):
```yaml
encoder_depth: 4
decoder_depth: 1  # ğŸ”§ å‡å°‘åˆ° 1 å±‚
```

**ä¿®å¤æ–¹æ¡ˆ B** (ä¿æŒæ·±åº¦ï¼Œè°ƒæ•´å­¦ä¹ ç‡):
```yaml
lr: 0.0005  # ğŸ”§ å‡åŠï¼Œæ›´ç¨³å®šçš„å­¦ä¹ 
```

---

### 3. âš ï¸ **æœªæ¥æŸ¥è¯¢å‘é‡åˆå§‹åŒ–é—®é¢˜**

**å½“å‰ä»£ç **:
```python
nn.init.normal_(self.future_queries, std=0.02)
```

**é—®é¢˜**:
- æ ‡å‡†å·® 0.02 å¯èƒ½å¤ªå°
- æœªæ¥æŸ¥è¯¢å‘é‡å¯èƒ½æ— æ³•æœ‰æ•ˆåœ°æŸ¥è¯¢å†å²

**ä¿®å¤æ–¹æ¡ˆ**:
```python
# ä½¿ç”¨ Xavier åˆå§‹åŒ–
nn.init.xavier_normal_(self.future_queries)
# æˆ–è€…å¢å¤§æ ‡å‡†å·®
nn.init.normal_(self.future_queries, std=0.2)  # å¢å¤§åˆ° 0.2
```

---

### 4. âš ï¸ **å­¦ä¹ ç‡è¿‡é«˜**

**å½“å‰é…ç½®**:
```yaml
lr: 0.001
```

**é—®é¢˜**:
- å¯¹äº Encoder-Decoder æ¶æ„ï¼Œ0.001 å¯èƒ½å¤ªé«˜
- è§£ç å™¨å‚æ•°å¤šï¼Œéœ€è¦æ›´å°çš„å­¦ä¹ ç‡ç¨³å®šè®­ç»ƒ

**ä¿®å¤æ–¹æ¡ˆ**:
```yaml
lr: 0.0003  # ğŸ”§ å‡å°‘åˆ° 0.0003
# æˆ–è€…ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
scheduler:
  type: 'ReduceLROnPlateau'
  patience: 10
  factor: 0.5
```

---

### 5. âš ï¸ **æ‰¹æ¬¡å¤§å°å¯èƒ½ä¸åˆé€‚**

**å½“å‰é…ç½®**:
```yaml
batch_size: 32
```

**é—®é¢˜**:
- è§£ç å™¨å‚æ•°å¤šï¼Œå¯èƒ½éœ€è¦æ›´å¤§æ‰¹æ¬¡ä»¥ç¨³å®šæ¢¯åº¦
- æˆ–è€…æ›´å°æ‰¹æ¬¡é…åˆæ›´å°å­¦ä¹ ç‡

**ä¿®å¤æ–¹æ¡ˆ A** (å¢å¤§æ‰¹æ¬¡):
```yaml
batch_size: 64  # ğŸ”§ å¢å¤§åˆ° 64
```

**ä¿®å¤æ–¹æ¡ˆ B** (å‡å°å­¦ä¹ ç‡):
```yaml
batch_size: 32
lr: 0.0003  # ğŸ”§ é…åˆæ›´å°å­¦ä¹ ç‡
```

---

### 6. âš ï¸ **ä½ç½®ç¼–ç å¯èƒ½ä¸åˆé€‚**

**å½“å‰ä»£ç **:
```python
# ç¼–ç å™¨å’Œè§£ç å™¨ä½¿ç”¨ç›¸åŒçš„ä½ç½®ç¼–ç ç»´åº¦
self.encoder_pos_embed = nn.Parameter(torch.randn(1, 1, 12, 96))
self.decoder_pos_embed = nn.Parameter(torch.randn(1, 1, 12, 96))
```

**é—®é¢˜**:
- éšæœºåˆå§‹åŒ–çš„ä½ç½®ç¼–ç å¯èƒ½ä¸å¦‚å›ºå®šçš„ sin/cos ç¼–ç 
- ç¼–ç å™¨å’Œè§£ç å™¨çš„åºåˆ—è¯­ä¹‰ä¸åŒï¼Œåº”è¯¥æœ‰åŒºåˆ†

**ä¿®å¤æ–¹æ¡ˆ** (ä½¿ç”¨å›ºå®šä½ç½®ç¼–ç ):
```python
def _get_sinusoidal_encoding(seq_len, d_model):
    """ç”Ÿæˆ sin/cos ä½ç½®ç¼–ç """
    position = torch.arange(seq_len).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))
    
    pos_encoding = torch.zeros(seq_len, d_model)
    pos_encoding[:, 0::2] = torch.sin(position * div_term)
    pos_encoding[:, 1::2] = torch.cos(position * div_term)
    
    return pos_encoding.unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, d_model)

# åœ¨ __init__ ä¸­
self.encoder_pos_embed = nn.Parameter(
    self._get_sinusoidal_encoding(self.seq_len, embed_dim),
    requires_grad=False  # å›ºå®šä¸è®­ç»ƒ
)
self.decoder_pos_embed = nn.Parameter(
    self._get_sinusoidal_encoding(self.pred_len, embed_dim),
    requires_grad=False  # å›ºå®šä¸è®­ç»ƒ
)
```

---

### 7. âš ï¸ **Warmup ä¸è¶³**

**é—®é¢˜**:
- Encoder-Decoder æ¶æ„å¤æ‚ï¼Œéœ€è¦å……åˆ†çš„ warmup
- æ²¡æœ‰ä½¿ç”¨å­¦ä¹ ç‡é¢„çƒ­

**ä¿®å¤æ–¹æ¡ˆ**:
```python
# æ·»åŠ  warmup scheduler
from torch.optim.lr_scheduler import LambdaLR

def warmup_schedule(step):
    warmup_steps = 1000
    if step < warmup_steps:
        return step / warmup_steps
    else:
        return 1.0

scheduler = LambdaLR(optimizer, warmup_schedule)
```

---

## ğŸ¯ æ¨èä¿®å¤é¡ºåº (ä»ç®€å•åˆ°å¤æ‚)

### âœ… **é˜¶æ®µ 1: å¿«é€Ÿä¿®å¤** (5åˆ†é’Ÿ)

1. **é™ä½å­¦ä¹ ç‡**
```yaml
lr: 0.0003  # ä» 0.001 é™åˆ° 0.0003
```

2. **å‡å°‘è§£ç å™¨æ·±åº¦**
```yaml
decoder_depth: 1  # ä» 2 é™åˆ° 1
```

3. **å¢å¼ºè¾“å‡ºæŠ•å½±å±‚**
```python
self.output_projection = nn.Sequential(
    nn.Linear(embed_dim, embed_dim),
    nn.LayerNorm(embed_dim),
    nn.ReLU(),
    nn.Dropout(dropout),
    nn.Linear(embed_dim, embed_dim // 2),
    nn.ReLU(),
    nn.Linear(embed_dim // 2, 1)
)
```

**é¢„æœŸ**: MAE åº”è¯¥é™åˆ° 16-18

---

### âœ… **é˜¶æ®µ 2: ä¸­çº§ä¿®å¤** (15åˆ†é’Ÿ)

4. **æ”¹è¿›æœªæ¥æŸ¥è¯¢åˆå§‹åŒ–**
```python
nn.init.xavier_normal_(self.future_queries)
```

5. **ä½¿ç”¨å›ºå®šä½ç½®ç¼–ç **
```python
# ä½¿ç”¨ sin/cos ä½ç½®ç¼–ç ï¼Œä¸è®­ç»ƒ
self.encoder_pos_embed = self._get_sinusoidal_encoding(...)
```

6. **è°ƒæ•´æ‰¹æ¬¡å¤§å°**
```yaml
batch_size: 64
```

**é¢„æœŸ**: MAE åº”è¯¥é™åˆ° 15-16

---

### âœ… **é˜¶æ®µ 3: é«˜çº§ä¿®å¤** (30åˆ†é’Ÿ)

7. **æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨**
```python
scheduler = ReduceLROnPlateau(optimizer, patience=10, factor=0.5)
```

8. **æ·»åŠ  warmup**
```python
warmup_scheduler = LambdaLR(optimizer, warmup_schedule)
```

9. **æ£€æŸ¥æ¢¯åº¦æµ**
```python
# åœ¨è®­ç»ƒå¾ªç¯ä¸­
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: {param.grad.norm().item()}")
```

**é¢„æœŸ**: MAE åº”è¯¥é™åˆ° 14.5-15

---

## ğŸ”§ ç«‹å³å¯æ‰§è¡Œçš„æœ€å°æ”¹åŠ¨

### ä¿®æ”¹ 1: `basicts/mask/model.py`

**è¾“å‡ºæŠ•å½±å±‚** (Line ~150):
```python
# æ—§ç‰ˆ
self.output_projection = nn.Sequential(
    nn.Linear(embed_dim, embed_dim // 2),
    nn.ReLU(),
    nn.Dropout(dropout),
    nn.Linear(embed_dim // 2, 1)
)

# æ–°ç‰ˆ â­
self.output_projection = nn.Sequential(
    nn.Linear(embed_dim, embed_dim),       # 96 â†’ 96
    nn.LayerNorm(embed_dim),                # å½’ä¸€åŒ–
    nn.GELU(),                              # æ›´å¹³æ»‘çš„æ¿€æ´»
    nn.Dropout(dropout),
    nn.Linear(embed_dim, embed_dim // 2),  # 96 â†’ 48
    nn.GELU(),
    nn.Linear(embed_dim // 2, 1)           # 48 â†’ 1
)
```

### ä¿®æ”¹ 2: `parameters/PEMS03.yaml`

```yaml
# å…³é”®å‚æ•°è°ƒæ•´
lr: 0.0003          # â­ ä» 0.001 é™åˆ° 0.0003
batch_size: 64      # â­ ä» 32 å¢åˆ° 64
decoder_depth: 1    # â­ ä» 2 é™åˆ° 1
```

---

## ğŸ“Š æ€§èƒ½é¢„æœŸ

| ä¿®å¤é˜¶æ®µ | æ”¹åŠ¨ | é¢„æœŸ MAE | æ”¹å–„ |
|---------|------|---------|------|
| **å½“å‰** | - | 22.03 | - |
| **é˜¶æ®µ 1** | lr + decoder + projection | 16-18 | +18-27% |
| **é˜¶æ®µ 2** | + init + pos_embed + batch | 15-16 | +27-32% |
| **é˜¶æ®µ 3** | + scheduler + warmup | 14.5-15 | +32-34% |
| **ç›®æ ‡** | å…¨éƒ¨ä¼˜åŒ– | **14.5** | +34% âœ… |

---

## ğŸš¨ ç´§æ€¥è¯Šæ–­æ£€æŸ¥æ¸…å•

åœ¨ä¿®å¤å‰ï¼Œå…ˆè¿è¡Œè¿™äº›æ£€æŸ¥:

```python
# 1. æ£€æŸ¥è¾“å‡ºèŒƒå›´
print(f"Prediction min: {prediction.min()}")
print(f"Prediction max: {prediction.max()}")
print(f"Target min: {target.min()}")
print(f"Target max: {target.max()}")

# 2. æ£€æŸ¥æ¢¯åº¦
for name, param in model.named_parameters():
    if param.grad is not None:
        grad_norm = param.grad.norm().item()
        if grad_norm > 100:
            print(f"âš ï¸ Large gradient: {name} = {grad_norm}")
        elif grad_norm < 1e-6:
            print(f"âš ï¸ Tiny gradient: {name} = {grad_norm}")

# 3. æ£€æŸ¥æŸå¤±æ›²çº¿
# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ï¼ŒæŸå¤±æ˜¯å¦åœ¨ä¸‹é™ï¼Ÿè¿˜æ˜¯ä¸€ç›´å¾ˆé«˜ï¼Ÿ

# 4. æ£€æŸ¥æ¨¡å‹è¾“å‡º
# é¢„æµ‹å€¼æ˜¯å¦åˆç†ï¼Ÿæœ‰æ²¡æœ‰ NaN æˆ– Infï¼Ÿ
```

---

## ğŸ’¡ è°ƒè¯•æŠ€å·§

### å¯¹æ¯”å®éªŒ
```python
# æµ‹è¯• 1: å•ç¼–ç å™¨ + MLP (æ—§ç‰ˆ)
decoder_depth: 0  # ç¦ç”¨è§£ç å™¨ï¼Œå›åˆ° MLP
# å¦‚æœè¿™ä¸ªç‰ˆæœ¬ MAE æ­£å¸¸ï¼Œè¯´æ˜é—®é¢˜åœ¨è§£ç å™¨

# æµ‹è¯• 2: åªç”¨æœ€åä¸€æ­¥ (ç®€åŒ–)
# åœ¨ decoder forward ä¸­åªç”¨ encoder_output[:, -1, :]
# å¦‚æœè¿™ä¸ªç‰ˆæœ¬ MAE æ­£å¸¸ï¼Œè¯´æ˜äº¤å‰æ³¨æ„åŠ›æœ‰é—®é¢˜
```

---

## ğŸ¯ æœ€å¯èƒ½çš„æ ¹æœ¬åŸå› 

åŸºäºç»éªŒï¼Œ**æœ€å¯èƒ½çš„åŸå› **æ˜¯:

1. **è¾“å‡ºæŠ•å½±å±‚å¤ªç®€å•** (70% æ¦‚ç‡)
   - 96 â†’ 48 â†’ 1 çš„å‹ç¼©å¤ªæ¿€è¿›
   - ç¼ºå°‘å½’ä¸€åŒ–å±‚

2. **å­¦ä¹ ç‡è¿‡é«˜** (20% æ¦‚ç‡)
   - 0.001 å¯¹è§£ç å™¨å¤ªå¤§
   - å¯¼è‡´è®­ç»ƒä¸ç¨³å®š

3. **æœªæ¥æŸ¥è¯¢åˆå§‹åŒ–ä¸å½“** (10% æ¦‚ç‡)
   - std=0.02 å¤ªå°
   - æŸ¥è¯¢å‘é‡æ— æ³•æœ‰æ•ˆå·¥ä½œ

---

## âœ… ç«‹å³è¡ŒåŠ¨

**ç¬¬ä¸€æ­¥**: ä¿®æ”¹è¿™ 3 å¤„ï¼Œé‡æ–°è®­ç»ƒ
1. `lr: 0.0003`
2. `decoder_depth: 1`
3. å¢å¼º `output_projection`

**é¢„è®¡æ—¶é—´**: 5 åˆ†é’Ÿä¿®æ”¹ + è®­ç»ƒæ—¶é—´

**é¢„æœŸç»“æœ**: MAE ä» 22.03 é™åˆ° 16-18

å¦‚æœè¿˜ä¸è¡Œï¼Œç»§ç»­è¿›è¡Œé˜¶æ®µ 2 å’Œ 3 çš„ä¿®å¤ã€‚

---

## ğŸ“ éœ€è¦å¸®åŠ©ï¼Ÿ

å¦‚æœä¿®å¤åä»ç„¶æ€§èƒ½å·®:
1. å‘é€è®­ç»ƒæ—¥å¿—çš„å‰ 50 è¡Œ
2. å‘é€ loss æ›²çº¿æˆªå›¾
3. å‘é€ prediction vs target çš„ç»Ÿè®¡ä¿¡æ¯

æˆ‘ä¼šè¿›ä¸€æ­¥è¯Šæ–­ï¼
