# ğŸ”— é«˜çº§è‡ªé€‚åº”å›¾å­¦ä¹ æ¨¡å—é›†æˆæŒ‡å—

## ğŸ“‹ æ¦‚è¿°

AGPST æ¨¡å‹ç°åœ¨é›†æˆäº†**é«˜çº§å¤šå°ºåº¦è‡ªé€‚åº”å›¾å­¦ä¹ æ¨¡å—** (`AdaptiveGraphLearner`)ï¼Œæä¾›æ¯”ç®€å•å›¾å­¦ä¹ æ›´å¼ºå¤§çš„å›¾ç»“æ„å­¦ä¹ èƒ½åŠ›ã€‚

---

## ğŸ†š ä¸¤ç§å›¾å­¦ä¹ æ¨¡å¼å¯¹æ¯”

### æ¨¡å¼1: ç®€å•å›¾å­¦ä¹  (Simple Graph Learning)

**ç‰¹ç‚¹**ï¼š
- åŸºäºé™æ€èŠ‚ç‚¹åµŒå…¥
- å•ä¸€å°ºåº¦å›¾ç»“æ„
- è½»é‡çº§ï¼Œè®¡ç®—å¿«é€Ÿ

**æ¶æ„**ï¼š
```python
adj = MM(node_embed1, node_embed2)  # (N, N)
adj = ReLU(adj)
adj = TopK(adj, k)
adj = Normalize(adj)
```

**é€‚ç”¨åœºæ™¯**ï¼š
- å¿«é€ŸåŸå‹éªŒè¯
- è®¡ç®—èµ„æºå—é™
- å›¾ç»“æ„ç›¸å¯¹é™æ€

---

### æ¨¡å¼2: é«˜çº§å›¾å­¦ä¹  (Advanced Graph Learning) âœ¨ **æ¨è**

**ç‰¹ç‚¹**ï¼š
- å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
- åŠ¨æ€+é™æ€å›¾èåˆ
- å¤šå°ºåº¦å›¾å­¦ä¹ ï¼ˆå±€éƒ¨+å…¨å±€ï¼‰
- åŸºäºæ—¶åºä¿¡æ¯çš„åŠ¨æ€å›¾
- InfoNCEå¯¹æ¯”å­¦ä¹ 

**æ¶æ„**ï¼š
```python
# é™æ€å›¾ï¼ˆå¤šå¤´ï¼‰
static_graphs = [
    LocalGraph_1, LocalGraph_2, ...,  # æ•æ‰å±€éƒ¨ç»“æ„
    GlobalGraph_1, GlobalGraph_2, ... # æ•æ‰å…¨å±€æ¨¡å¼
]

# åŠ¨æ€å›¾ï¼ˆåŸºäºæ—¶åºç‰¹å¾ï¼‰
temporal_features = TemporalAttention(patches)
dynamic_embeds = DynamicEncoder(temporal_features)
dynamic_graphs = Learn(dynamic_embeds)

# èåˆ
final_graph = Fusion(static_graphs, dynamic_graphs)

# å¯¹æ¯”å­¦ä¹ 
contrastive_loss = InfoNCE(node_embeddings)
```

**é€‚ç”¨åœºæ™¯**ï¼š
- è¿½æ±‚æœ€ä½³æ€§èƒ½
- å¤æ‚æ—¶ç©ºä¾èµ–
- å›¾ç»“æ„åŠ¨æ€å˜åŒ–
- æœ‰å……è¶³è®¡ç®—èµ„æº

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ–¹å¼1: å¯ç”¨é«˜çº§å›¾å­¦ä¹ ï¼ˆæ¨èï¼‰

ç¼–è¾‘ `parameters/PEMS03_v3.yaml`:

```yaml
# Adaptive graph learning
use_advanced_graph: True   # å¯ç”¨é«˜çº§å›¾å­¦ä¹ 
graph_heads: 4             # å¤šå¤´æ³¨æ„åŠ›æ•°é‡
dim: 10                    # èŠ‚ç‚¹åµŒå…¥ç»´åº¦
topK: 10                   # Top-Kç¨€ç–åŒ–
```

### æ–¹å¼2: ä½¿ç”¨ç®€å•å›¾å­¦ä¹ ï¼ˆå¿«é€Ÿï¼‰

```yaml
# Adaptive graph learning
use_advanced_graph: False  # ä½¿ç”¨ç®€å•å›¾å­¦ä¹ 
dim: 10
topK: 10
```

---

## ğŸ“Š å‚æ•°è¯¦è§£

### æ ¸å¿ƒå‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `use_advanced_graph` | bool | `True` | æ˜¯å¦ä½¿ç”¨é«˜çº§å›¾å­¦ä¹  |
| `graph_heads` | int | `4` | å›¾æ³¨æ„åŠ›å¤´æ•°ï¼ˆå»ºè®®2-8ï¼‰ |
| `dim` | int | `10` | èŠ‚ç‚¹åµŒå…¥ç»´åº¦ |
| `topK` | int | `10` | Top-Ké‚»å±…æ•°é‡ |

### é«˜çº§å‚æ•°ï¼ˆåœ¨ graph_learning.py ä¸­ï¼‰

```python
AdaptiveGraphLearner(
    num_nodes=358,           # èŠ‚ç‚¹æ•°é‡
    node_dim=10,             # èŠ‚ç‚¹åµŒå…¥ç»´åº¦
    embed_dim=96,            # ç‰¹å¾ç»´åº¦
    graph_heads=4,           # å¤šå¤´æ•°é‡
    topk=10,                 # Top-Kç¨€ç–åŒ–
    dropout=0.1,             # Dropoutç‡
    use_temporal_info=True   # æ˜¯å¦ä½¿ç”¨æ—¶åºä¿¡æ¯
)
```

---

## ğŸ”¬ æŠ€æœ¯ç»†èŠ‚

### å¤šå°ºåº¦å›¾å­¦ä¹ 

**å±€éƒ¨å›¾** (Local Graphs):
- æ•°é‡: `graph_heads // 2`
- ç»´åº¦: `node_dim // 2`
- æ¸©åº¦: `temperature * 2`
- ç›®æ ‡: æ•æ‰è¿‘é‚»å…³ç³»

**å…¨å±€å›¾** (Global Graphs):
- æ•°é‡: `graph_heads - local_heads`
- ç»´åº¦: `node_dim`
- æ¸©åº¦: `temperature * 0.5`
- ç›®æ ‡: æ•æ‰é•¿ç¨‹ä¾èµ–

### åŠ¨æ€å›¾å­¦ä¹ æµç¨‹

```
è¾“å…¥: patch_features (B, N, P, D)
  â†“
æ—¶åºæ³¨æ„åŠ›èšåˆ
  â†“
åŠ¨æ€èŠ‚ç‚¹åµŒå…¥ç¼–ç 
  â†“
GNNå¢å¼º (2å±‚)
  â†“
åŠ¨æ€ç›¸ä¼¼åº¦è®¡ç®—
  â†“
åŠ¨æ€å›¾ (B, H, N, N)
```

### å›¾èåˆç­–ç•¥

```python
# è‡ªé€‚åº”èåˆæƒé‡
fusion_weights = Sigmoid(Linear(node_features))

# èåˆ
fused_graph = (1 - Î±) * static_graph + Î± * dynamic_graph

# å¤šå¤´èšåˆ
final_graph = EdgeEncoder(MultiHeadGraphs)
```

### å¯¹æ¯”å­¦ä¹ 

**InfoNCE Loss**:
```python
# èŠ‚ç‚¹åµŒå…¥æŠ•å½±
z = Projection(node_embeddings)  # (B, N, D')
z = Normalize(z)

# ç›¸ä¼¼åº¦çŸ©é˜µ
sim = MM(z, z^T) / temperature

# å¯¹æ¯”æŸå¤±
loss = -log(exp(sim_pos) / sum(exp(sim_all)))
```

---

## ğŸ’¡ æ€§èƒ½å¯¹æ¯”

### è®¡ç®—å¼€é”€

| æ¨¡å¼ | å‚æ•°é‡ | å‰å‘æ—¶é—´ | GPUå†…å­˜ |
|------|--------|----------|---------|
| Simple | ~7K | 1.0x | 1.0x |
| Advanced | ~50K | 1.5-2.0x | 1.3-1.5x |

### é¢„æµ‹ç²¾åº¦ï¼ˆé¢„æœŸæå‡ï¼‰

| æ•°æ®é›† | Simple MAE | Advanced MAE | æå‡ |
|--------|------------|--------------|------|
| PEMS03 | X.XX | X.XX - 0.5 | ~5-10% |
| PEMS04 | X.XX | X.XX - 0.3 | ~3-8% |
| PEMS07 | X.XX | X.XX - 0.4 | ~4-9% |
| PEMS08 | X.XX | X.XX - 0.6 | ~6-12% |

*æ³¨: å®é™…æ•ˆæœéœ€è¦å®éªŒéªŒè¯*

---

## ğŸ§ª å®éªŒå»ºè®®

### å¯¹æ¯”å®éªŒ

**å®éªŒ1: å›¾å­¦ä¹ æ¨¡å¼å¯¹æ¯”**
```yaml
# Baseline
use_advanced_graph: False

# Advanced
use_advanced_graph: True
graph_heads: 4
```

**å®éªŒ2: å›¾å¤´æ•°æ¶ˆè**
```yaml
use_advanced_graph: True
graph_heads: [2, 4, 6, 8]  # åˆ†åˆ«æµ‹è¯•
```

**å®éªŒ3: Top-K æ•æ„Ÿæ€§**
```yaml
use_advanced_graph: True
topK: [5, 10, 15, 20]  # åˆ†åˆ«æµ‹è¯•
```

### å¯è§†åŒ–å»ºè®®

1. **å­¦ä¹ åˆ°çš„å›¾ç»“æ„**
   ```python
   # åœ¨ forward ä¸­ä¿å­˜
   learned_adjs, _ = self.graph_learner(patch_features)
   torch.save(learned_adjs, 'learned_graphs.pt')
   
   # å¯è§†åŒ–
   import networkx as nx
   import matplotlib.pyplot as plt
   adj = learned_adjs[0].detach().cpu().numpy()
   G = nx.from_numpy_array(adj)
   nx.draw(G)
   ```

2. **å¯¹æ¯”å­¦ä¹ æ•ˆæœ**
   ```python
   # è®°å½•å¯¹æ¯”æŸå¤±
   if self.contrastive_loss is not None:
       swanlab.log({"train/contrastive_loss": self.contrastive_loss})
   ```

---

## ğŸ”§ è°ƒè¯•ä¸ä¼˜åŒ–

### å¸¸è§é—®é¢˜

**Q1: æ˜¾å­˜ä¸è¶³ï¼Ÿ**
```yaml
# å‡å°‘å›¾å¤´æ•°
graph_heads: 2

# æˆ–ä½¿ç”¨ç®€å•æ¨¡å¼
use_advanced_graph: False
```

**Q2: è®­ç»ƒè¿‡æ…¢ï¼Ÿ**
```yaml
# å…³é—­æ—¶åºä¿¡æ¯ï¼ˆåœ¨ä»£ç ä¸­ï¼‰
use_temporal_info: False

# æˆ–å‡å°‘ topK
topK: 5
```

**Q3: å¯¹æ¯”æŸå¤±ä¸ºNaNï¼Ÿ**
- æ£€æŸ¥æ¸©åº¦å‚æ•°æ˜¯å¦åˆç†
- å¢åŠ æ•°å€¼ç¨³å®šæ€§å¤„ç†ï¼ˆå·²åœ¨ä»£ç ä¸­å®ç°ï¼‰

### æ€§èƒ½ä¼˜åŒ–

**æŠ€å·§1: æ¢¯åº¦ç´¯ç§¯**
```python
# åœ¨ main.py ä¸­
accumulation_steps = 2
for i, batch in enumerate(dataloader):
    loss = loss / accumulation_steps
    loss.backward()
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

**æŠ€å·§2: æ··åˆç²¾åº¦è®­ç»ƒ**
```python
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

with autocast():
    output = model(input)
    loss = criterion(output, target)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

---

## ğŸ“ˆ è¿›é˜¶ä½¿ç”¨

### è‡ªå®šä¹‰å›¾å¤´é…ç½®

ä¿®æ”¹ `basicts/mask/graph_learning.py`:

```python
# è°ƒæ•´å±€éƒ¨/å…¨å±€å›¾æ¯”ä¾‹
self.local_graph_heads = graph_heads // 3  # 1/3å±€éƒ¨
self.global_graph_heads = graph_heads - self.local_graph_heads  # 2/3å…¨å±€
```

### è‡ªå®šä¹‰èåˆç­–ç•¥

```python
# åœ¨ AdaptiveGraphLearner.forward ä¸­
# æ”¹ä¸ºå›ºå®šæƒé‡èåˆ
alpha = 0.5  # 50% static, 50% dynamic
fused_adjs = (1 - alpha) * static_expanded + alpha * dynamic_adjs
```

### æ·»åŠ é¢å¤–çš„å›¾æ­£åˆ™åŒ–

```python
# åœ¨æ¨¡å‹è®­ç»ƒå¾ªç¯ä¸­
graph_reg = torch.norm(learned_adjs, p='fro') * 0.001
total_loss = prediction_loss + contrastive_loss + graph_reg
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [å»å™ªæ¨¡å—æŒ‡å—](./DENOISING_MODULE.md)
- [å¿«é€Ÿå¼€å§‹](./ADAPTIVE_GRAPH_QUICKSTART.md)
- [å®Œæ•´æ•™ç¨‹](./ADAPTIVE_GRAPH_GUIDE.md)

---

## ğŸ¯ å¿«é€Ÿå¼€å§‹ç¤ºä¾‹

```bash
# 1. æµ‹è¯•ç®€å•å›¾å­¦ä¹ 
python main.py --config=parameters/PEMS03_v3.yaml --test_mode=1 \
    --device=cuda

# 2. æµ‹è¯•é«˜çº§å›¾å­¦ä¹ ï¼ˆéœ€è¦å…ˆåœ¨é…ç½®ä¸­è®¾ç½® use_advanced_graph: Trueï¼‰
python main.py --config=parameters/PEMS03_v3.yaml --test_mode=1 \
    --device=cuda

# 3. å®Œæ•´è®­ç»ƒ
python main.py --config=parameters/PEMS03_v3.yaml --device=cuda
```

---

## âœ… æ£€æŸ¥æ¸…å•

åœ¨ä½¿ç”¨é«˜çº§å›¾å­¦ä¹ å‰ï¼Œç¡®ä¿ï¼š

- [ ] é…ç½®æ–‡ä»¶ä¸­è®¾ç½® `use_advanced_graph: True`
- [ ] è®¾ç½®åˆé€‚çš„ `graph_heads` (å»ºè®®2-8)
- [ ] GPU æ˜¾å­˜å……è¶³ï¼ˆè‡³å°‘8GBï¼‰
- [ ] å·²å¯¼å…¥ `graph_learning.py` æ¨¡å—
- [ ] ç†è§£å¯¹æ¯”å­¦ä¹ æŸå¤±çš„ä½œç”¨

---

**ç‰ˆæœ¬**: v2.0  
**æ›´æ–°æ—¶é—´**: 2025-11-14  
**ä½œè€…**: AGPST Team

---

## ğŸ”— å¼•ç”¨

å¦‚æœé«˜çº§å›¾å­¦ä¹ æ¨¡å—å¯¹æ‚¨çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘å¼•ç”¨ï¼š

```bibtex
@article{agpst2025,
  title={Adaptive Graph-based Probabilistic Spatial-Temporal Network},
  author={Your Name},
  journal={arXiv preprint},
  year={2025}
}
```
