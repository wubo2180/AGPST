# ========================================================================
# åœ¨AGPSTæ¨¡å‹ä¸­é›†æˆPostPatch AdaptiveGraphLearnerçš„å®Œæ•´ç¤ºä¾‹
# ========================================================================

"""
é›†æˆæ–¹æ¡ˆï¼šåœ¨patch embeddingä¹‹åï¼ŒGNN encoderä¹‹å‰ä½¿ç”¨åŠ¨æ€å›¾å­¦ä¹ 

æ•°æ®æµç¨‹ï¼š
(B=4, L=864, N=358, C=1) 
-> PatchEmbedding -> (B=4, P=72, N=358, D=96)  # patch_size=12
-> PostPatchAdaptiveGraphLearner -> å­¦ä¹ åŠ¨æ€é‚»æ¥çŸ©é˜µ (B, N, N)
-> ä½¿ç”¨åŠ¨æ€å›¾è¿›è¡ŒGNN encoding
"""

import torch
import torch.nn as nn
from .post_patch_adaptive_graph import PostPatchDynamicGraphConv

class ImprovedPretrainModel(nn.Module):
    def __init__(self, num_nodes, dim, topK, adaptive, epochs, patch_size, 
                 in_channel, embed_dim, num_heads, mlp_ratio, 
                 dropout, mask_ratio, encoder_depth, decoder_depth,
                 patch_sizes=None, mode="pre-train"):
        super().__init__()
        
        # ... å…¶ä»–åˆå§‹åŒ–ä»£ç ä¿æŒä¸å˜ ...
        
        # === æ ¸å¿ƒæ”¹è¿›ï¼šæ·»åŠ åŠ¨æ€å›¾å­¦ä¹ æ¨¡å— ===
        self.use_dynamic_graph = True
        if self.use_dynamic_graph:
            self.dynamic_graph_conv = PostPatchDynamicGraphConv(
                embed_dim=embed_dim,
                num_nodes=num_nodes,
                node_dim=dim,
                num_heads=4,
                topk=topK,
                dropout=dropout
            )
            print(f"âœ… å¯ç”¨åŠ¨æ€å›¾å­¦ä¹ : {num_nodes}èŠ‚ç‚¹, Top-{topK}ç¨€ç–åŒ–")
        
        # å…¶ä»–ç»„ä»¶ä¿æŒä¸å˜
        self.patch_embedding = PatchEmbedding(patch_size, in_channel, embed_dim, 
                                              num_nodes, topK, norm_layer=None, 
                                              patch_sizes=patch_sizes)
        # ... å…¶ä½™ç»„ä»¶ ...

    def encoding(self, long_term_history, epoch, adp, mask=True):
        """æ”¹è¿›çš„ç¼–ç è¿‡ç¨‹ï¼Œé›†æˆåŠ¨æ€å›¾å­¦ä¹ """
        
        if mask:
            # Step 1: Patch Embedding (ç»´åº¦å˜æ¢)
            # (B=4, L=864, N=358, C=1) -> (B=4, C=96, P=72, N=358, K=6)
            patches = self.patch_embedding(long_term_history)
            batch_size, num_dim, num_time, num_nodes, khop = patches.shape
            
            # è°ƒæ•´ç»´åº¦ä¸º (B, P, N, D)
            patches = patches.squeeze(-1).permute(0, 2, 3, 1)  # (B, P, N, D)
            print(f"ğŸ“Š PatchåµŒå…¥åç»´åº¦: {patches.shape}")
            
            # Step 2: ä½ç½®ç¼–ç  (ä¿æŒåŸæœ‰é€»è¾‘)
            patches, self.pos_mat = self.positional_encoding(patches.permute(0, 3, 1, 2))  # ä¸´æ—¶è°ƒæ•´ç»´åº¦
            patches = patches.permute(0, 2, 3, 1)  # è°ƒå› (B, P, N, D)
            
            # Step 3: ğŸ¯ æ ¸å¿ƒæ”¹è¿› - åŠ¨æ€å›¾å­¦ä¹ ä¸å›¾å·ç§¯
            if self.use_dynamic_graph:
                # ä½¿ç”¨åŠ¨æ€å›¾è¿›è¡Œç¼–ç 
                patches_enhanced, learned_adjs = self.dynamic_graph_conv(patches)
                print(f"ğŸ”— å­¦ä¹ åˆ°çš„åŠ¨æ€å›¾: {learned_adjs.shape}")
                
                # å¯é€‰ï¼šå¯è§†åŒ–å­¦åˆ°çš„å›¾ç»“æ„ (è°ƒè¯•æ—¶ä½¿ç”¨)
                if torch.rand(1).item() < 0.01:  # 1%æ¦‚ç‡æ‰“å°
                    avg_adj = learned_adjs.mean(0)  # (N, N)
                    sparsity = (avg_adj > 0.01).float().mean().item()
                    print(f"ğŸ“ˆ å›¾ç¨€ç–åº¦: {sparsity:.3f}, å¹³å‡åº¦æ•°: {avg_adj.sum(1).mean():.2f}")
                
                patches = patches_enhanced
            else:
                # ä½¿ç”¨åŸæœ‰çš„é™æ€å›¾ (å¤‡é€‰æ–¹æ¡ˆ)
                patches = patches.permute(0, 3, 1, 2)  # (B, D, P, N) for original GNN
                patches, _ = self.GNN_encoder((patches, adp))
                patches = patches.permute(0, 2, 3, 1)  # è°ƒå› (B, P, N, D)
            
            # Step 4: Transformerç¼–ç  (ä¿æŒåŸæœ‰é€»è¾‘)
            patches = patches.permute(0, 2, 1, 3)  # (B, N, P, D) for transformer
            
            # è‡ªé€‚åº”mask ratio
            if self.adaptive:
                mask_ratio = self.mask_ratio * math.pow((epoch+1) / self.epochs, self.lamda)
            else:
                mask_ratio = self.mask_ratio
                
            # Masking
            Maskg = MaskGenerator(patches.shape[2], mask_ratio)
            unmasked_token_index, masked_token_index = Maskg.uniform_rand()
            
            encoder_input = patches[:, :, unmasked_token_index, :]
            hidden_states_unmasked = self.encoder(encoder_input)
            hidden_states_unmasked = self.encoder_norm(hidden_states_unmasked)
            
        else:
            # æ¨ç†æ¨¡å¼ (ä¸ä½¿ç”¨mask)
            # ... ç±»ä¼¼çš„æµç¨‹ï¼Œä½†ä¸è¿›è¡Œmasking ...
            pass
            
        return hidden_states_unmasked, unmasked_token_index, masked_token_index

    def forward(self, long_term_history, epoch):
        """å‰å‘ä¼ æ’­"""
        
        # ğŸ”§ æ”¹è¿›ï¼šæ„å»ºåŠ¨æ€é‚»æ¥çŸ©é˜µ
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬è®©åŠ¨æ€å›¾å­¦ä¹ åœ¨encodingå†…éƒ¨å®Œæˆ
        # ä¸éœ€è¦é¢„å…ˆæ„å»ºadpï¼Œè€Œæ˜¯è®©æ¨¡å‹è‡ªå·±å­¦ä¹ 
        
        # é™æ€å›¾ä½œä¸ºå¤‡é€‰ (å¯é€‰)
        if hasattr(self, 'nodevec1') and hasattr(self, 'nodevec2'):
            static_adp = F.softmax(F.relu(torch.mm(self.nodevec1, self.nodevec2)), dim=1)
        else:
            static_adp = None
        
        # ç¼–ç 
        hidden_states_unmasked, unmasked_token_index, masked_token_index = self.encoding(
            long_term_history, epoch, static_adp, mask=True
        )
        
        # è§£ç 
        reconstruction_full = self.decoding(hidden_states_unmasked, masked_token_index, static_adp)
        
        # æå–masked tokens
        reconstruction_masked_tokens, label_masked_tokens = self.get_reconstructed_masked_tokens(
            reconstruction_full, long_term_history, unmasked_token_index, masked_token_index
        )
        
        return reconstruction_masked_tokens, label_masked_tokens


# ========================================================================
# ä½¿ç”¨ç¤ºä¾‹å’Œé…ç½®å»ºè®®
# ========================================================================

def create_improved_model(config):
    """åˆ›å»ºæ”¹è¿›ç‰ˆAGPSTæ¨¡å‹"""
    
    model = ImprovedPretrainModel(
        num_nodes=config['num_nodes'],      # 358
        dim=config['dim'],                  # 10
        topK=config['topK'],               # 6
        adaptive=config['adaptive'],        # True
        epochs=config['pretrain_epochs'],   # 100
        patch_size=config['patch_size'],    # 12
        in_channel=config['in_channel'],    # 1
        embed_dim=config['embed_dim'],      # 96
        num_heads=config['num_heads'],      # 4
        mlp_ratio=config['mlp_ratio'],      # 4
        dropout=config['dropout'],          # 0.1
        mask_ratio=config['mask_ratio'],    # 0.25
        encoder_depth=config['encoder_depth'],  # 4
        decoder_depth=config['decoder_depth'],  # 1
        patch_sizes=config['patch_sizes']   # [6, 12, 24]
    )
    
    return model


# ========================================================================
# æ€§èƒ½åˆ†æå’Œè°ƒè¯•å·¥å…·
# ========================================================================

class GraphAnalyzer:
    """åˆ†æåŠ¨æ€å›¾å­¦ä¹ æ•ˆæœçš„å·¥å…·ç±»"""
    
    @staticmethod
    def analyze_learned_graphs(model, dataloader, num_samples=5):
        """åˆ†ææ¨¡å‹å­¦åˆ°çš„å›¾ç»“æ„"""
        model.eval()
        graph_stats = []
        
        with torch.no_grad():
            for i, (_, history_data) in enumerate(dataloader):
                if i >= num_samples:
                    break
                    
                # è·å–patchç‰¹å¾
                patches = model.patch_embedding(history_data)
                patches = patches.squeeze(-1).permute(0, 2, 3, 1)  # (B, P, N, D)
                
                # è·å–åŠ¨æ€å›¾
                if hasattr(model, 'dynamic_graph_conv'):
                    _, learned_adjs = model.dynamic_graph_conv(patches)
                    
                    # ç»Ÿè®¡å›¾å±æ€§
                    avg_adj = learned_adjs.mean(0)  # (N, N)
                    sparsity = (avg_adj > 0.01).float().mean().item()
                    avg_degree = avg_adj.sum(1).mean().item()
                    max_degree = avg_adj.sum(1).max().item()
                    
                    graph_stats.append({
                        'sparsity': sparsity,
                        'avg_degree': avg_degree,
                        'max_degree': max_degree,
                        'connectivity': (avg_adj.sum(1) > 0).float().mean().item()
                    })
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        if graph_stats:
            print("\nğŸ“Š åŠ¨æ€å›¾å­¦ä¹ ç»Ÿè®¡:")
            for key in graph_stats[0].keys():
                values = [stat[key] for stat in graph_stats]
                print(f"  {key}: å‡å€¼={np.mean(values):.3f}, æ ‡å‡†å·®={np.std(values):.3f}")


# ========================================================================
# é…ç½®æ–‡ä»¶å»ºè®®ä¿®æ”¹
# ========================================================================

"""
# parameters/PEMS03_multiscale.yaml å»ºè®®ä¿®æ”¹:

# 1. å¢å¼ºåŠ¨æ€å›¾å­¦ä¹ 
topK: 8  # åŸ6 -> 8 (ç¨å¾®å¢åŠ è¿æ¥)
adaptive: True  # ä¿æŒè‡ªé€‚åº”

# 2. ä¼˜åŒ–embeddingç»´åº¦
embed_dim: 128  # åŸ96 -> 128 (å¢åŠ è¡¨è¾¾èƒ½åŠ›)

# 3. è°ƒæ•´è®­ç»ƒå‚æ•°
lr: 0.0015  # åŸ0.002 -> 0.0015 (æ›´ç¨³å®š)
mask_ratio: 0.4  # åŸ0.25 -> 0.4 (æ›´å¼ºè‡ªç›‘ç£)

# 4. å¢åŠ æ¨¡å‹æ·±åº¦
encoder_depth: 6  # åŸ4 -> 6
decoder_depth: 2  # åŸ1 -> 2
"""