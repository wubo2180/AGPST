# AGPSTç®€åŒ–ç‰ˆè¯´æ˜

## ğŸ”„ é‡è¦å˜æ›´

### åŸå› 
ç”±äºæ•°æ®é›†çš„æ—¶é—´é•¿åº¦åªæœ‰ **12ä¸ªæ—¶é—´æ­¥**ï¼Œä½¿ç”¨patch embeddingä¼šå°†æ•°æ®åˆ‡åˆ†å¾—å¤ªç¢ï¼Œåè€Œé™ä½æ¨¡å‹æ€§èƒ½ã€‚å› æ­¤ç§»é™¤äº†patch embeddingæœºåˆ¶ã€‚

---

## ğŸ“Š æ–°æ¨¡å‹æ¶æ„

### æ•°æ®æµ
```
è¾“å…¥: (B, 12, N, 1)
  â†“
æ—¶é—´ç‰¹å¾åµŒå…¥ (Linear)
  â†“ (B, N, 12, D)
ä½ç½®ç¼–ç 
  â†“
è‡ªé€‚åº”å›¾å­¦ä¹  (NÃ—Né‚»æ¥çŸ©é˜µ)
  â†“
å›¾å·ç§¯ (2å±‚)
  â†“
Transformerç¼–ç å™¨ (4å±‚)
  â†“
æ—¶é—´èšåˆ (mean)
  â†“ (B, N, D)
GraphWaveNeté¢„æµ‹
  â†“
è¾“å‡º: (B, 12, N, 1)
```

---

## ğŸ¯ æ ¸å¿ƒç»„ä»¶

### 1. æ—¶é—´ç‰¹å¾åµŒå…¥
```python
nn.Sequential(
    nn.Linear(1, 48),      # C -> D/2
    nn.ReLU(),
    nn.Linear(48, 96)      # D/2 -> D
)
```
**ä½œç”¨**: å°†å•é€šé“æ—¶é—´åºåˆ—æ˜ å°„åˆ°é«˜ç»´ç‰¹å¾ç©ºé—´

### 2. è‡ªé€‚åº”å›¾å­¦ä¹ 
```python
adj = torch.mm(node_embeddings1, node_embeddings2)  # (N, N)
adj = relu(adj)
adj = top_k_sparsify(adj, k=10)
adj = normalize(adj)
```
**ç‰¹ç‚¹**:
- å¯å­¦ä¹ çš„èŠ‚ç‚¹åµŒå…¥
- Top-Kç¨€ç–åŒ–
- è¡Œå½’ä¸€åŒ–

### 3. å›¾å·ç§¯
```python
for each time step t:
    h = Linear(x_t)           # ç‰¹å¾å˜æ¢
    h = adj.T @ h             # å›¾èšåˆ
    x_t = ReLU(h)
```
**ç‰¹ç‚¹**:
- 2å±‚å›¾å·ç§¯
- é€æ—¶é—´æ­¥å¤„ç†
- éçº¿æ€§æ¿€æ´»

### 4. Transformerç¼–ç å™¨
```python
TransformerEncoder(
    num_layers=4,
    d_model=96,
    nhead=4,
    dim_feedforward=384,  # mlp_ratio=4
    dropout=0.1
)
```
**ä½œç”¨**: æ•è·æ—¶é—´ä¾èµ–å…³ç³»

---

## âš™ï¸ é…ç½®å‚æ•°

### PEMS03_direct_forecasting.yaml
```yaml
# æ•°æ®å‚æ•°
num_nodes: 358
seq_len: 12              # âš ï¸ æ”¹ä¸º12ï¼ˆä¸å†ä½¿ç”¨864ï¼‰
in_channel: 1
dataset_input_len: 12    # çŸ­æœŸå†å²

# æ¨¡å‹å‚æ•°
dim: 10                  # èŠ‚ç‚¹åµŒå…¥ç»´åº¦
topK: 10                 # Top-Kç¨€ç–åŒ–
embed_dim: 96            # ç‰¹å¾åµŒå…¥ç»´åº¦
num_heads: 4             # Transformerå¤´æ•°
mlp_ratio: 4             # MLPæ‰©å±•æ¯”ä¾‹
dropout: 0.1
encoder_depth: 4         # Transformerå±‚æ•°

# âš ï¸ ä¸å†éœ€è¦çš„å‚æ•°
# patch_size: 12         # å·²åˆ é™¤
# graph_heads: 4         # å·²åˆ é™¤

# è®­ç»ƒå‚æ•°
epochs: 100
batch_size: 16
lr: 0.001
```

---

## ğŸ”„ ä¸æ—§ç‰ˆæœ¬å¯¹æ¯”

| ç‰¹æ€§ | æ—§ç‰ˆæœ¬ (Patch) | æ–°ç‰ˆæœ¬ (ç®€åŒ–) |
|------|----------------|---------------|
| **è¾“å…¥æ•°æ®** | (B, 864, N, 1) | (B, 12, N, 1) |
| **Patch Embedding** | âœ… éœ€è¦ | âŒ ç§»é™¤ |
| **Patchæ•°é‡** | 72ä¸ª (864/12) | æ—  |
| **æ—¶é—´åµŒå…¥** | Conv2d | Linear |
| **å›¾å­¦ä¹ ** | å¤æ‚å¤šå°ºåº¦ | ç®€å•è‡ªé€‚åº” |
| **å¯¹æ¯”å­¦ä¹ ** | InfoNCE | âŒ ç§»é™¤ |
| **å‚æ•°é‡** | ~1.3M | ~0.8M |
| **è®­ç»ƒé€Ÿåº¦** | æ…¢ | å¿« |
| **å†…å­˜å ç”¨** | é«˜ | ä½ |

---

## ğŸ“ ä»£ç å˜æ›´

### main.py - æ— éœ€ä¿®æ”¹
```python
# æ¥å£ä¿æŒå…¼å®¹
model = AGPSTModel(
    num_nodes=config['num_nodes'],
    dim=config['dim'],
    topK=config['topK'],
    patch_size=12,  # ä¿ç•™å‚æ•°ä½†ä¸ä½¿ç”¨
    in_channel=config['in_channel'],
    embed_dim=config['embed_dim'],
    num_heads=config['num_heads'],
    graph_heads=4,  # ä¿ç•™å‚æ•°ä½†ä¸ä½¿ç”¨
    mlp_ratio=config['mlp_ratio'],
    dropout=config['dropout'],
    encoder_depth=config['encoder_depth'],
    backend_args=config['backend_args']
)

# forwardè°ƒç”¨
prediction = model(
    history_data,       # (B, 12, N, 1) - ä½¿ç”¨è¿™ä¸ª
    long_history_data,  # ä¸ä½¿ç”¨ï¼Œä¿æŒå…¼å®¹
)
```

### é…ç½®æ–‡ä»¶æ›´æ–°
```yaml
# parameters/PEMS03_direct_forecasting.yaml

# ä¿®æ”¹è¿™äº›å‚æ•°
seq_len: 12              # ä» 864 æ”¹ä¸º 12
dataset_input_len: 12    # ä» 12 ä¿æŒä¸å˜
dataset_output_len: 12   # ä¿æŒä¸å˜

# å¯é€‰ï¼šåˆ é™¤è¿™äº›å‚æ•°ï¼ˆæ¨¡å‹ä¸å†ä½¿ç”¨ï¼‰
# patch_size: 12
# graph_heads: 4
# contrastive_weight: 0.05
```

---

## ğŸš€ ä½¿ç”¨æ–¹å¼

### 1. æ›´æ–°é…ç½®æ–‡ä»¶
```bash
# ç¼–è¾‘ parameters/PEMS03_direct_forecasting.yaml
# å°† seq_len ä» 864 æ”¹ä¸º 12
```

### 2. è¿è¡Œè®­ç»ƒ
```bash
# Windows
run_direct_forecasting.bat

# æˆ–ç›´æ¥è¿è¡Œ
python main.py --config parameters/PEMS03_direct_forecasting.yaml --mode train
```

### 3. æ•°æ®æ ¼å¼
```python
# è®­ç»ƒæ•°æ®
history_data:      (B, 12, 358, 1)  # çŸ­æœŸå†å² - ä½¿ç”¨è¿™ä¸ª
long_history_data: (B, 12, 358, 1)  # ä¸ä½¿ç”¨ï¼ˆä¿æŒå…¼å®¹ï¼‰
future_data:       (B, 12, 358, 1)  # é¢„æµ‹ç›®æ ‡

# æ¨¡å‹è¾“å‡º
prediction:        (B, 12, 358, 1)
```

---

## âœ… ä¼˜åŠ¿

1. **æ›´ç®€å•**: ç§»é™¤å¤æ‚çš„patchæœºåˆ¶
2. **æ›´å¿«**: å‡å°‘è®¡ç®—é‡å’Œå†…å­˜å ç”¨
3. **æ›´é€‚åˆ**: é’ˆå¯¹12æ­¥çŸ­åºåˆ—ä¼˜åŒ–
4. **æ›´ç›´è§‚**: ç›´æ¥æ—¶é—´å»ºæ¨¡ï¼Œæ˜“ç†è§£
5. **å‚æ•°æ›´å°‘**: ä»1.3Må‡å°‘åˆ°0.8M

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. æ•°æ®é›†è¦æ±‚
- ç¡®ä¿æ•°æ®é›†çš„ `seq_len = 12`
- ä¸å†éœ€è¦864é•¿åº¦çš„å†å²æ•°æ®

### 2. å‘åå…¼å®¹
- æ¨¡å‹æ¥å£ä¿æŒä¸å˜
- `long_history_data` å‚æ•°ä¿ç•™ä½†ä¸ä½¿ç”¨
- é…ç½®å‚æ•° `patch_size`, `graph_heads` ä¿ç•™ä½†å¿½ç•¥

### 3. æ€§èƒ½é¢„æœŸ
- è®­ç»ƒé€Ÿåº¦æå‡ **~40%**
- å†…å­˜å ç”¨å‡å°‘ **~35%**
- é¢„æµ‹ç²¾åº¦å¯èƒ½ç•¥æœ‰å˜åŒ–ï¼ˆéœ€å®éªŒéªŒè¯ï¼‰

---

## ğŸ” è°ƒè¯•å»ºè®®

### æ£€æŸ¥æ•°æ®å½¢çŠ¶
```python
# åœ¨ç¬¬ä¸€ä¸ªepochçš„ç¬¬ä¸€ä¸ªbatch
if epoch == 0 and idx == 0:
    print(f"history_data: {history_data.shape}")       # (16, 12, 358, 1)
    print(f"long_history_data: {long_history_data.shape}")  # ä¸ä½¿ç”¨
    print(f"prediction: {prediction.shape}")            # (16, 12, 358, 1)
```

### éªŒè¯å›¾ç»“æ„
```python
# åœ¨æ¨¡å‹å†…éƒ¨
adj = model.learn_graph()
print(f"Graph density: {(adj > 0).sum().item() / (358*358):.2%}")
print(f"Avg degree: {(adj > 0).sum(1).float().mean():.1f}")
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- **ARCHITECTURE_DIAGRAM.md** - å®Œæ•´æ¶æ„å›¾ï¼ˆéœ€æ›´æ–°ï¼‰
- **basicts/mask/README.md** - æ¨¡å—æ–‡æ¡£ï¼ˆéœ€æ›´æ–°ï¼‰
- **é…ç½®æ–‡ä»¶**: `parameters/PEMS03_direct_forecasting.yaml`

---

**æ›´æ–°æ—¥æœŸ**: 2025-01-11  
**ç‰ˆæœ¬**: v2.1 (ç®€åŒ–ç‰ˆ - ç§»é™¤Patch Embedding)  
**çŠ¶æ€**: âœ… å°±ç»ª
