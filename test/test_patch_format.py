"""
æµ‹è¯•ä¿®æ”¹åçš„PatchEmbeddingè¾“å‡ºæ ¼å¼
éªŒè¯è¾“å‡ºæ˜¯å¦ä¸º (B, N, P, d) æ ¼å¼
"""

import torch
from basicts.mask.patch import PatchEmbedding

def test_patch_embedding_format():
    """æµ‹è¯•PatchEmbeddingçš„è¾“å‡ºæ ¼å¼"""
    
    print("=== æµ‹è¯•PatchEmbeddingè¾“å‡ºæ ¼å¼ ===")
    
    # åˆ›å»ºPatchEmbeddingå®ä¾‹
    patch_size = 12
    in_channel = 1
    embed_dim = 96
    
    patch_embedding = PatchEmbedding(
        patch_size=patch_size,
        in_channel=in_channel,
        embed_dim=embed_dim,
        norm_layer=None
    )
    
    print(f"PatchEmbeddingé…ç½®:")
    print(f"  patch_size: {patch_size}")
    print(f"  in_channel: {in_channel}")  
    print(f"  embed_dim: {embed_dim}")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ® (B, N, C, L)
    B, N, C, L = 4, 358, 1, 864
    test_data = torch.randn(B, N, C, L)
    
    print(f"\nè¾“å…¥æ•°æ®:")
    print(f"  å½¢çŠ¶: {test_data.shape}")
    print(f"  æ ¼å¼: (B, N, C, L)")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        output = patch_embedding(test_data)
    
    print(f"\nè¾“å‡ºæ•°æ®:")
    print(f"  å½¢çŠ¶: {output.shape}")
    print(f"  æ ¼å¼: (B, N, P, d)")
    
    # éªŒè¯è¾“å‡ºæ ¼å¼
    expected_P = L // patch_size  # 864 // 12 = 72
    expected_shape = (B, N, expected_P, embed_dim)
    
    print(f"\næ ¼å¼éªŒè¯:")
    print(f"  æœŸæœ›å½¢çŠ¶: {expected_shape}")
    print(f"  å®é™…å½¢çŠ¶: {output.shape}")
    print(f"  æ ¼å¼æ­£ç¡®: {'âœ…' if output.shape == expected_shape else 'âŒ'}")
    
    # è¯¦ç»†ç»´åº¦åˆ†æ
    print(f"\nç»´åº¦åˆ†æ:")
    print(f"  B (batch_size): {output.shape[0]} = {B}")
    print(f"  N (num_nodes): {output.shape[1]} = {N}")  
    print(f"  P (num_patches): {output.shape[2]} = {L}/{patch_size} = {expected_P}")
    print(f"  d (embed_dim): {output.shape[3]} = {embed_dim}")
    
    return output.shape == expected_shape

def test_compatibility_with_dynamic_graph():
    """æµ‹è¯•ä¸PostPatchDynamicGraphConvçš„å…¼å®¹æ€§"""
    
    print("\n=== æµ‹è¯•ä¸åŠ¨æ€å›¾å­¦ä¹ çš„å…¼å®¹æ€§ ===")
    
    try:
        from basicts.mask.post_patch_adaptive_graph import PostPatchDynamicGraphConv
        
        # åˆ›å»ºç»„ä»¶
        patch_embedding = PatchEmbedding(12, 1, 96, None)
        dynamic_graph = PostPatchDynamicGraphConv(
            embed_dim=96,
            num_nodes=358,
            node_dim=10,
            num_heads=4,
            topk=6,
            dropout=0.1
        )
        
        # æµ‹è¯•æ•°æ®æµ
        test_data = torch.randn(4, 358, 1, 864)  # (B, N, C, L)
        
        with torch.no_grad():
            # Step 1: Patch Embedding
            patches = patch_embedding(test_data)  # æœŸæœ›: (B, N, P, d)
            print(f"PatchEmbeddingè¾“å‡º: {patches.shape}")
            
            # Step 2: åŠ¨æ€å›¾å­¦ä¹  (æœŸæœ›è¾“å…¥æ ¼å¼ä¸º (B, P, N, d))
            patches_for_graph = patches.permute(0, 2, 1, 3)  # (B, N, P, d) -> (B, P, N, d)
            print(f"è½¬æ¢ä¸ºå›¾å­¦ä¹ æ ¼å¼: {patches_for_graph.shape}")
            
            # Step 3: åŠ¨æ€å›¾å­¦ä¹ 
            enhanced_patches, learned_adj = dynamic_graph(patches_for_graph)
            print(f"åŠ¨æ€å›¾å­¦ä¹ è¾“å‡º: {enhanced_patches.shape}")
            print(f"å­¦ä¹ çš„é‚»æ¥çŸ©é˜µ: {learned_adj.shape}")
            
            print("âœ… ä¸PostPatchDynamicGraphConvå…¼å®¹!")
            
    except Exception as e:
        print(f"âŒ å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        return False
        
    return True

if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•ä¿®æ”¹åçš„PatchEmbeddingæ ¼å¼")
    print("=" * 50)
    
    # æµ‹è¯•è¾“å‡ºæ ¼å¼
    format_correct = test_patch_embedding_format()
    
    # æµ‹è¯•å…¼å®¹æ€§
    compatibility_ok = test_compatibility_with_dynamic_graph()
    
    print("\n" + "=" * 50)
    if format_correct and compatibility_ok:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        print("âœ… PatchEmbeddingç°åœ¨è¾“å‡º (B, N, P, d) æ ¼å¼")
        print("âœ… ä¸PostPatchDynamicGraphConvå®Œå…¨å…¼å®¹")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ")