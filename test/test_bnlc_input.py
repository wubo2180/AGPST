"""
æµ‹è¯•ä¿®æ”¹åçš„PatchEmbeddingå¤„ç†(B, N, L, C)è¾“å…¥æ ¼å¼
"""

import torch
from basicts.mask.patch import PatchEmbedding

def test_bnlc_input_format():
    """æµ‹è¯•(B, N, L, C)è¾“å…¥æ ¼å¼"""
    
    print("=== æµ‹è¯•PatchEmbeddingå¤„ç†(B, N, L, C)è¾“å…¥ ===")
    
    # åˆ›å»ºPatchEmbedding
    patch_embedding = PatchEmbedding(
        patch_size=12,
        in_channel=1,
        embed_dim=96,
        norm_layer=None
    )
    
    print(f"PatchEmbeddingé…ç½®:")
    print(f"  patch_size: 12")
    print(f"  in_channel: 1")
    print(f"  embed_dim: 96")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ® (B, N, L, C)
    B, N, L, C = 4, 358, 864, 1
    test_data = torch.randn(B, N, L, C)
    
    print(f"\nè¾“å…¥æ•°æ®:")
    print(f"  å½¢çŠ¶: {test_data.shape}")
    print(f"  æ ¼å¼: (B, N, L, C)")
    print(f"  B={B}, N={N}, L={L}, C={C}")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        try:
            output = patch_embedding(test_data)
            
            print(f"\nè¾“å‡ºæ•°æ®:")
            print(f"  å½¢çŠ¶: {output.shape}")
            print(f"  æ ¼å¼: (B, N, P, d)")
            
            # éªŒè¯è¾“å‡ºç»´åº¦
            expected_P = L // 12  # 864 // 12 = 72
            expected_shape = (B, N, expected_P, 96)
            
            print(f"\nç»´åº¦éªŒè¯:")
            print(f"  æœŸæœ›å½¢çŠ¶: {expected_shape}")
            print(f"  å®é™…å½¢çŠ¶: {output.shape}")
            print(f"  ç»´åº¦æ­£ç¡®: {'OK' if output.shape == expected_shape else 'FAIL'}")
            
            # è¯¦ç»†åˆ†æ
            print(f"\nè¯¦ç»†åˆ†æ:")
            print(f"  B (batch_size): {output.shape[0]} = {B}")
            print(f"  N (num_nodes): {output.shape[1]} = {N}")
            print(f"  P (num_patches): {output.shape[2]} = {L}/{12} = {expected_P}")
            print(f"  d (embed_dim): {output.shape[3]} = 96")
            
            return output.shape == expected_shape
            
        except Exception as e:
            print(f"\nå‰å‘ä¼ æ’­å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

def test_different_input_sizes():
    """æµ‹è¯•ä¸åŒè¾“å…¥å°ºå¯¸"""
    
    print(f"\n=== æµ‹è¯•ä¸åŒè¾“å…¥å°ºå¯¸ ===")
    
    patch_embedding = PatchEmbedding(12, 1, 96, None)
    
    test_cases = [
        {"shape": (2, 100, 240, 1), "desc": "å°è§„æ¨¡æ•°æ®"},
        {"shape": (4, 358, 864, 1), "desc": "PEMS03æ•°æ®"},
        {"shape": (8, 207, 1440, 1), "desc": "å¤§è§„æ¨¡æ•°æ®"},
    ]
    
    all_passed = True
    
    for case in test_cases:
        B, N, L, C = case["shape"]
        desc = case["desc"]
        
        print(f"\næµ‹è¯• {desc}: {case['shape']}")
        
        try:
            test_data = torch.randn(B, N, L, C)
            
            with torch.no_grad():
                output = patch_embedding(test_data)
                
            expected_P = L // 12
            expected_shape = (B, N, expected_P, 96)
            
            success = output.shape == expected_shape
            print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
            print(f"  æœŸæœ›å½¢çŠ¶: {expected_shape}")
            print(f"  ç»“æœ: {'OK' if success else 'FAIL'}")
            
            if not success:
                all_passed = False
                
        except Exception as e:
            print(f"  æµ‹è¯•å¤±è´¥: {e}")
            all_passed = False
    
    return all_passed

def test_compatibility_with_dynamic_graph():
    """æµ‹è¯•ä¸åŠ¨æ€å›¾å­¦ä¹ çš„å…¼å®¹æ€§"""
    
    print(f"\n=== æµ‹è¯•ä¸PostPatchDynamicGraphConvå…¼å®¹æ€§ ===")
    
    try:
        from basicts.mask.post_patch_adaptive_graph import PostPatchDynamicGraphConv
        
        # åˆ›å»ºç»„ä»¶
        patch_embedding = PatchEmbedding(12, 1, 96, None)
        dynamic_graph = PostPatchDynamicGraphConv(
            embed_dim=96,
            num_nodes=358,
            node_dim=10,
            graph_heads=4,  # ä½¿ç”¨æ–°çš„å‚æ•°å
            topk=6,
            dropout=0.1
        )
        
        print("ç»„ä»¶åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•å®Œæ•´æ•°æ®æµ (B, N, L, C)
        test_data = torch.randn(4, 358, 864, 1)
        print(f"åŸå§‹è¾“å…¥: {test_data.shape} (B, N, L, C)")
        
        with torch.no_grad():
            # Step 1: Patch Embedding
            patches = patch_embedding(test_data)
            print(f"PatchEmbeddingè¾“å‡º: {patches.shape} (B, N, P, d)")
            
            # Step 2: åŠ¨æ€å›¾å­¦ä¹  (ç›´æ¥å…¼å®¹)
            enhanced_patches, learned_adj = dynamic_graph(patches)
            print(f"åŠ¨æ€å›¾å­¦ä¹ è¾“å‡º: {enhanced_patches.shape} (B, N, P, D)")
            print(f"å­¦ä¹ çš„é‚»æ¥çŸ©é˜µ: {learned_adj.shape} (B, N, N)")
            
            print("OK: å®Œæ•´æµæ°´çº¿æµ‹è¯•æˆåŠŸ!")
            return True
            
    except Exception as e:
        print(f"å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•PatchEmbeddingå¤„ç†(B, N, L, C)è¾“å…¥æ ¼å¼")
    print("=" * 60)
    
    # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    basic_test = test_bnlc_input_format()
    
    # æµ‹è¯•ä¸åŒå°ºå¯¸
    size_test = test_different_input_sizes()
    
    # æµ‹è¯•å…¼å®¹æ€§
    compat_test = test_compatibility_with_dynamic_graph()
    
    print("\n" + "=" * 60)
    if basic_test and size_test and compat_test:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        print("âœ… PatchEmbeddingæ­£ç¡®å¤„ç† (B, N, L, C) è¾“å…¥")
        print("âœ… è¾“å‡ºæ ¼å¼ (B, N, P, d) æ­£ç¡®")
        print("âœ… ä¸PostPatchDynamicGraphConvå®Œå…¨å…¼å®¹")
        
        print(f"\nğŸ“ æ•°æ®æµæ€»ç»“:")
        print(f"è¾“å…¥: (B, N, L, C) = (4, 358, 864, 1)")
        print(f"â†“ è½¬æ¢: (B, N, L, C) â†’ (B, N, C, L)")
        print(f"â†“ PatchEmbedding")
        print(f"è¾“å‡º: (B, N, P, d) = (4, 358, 72, 96)")
        print(f"â†“ PostPatchDynamicGraphConv")
        print(f"æœ€ç»ˆ: (B, N, P, D) + adj(B, N, N)")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ")