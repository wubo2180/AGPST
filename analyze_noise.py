"""
æ·±åº¦å™ªå£°åˆ†æ - è¯„ä¼°æ•°æ®å™ªå£°æ°´å¹³
"""
import numpy as np
import matplotlib.pyplot as plt
from scipy import signal, stats
from scipy.fft import fft, fftfreq
import os
plt.rcParams['font.sans-serif'] = ['SimHei']


def load_dataset(dataset_name='PEMS03', mode='train'):
    """åŠ è½½æ•°æ®é›†"""
    data_path = f'datasets/{dataset_name}/{mode}_data.npy'
    
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return None
    
    data = np.load(data_path)
    print(f"âœ… åŠ è½½æ•°æ®: {data_path}")
    print(f"   æ•°æ®å½¢çŠ¶: {data.shape}")
    
    return data


def estimate_snr(data, window_size=50):
    """
    ä¼°è®¡ä¿¡å™ªæ¯” (Signal-to-Noise Ratio)
    ä½¿ç”¨æ»‘åŠ¨çª—å£ä¼°è®¡å±€éƒ¨ä¿¡å·å’Œå™ªå£°
    """
    T, N = data.shape
    snr_values = np.zeros(N)
    
    for node in range(N):
        time_series = data[:, node]
        
        # ä½¿ç”¨ç§»åŠ¨å¹³å‡ä½œä¸ºä¿¡å·
        signal_estimate = np.convolve(time_series, np.ones(window_size)/window_size, mode='same')
        
        # å™ªå£° = åŸå§‹æ•°æ® - ä¿¡å·
        noise = time_series - signal_estimate
        
        # è®¡ç®—ä¿¡å·åŠŸç‡å’Œå™ªå£°åŠŸç‡
        signal_power = np.mean(signal_estimate ** 2)
        noise_power = np.mean(noise ** 2)
        
        # SNR (dB)
        if noise_power > 0:
            snr_values[node] = 10 * np.log10(signal_power / noise_power)
        else:
            snr_values[node] = 100  # éå¸¸å¹²å‡€
    
    return snr_values


def analyze_frequency_spectrum(data, sample_rate=1.0, num_nodes=10):
    """
    åˆ†æé¢‘è°± - æ£€æµ‹é«˜é¢‘å™ªå£°
    """
    T, N = data.shape
    
    # éšæœºé€‰æ‹©å‡ ä¸ªèŠ‚ç‚¹
    sample_nodes = np.random.choice(N, min(num_nodes, N), replace=False)
    
    all_freqs = []
    all_power = []
    
    for node in sample_nodes:
        time_series = data[:, node]
        
        # FFT
        yf = fft(time_series)
        xf = fftfreq(T, 1/sample_rate)[:T//2]
        power = 2.0/T * np.abs(yf[:T//2])
        
        all_freqs.append(xf)
        all_power.append(power)
    
    # å¹³å‡é¢‘è°±
    avg_power = np.mean(all_power, axis=0)
    
    return xf, avg_power


def detect_outliers(data, method='iqr'):
    """
    æ£€æµ‹å¼‚å¸¸å€¼
    method: 'iqr' (å››åˆ†ä½è·) æˆ– 'zscore' (Zåˆ†æ•°)
    """
    T, N = data.shape
    
    if method == 'iqr':
        # IQRæ–¹æ³•
        q1 = np.percentile(data, 25, axis=0)
        q3 = np.percentile(data, 75, axis=0)
        iqr = q3 - q1
        
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        
        outliers = (data < lower_bound) | (data > upper_bound)
        
    else:  # zscore
        # Z-scoreæ–¹æ³•
        mean = np.mean(data, axis=0)
        std = np.std(data, axis=0)
        z_scores = np.abs((data - mean) / std)
        outliers = z_scores > 3
    
    outlier_ratio = outliers.sum() / data.size * 100
    
    return outliers, outlier_ratio


def analyze_autocorrelation(data, max_lag=50, num_samples=5):
    """
    åˆ†æè‡ªç›¸å…³æ€§ - æ£€æµ‹æ—¶åºæ¨¡å¼å’Œå™ªå£°
    """
    T, N = data.shape
    sample_nodes = np.random.choice(N, min(num_samples, N), replace=False)
    
    all_acf = []
    
    for node in sample_nodes:
        time_series = data[:, node]
        time_series = (time_series - time_series.mean()) / time_series.std()
        
        acf = []
        for lag in range(max_lag):
            if lag == 0:
                acf.append(1.0)
            else:
                correlation = np.corrcoef(time_series[:-lag], time_series[lag:])[0, 1]
                acf.append(correlation)
        
        all_acf.append(acf)
    
    avg_acf = np.mean(all_acf, axis=0)
    
    return avg_acf


def plot_comprehensive_analysis(data, save_path='figure/noise_analysis_report.png'):
    """
    ç”Ÿæˆç»¼åˆå™ªå£°åˆ†ææŠ¥å‘Šï¼ˆ4åˆ1å›¾è¡¨ï¼‰
    """
    fig = plt.figure(figsize=(16, 10))
    
    # 1. SNRåˆ†å¸ƒ
    ax1 = plt.subplot(2, 2, 1)
    snr_values = estimate_snr(data)
    
    # åˆ›å»ºçƒ­å›¾æ˜¾ç¤º
    T, N = data.shape
    n_rows = int(np.sqrt(N))
    n_cols = int(np.ceil(N / n_rows))
    snr_grid = np.full((n_rows, n_cols), np.nan)
    snr_grid.flat[:N] = snr_values
    
    im1 = ax1.imshow(snr_grid, cmap='RdYlGn', aspect='auto', vmin=0, vmax=30)
    ax1.set_title('å„èŠ‚ç‚¹ä¿¡å™ªæ¯”(SNR)åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    ax1.set_xlabel('åˆ—ç´¢å¼•', fontsize=10)
    ax1.set_ylabel('è¡Œç´¢å¼•', fontsize=10)
    plt.colorbar(im1, ax=ax1, label='SNR (dB)')
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    avg_snr = np.mean(snr_values)
    ax1.text(0.02, 0.98, f'å¹³å‡SNR: {avg_snr:.2f} dB', 
             transform=ax1.transAxes, fontsize=10,
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    # 2. é¢‘è°±åˆ†æ
    ax2 = plt.subplot(2, 2, 2)
    freqs, power = analyze_frequency_spectrum(data)
    
    ax2.semilogy(freqs, power, linewidth=1.5, color='steelblue')
    ax2.set_title('å¹³å‡åŠŸç‡è°±å¯†åº¦', fontsize=12, fontweight='bold')
    ax2.set_xlabel('é¢‘ç‡', fontsize=10)
    ax2.set_ylabel('åŠŸç‡ (å¯¹æ•°å°ºåº¦)', fontsize=10)
    ax2.grid(True, alpha=0.3)
    
    # æ ‡è®°é«˜é¢‘åŒºåŸŸ
    high_freq_threshold = freqs.max() * 0.3
    high_freq_idx = freqs > high_freq_threshold
    high_freq_power = power[high_freq_idx].sum()
    total_power = power.sum()
    high_freq_ratio = high_freq_power / total_power * 100
    
    ax2.axvline(high_freq_threshold, color='red', linestyle='--', linewidth=1.5, 
                label=f'é«˜é¢‘åŒº (>{high_freq_threshold:.2f})')
    ax2.text(0.02, 0.98, f'é«˜é¢‘èƒ½é‡å æ¯”: {high_freq_ratio:.2f}%', 
             transform=ax2.transAxes, fontsize=10,
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    ax2.legend(fontsize=9)
    
    # 3. å¼‚å¸¸å€¼æ£€æµ‹
    ax3 = plt.subplot(2, 2, 3)
    outliers, outlier_ratio = detect_outliers(data, method='iqr')
    
    # æ˜¾ç¤ºå¼‚å¸¸å€¼åˆ†å¸ƒ
    outlier_counts = outliers.sum(axis=0)
    ax3.bar(range(N), outlier_counts, color='coral', alpha=0.7, edgecolor='black')
    ax3.set_title('å„èŠ‚ç‚¹å¼‚å¸¸å€¼æ•°é‡', fontsize=12, fontweight='bold')
    ax3.set_xlabel('èŠ‚ç‚¹ç´¢å¼•', fontsize=10)
    ax3.set_ylabel('å¼‚å¸¸å€¼æ•°é‡', fontsize=10)
    ax3.grid(True, alpha=0.3, axis='y')
    
    ax3.text(0.02, 0.98, f'æ€»å¼‚å¸¸å€¼æ¯”ä¾‹: {outlier_ratio:.2f}%', 
             transform=ax3.transAxes, fontsize=10,
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    # 4. è‡ªç›¸å…³åˆ†æ
    ax4 = plt.subplot(2, 2, 4)
    acf = analyze_autocorrelation(data)
    lags = range(len(acf))
    
    ax4.plot(lags, acf, linewidth=2, color='steelblue', marker='o', markersize=4)
    ax4.axhline(0, color='black', linestyle='-', linewidth=0.8)
    ax4.axhline(0.5, color='red', linestyle='--', linewidth=1, alpha=0.5, label='ä¸­ç­‰ç›¸å…³')
    ax4.set_title('å¹³å‡è‡ªç›¸å…³å‡½æ•°', fontsize=12, fontweight='bold')
    ax4.set_xlabel('æ»åé‡', fontsize=10)
    ax4.set_ylabel('ç›¸å…³ç³»æ•°', fontsize=10)
    ax4.grid(True, alpha=0.3)
    ax4.legend(fontsize=9)
    
    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªä½ç›¸å…³çš„æ»å
    low_corr_lag = np.argmax(np.abs(acf[1:]) < 0.3) + 1 if any(np.abs(acf[1:]) < 0.3) else len(acf)
    ax4.text(0.02, 0.98, f'å¿«é€Ÿè¡°å‡æ»å: {low_corr_lag}', 
             transform=ax4.transAxes, fontsize=10,
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\nğŸ“Š ç»¼åˆåˆ†ææŠ¥å‘Šå·²ä¿å­˜: {save_path}")
    plt.close()
    
    return {
        'avg_snr': avg_snr,
        'high_freq_ratio': high_freq_ratio,
        'outlier_ratio': outlier_ratio,
        'low_corr_lag': low_corr_lag
    }


def generate_detailed_recommendation(metrics):
    """æ ¹æ®å¤šä¸ªæŒ‡æ ‡ç”Ÿæˆè¯¦ç»†å»ºè®®"""
    print("\n" + "="*60)
    print("ğŸ“‹ å™ªå£°åˆ†ææŠ¥å‘Š")
    print("="*60)
    
    avg_snr = metrics['avg_snr']
    high_freq_ratio = metrics['high_freq_ratio']
    outlier_ratio = metrics['outlier_ratio']
    
    print(f"\nğŸ“Š å…³é”®æŒ‡æ ‡:")
    print(f"  â€¢ å¹³å‡ä¿¡å™ªæ¯”: {avg_snr:.2f} dB")
    print(f"  â€¢ é«˜é¢‘èƒ½é‡å æ¯”: {high_freq_ratio:.2f}%")
    print(f"  â€¢ å¼‚å¸¸å€¼æ¯”ä¾‹: {outlier_ratio:.2f}%")
    
    # è¯„åˆ†ç³»ç»Ÿ
    score = 0
    reasons = []
    
    # SNRè¯„ä¼°
    if avg_snr < 10:
        score += 3
        reasons.append(f"âŒ SNRè¿‡ä½ ({avg_snr:.2f} dB < 10 dB)")
    elif avg_snr < 20:
        score += 2
        reasons.append(f"âš ï¸  SNRè¾ƒä½ ({avg_snr:.2f} dB < 20 dB)")
    else:
        reasons.append(f"âœ… SNRè‰¯å¥½ ({avg_snr:.2f} dB >= 20 dB)")
    
    # é«˜é¢‘å™ªå£°è¯„ä¼°
    if high_freq_ratio > 30:
        score += 3
        reasons.append(f"âŒ é«˜é¢‘å™ªå£°ä¸¥é‡ ({high_freq_ratio:.2f}% > 30%)")
    elif high_freq_ratio > 10:
        score += 2
        reasons.append(f"âš ï¸  é«˜é¢‘å™ªå£°æ˜æ˜¾ ({high_freq_ratio:.2f}% > 10%)")
    else:
        reasons.append(f"âœ… é«˜é¢‘å™ªå£°è¾ƒå°‘ ({high_freq_ratio:.2f}% <= 10%)")
    
    # å¼‚å¸¸å€¼è¯„ä¼°
    if outlier_ratio > 5:
        score += 2
        reasons.append(f"âŒ å¼‚å¸¸å€¼è¿‡å¤š ({outlier_ratio:.2f}% > 5%)")
    elif outlier_ratio > 1:
        score += 1
        reasons.append(f"âš ï¸  æœ‰ä¸€å®šå¼‚å¸¸å€¼ ({outlier_ratio:.2f}% > 1%)")
    else:
        reasons.append(f"âœ… å¼‚å¸¸å€¼å¾ˆå°‘ ({outlier_ratio:.2f}% <= 1%)")
    
    print(f"\nğŸ” è¯Šæ–­ç»“æœ:")
    for reason in reasons:
        print(f"  {reason}")
    
    print(f"\nğŸ’¯ å™ªå£°è¯„åˆ†: {score}/8")
    
    print("\n" + "-"*60)
    print("ğŸ’¡ æ¨èæ–¹æ¡ˆ:")
    print("-"*60)
    
    if score >= 6:
        print("\nğŸ”´ æ•°æ®å™ªå£°ä¸¥é‡ï¼Œå¼ºçƒˆæ¨èä½¿ç”¨æ³¨æ„åŠ›å»å™ª:")
        print("\n```yaml")
        print("use_denoising: True")
        print("denoise_type: 'attention'")
        print("```")
        print("\nç†ç”±: æ•°æ®åŒ…å«ä¸¥é‡å™ªå£°ï¼Œéœ€è¦å¼ºå¤§çš„è‡ªé€‚åº”å»å™ªèƒ½åŠ›")
        
    elif score >= 3:
        print("\nğŸŸ¡ æ•°æ®æœ‰ä¸€å®šå™ªå£°ï¼Œæ¨èä½¿ç”¨å·ç§¯å»å™ª:")
        print("\n```yaml")
        print("use_denoising: True")
        print("denoise_type: 'conv'")
        print("```")
        print("\nç†ç”±: è½»é‡çº§å»å™ªå³å¯å¤„ç†ï¼Œå…¼é¡¾æ•ˆç‡å’Œæ•ˆæœ")
        
    else:
        print("\nğŸŸ¢ æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå¯ä»¥ä¸ä½¿ç”¨å»å™ª:")
        print("\n```yaml")
        print("use_denoising: False")
        print("```")
        print("\nä½†å»ºè®®åšå¯¹æ¯”å®éªŒéªŒè¯:")
        print("\n1. Baseline (æ— å»å™ª)")
        print("2. Convå»å™ª")
        print("3. å¯¹æ¯”éªŒè¯é›†æ€§èƒ½")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "ğŸ”¬ " + "="*58 + " ğŸ”¬")
    print("ğŸ”¬  æ·±åº¦å™ªå£°åˆ†æå·¥å…·")
    print("ğŸ”¬ " + "="*58 + " ğŸ”¬\n")
    
    # åŠ è½½æ•°æ®
    dataset_name = 'METR-LA'
    mode = 'train'
    
    data = load_dataset(dataset_name, mode)
    
    if data is None:
        print("\nâŒ æ— æ³•åŠ è½½æ•°æ®ï¼Œç¨‹åºé€€å‡º")
        return
    
    print("\n" + "="*60)
    print("ğŸ“Š æ‰§è¡Œå™ªå£°åˆ†æ...")
    print("="*60)
    
    # ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
    metrics = plot_comprehensive_analysis(data)
    
    # ç”Ÿæˆè¯¦ç»†å»ºè®®
    generate_detailed_recommendation(metrics)
    
    print("\n" + "="*60)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("="*60)
    print("\nğŸ“‚ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: figure/noise_analysis_report.png")
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("   1. æŸ¥çœ‹å™ªå£°åˆ†ææŠ¥å‘Šå›¾è¡¨")
    print("   2. æ ¹æ®æ¨èé…ç½®å»å™ªå‚æ•°")
    print("   3. è¿è¡Œå¯¹æ¯”å®éªŒéªŒè¯æ•ˆæœ")
    print("   4. é€‰æ‹©æœ€ä½³é…ç½®è¿›è¡Œå®Œæ•´è®­ç»ƒ\n")


if __name__ == '__main__':
    main()
