# HimNet è®¾è®¡ç†å¿µå€Ÿé‰´åˆ†æ

## ğŸ“Š HimNet vs æˆ‘ä»¬çš„æ¶æ„å¯¹æ¯”

| ç»´åº¦ | HimNet (KDD'24) | æˆ‘ä»¬çš„ AlternatingST | å€Ÿé‰´ä»·å€¼ |
|------|-----------------|---------------------|---------|
| **ç¼–ç æ–¹å¼** | åŒç¼–ç å™¨å¹¶è¡Œ | äº¤æ›¿ç¼–ç  | âœ… æˆ‘ä»¬æ›´ä¼˜ (æœ‰ä¿¡æ¯æµ) |
| **èŠ‚ç‚¹å»ºæ¨¡** | å…ƒå­¦ä¹ èŠ‚ç‚¹åµŒå…¥ | ç»Ÿä¸€å‚æ•° | â­â­â­â­â­ é«˜ä»·å€¼ |
| **å›¾å·ç§¯** | HimGCN (èŠ‚ç‚¹ç‰¹å®šå‚æ•°) | Self-Attention | â­â­â­â­ å¯å°è¯• |
| **è®­ç»ƒç­–ç•¥** | è®¡åˆ’é‡‡æ · + æ•™å¸ˆå¼ºåˆ¶ | æ ‡å‡†è®­ç»ƒ | â­â­â­ ä¸­ç­‰ä»·å€¼ |
| **æŸå¤±å‡½æ•°** | Huber/MaskedMAE | MAE | â­â­â­â­ å€¼å¾—å°è¯• |
| **å¤æ‚åº¦** | 3.2M å‚æ•° | 1.23M å‚æ•° | âœ… æˆ‘ä»¬æ›´è½»é‡ |

---

## ğŸ¯ æ ¸å¿ƒå€Ÿé‰´ç‚¹è¯¦è§£

### 1. å¼‚è´¨æ€§èŠ‚ç‚¹åµŒå…¥ (Heterogeneity-Aware Node Embedding)

**HimNet çš„å®ç°**:
```python
class HimGCN(nn.Module):
    def __init__(self, num_nodes, d_meta=64):
        # æ¯ä¸ªèŠ‚ç‚¹æœ‰ç‹¬ç«‹çš„å…ƒåµŒå…¥
        self.meta_node_emb = nn.Parameter(torch.randn(num_nodes, d_meta))
        self.meta_fc = nn.Linear(d_meta, d_model * d_model)
    
    def forward(self, x, adj):
        # ç”ŸæˆèŠ‚ç‚¹ç‰¹å®šçš„å·ç§¯æƒé‡
        W_spatial = self.meta_fc(self.meta_node_emb)  # (N, D*D)
        W_spatial = W_spatial.reshape(num_nodes, d_model, d_model)
        
        # æ¯ä¸ªèŠ‚ç‚¹ä½¿ç”¨è‡ªå·±çš„æƒé‡
        out = []
        for i in range(num_nodes):
            out.append(torch.matmul(x[:, i], W_spatial[i]))  # (B, T, D) @ (D, D)
        return torch.stack(out, dim=1)  # (B, N, T, D)
```

**å¯¹æˆ‘ä»¬çš„åº”ç”¨**:
```python
class HeterogeneousSpatialEncoder(nn.Module):
    """
    å¼‚è´¨æ€§æ„ŸçŸ¥çš„ç©ºé—´ç¼–ç å™¨
    ä¸ºä¸åŒèŠ‚ç‚¹ç”Ÿæˆä¸åŒçš„æ³¨æ„åŠ›æƒé‡
    """
    def __init__(self, num_nodes, d_model, d_meta=64):
        super().__init__()
        # èŠ‚ç‚¹å…ƒåµŒå…¥
        self.node_emb = nn.Parameter(torch.randn(num_nodes, d_meta))
        
        # ç”ŸæˆèŠ‚ç‚¹ç‰¹å®šçš„ Query/Key åç½®
        self.meta_q = nn.Linear(d_meta, d_model)
        self.meta_k = nn.Linear(d_meta, d_model)
        
        # æ ‡å‡† Transformer
        self.encoder = nn.TransformerEncoder(...)
        
    def forward(self, x):
        B, N, T, D = x.shape
        
        # ä¸ºæ¯ä¸ªèŠ‚ç‚¹ç”Ÿæˆç‰¹å®šçš„åç½®
        node_q_bias = self.meta_q(self.node_emb)  # (N, D)
        node_k_bias = self.meta_k(self.node_emb)  # (N, D)
        
        # é‡å¡‘å¹¶æ·»åŠ èŠ‚ç‚¹åç½®
        x_flat = x.reshape(B*T, N, D)
        x_flat = x_flat + node_q_bias.unsqueeze(0)  # å¹¿æ’­åˆ° (B*T, N, D)
        
        # Transformer ç¼–ç 
        spatial_features = self.encoder(x_flat)
        return spatial_features.reshape(B, N, T, D)
```

**ä¼˜åŠ¿**:
- âœ… æ•è·èŠ‚ç‚¹å¼‚è´¨æ€§ (é«˜é€Ÿå…¬è·¯ vs åŸå¸‚é“è·¯)
- âœ… å‚æ•°å¢åŠ å°‘: `N Ã— d_meta` (358 Ã— 64 = 22k å‚æ•°)
- âœ… ä¸æˆ‘ä»¬çš„ Transformer æ¶æ„å…¼å®¹

---

### 2. å¼•å…¥å›¾å·ç§¯å±‚ (Graph Convolution)

**HimNet çš„ HimGCN**:
```python
class HimGCN(nn.Module):
    def forward(self, x, adj):
        # 1. é‚»æ¥çŸ©é˜µå½’ä¸€åŒ–
        D = torch.diag(adj.sum(1))
        A_norm = D^(-0.5) @ adj @ D^(-0.5)
        
        # 2. å›¾å·ç§¯: X' = A_norm @ X @ W
        # æ¯ä¸ªèŠ‚ç‚¹ i èšåˆé‚»å±…ä¿¡æ¯
        support = torch.matmul(A_norm, x)  # (N, N) @ (B, N, T, D)
        out = torch.matmul(support, W_spatial)  # (B, N, T, D) @ (D, D)
        return out
```

**å¯¹æˆ‘ä»¬çš„æ··åˆæ–¹æ¡ˆ**:
```python
class HybridSpatialEncoder(nn.Module):
    """
    æ··åˆç©ºé—´ç¼–ç å™¨: GCN + Transformer
    - GCN: åˆ©ç”¨ç‰©ç†é‚»æ¥å…³ç³»
    - Transformer: å­¦ä¹ è¯­ä¹‰å…³ç³»
    """
    def __init__(self, num_nodes, d_model, adj_mx):
        super().__init__()
        # GCN åˆ†æ”¯
        self.gcn = nn.ModuleList([
            GraphConv(d_model, d_model) for _ in range(2)
        ])
        
        # Transformer åˆ†æ”¯
        self.transformer = nn.TransformerEncoder(...)
        
        # èåˆ
        self.fusion = nn.Linear(d_model * 2, d_model)
        
        # é‚»æ¥çŸ©é˜µå½’ä¸€åŒ–
        self.adj_mx = self._normalize_adj(adj_mx)
        
    def forward(self, x):
        B, N, T, D = x.shape
        
        # GCN è·¯å¾„
        x_gcn = x.reshape(B*T, N, D)
        for gcn_layer in self.gcn:
            x_gcn = gcn_layer(x_gcn, self.adj_mx)  # (B*T, N, D)
        x_gcn = x_gcn.reshape(B, N, T, D)
        
        # Transformer è·¯å¾„
        x_trans = x.reshape(B*T, N, D)
        x_trans = self.transformer(x_trans).reshape(B, N, T, D)
        
        # èåˆä¸¤æ¡è·¯å¾„
        x_fused = torch.cat([x_gcn, x_trans], dim=-1)  # (B, N, T, 2D)
        return self.fusion(x_fused)  # (B, N, T, D)
```

**ä¼˜åŠ¿**:
- âœ… GCN åˆ©ç”¨å…ˆéªŒçŸ¥è¯† (é‚»æ¥çŸ©é˜µ)
- âœ… Transformer å­¦ä¹ éšå¼å…³ç³»
- âœ… åŒè·¯å¾„äº’è¡¥

---

### 3. è®¡åˆ’é‡‡æ · (Scheduled Sampling)

**HimNet çš„è®­ç»ƒç­–ç•¥**:
```python
# è®­ç»ƒæ—¶é€æ­¥å‡å°‘æ•™å¸ˆå¼ºåˆ¶æ¯”ä¾‹
teacher_forcing_ratio = 0.5  # åˆå§‹ 50% ä½¿ç”¨çœŸå®æ ‡ç­¾

for epoch in range(epochs):
    if epoch > 10:  # é¢„çƒ­æœŸåå¯ç”¨
        # ä»¥ä¸€å®šæ¦‚ç‡ä½¿ç”¨æ¨¡å‹é¢„æµ‹è€ŒéçœŸå®æ ‡ç­¾
        use_gt = random.random() < teacher_forcing_ratio
        
        if use_gt:
            decoder_input = ground_truth[:, t-1]
        else:
            decoder_input = model_prediction[:, t-1]
        
        # é€æ­¥é™ä½æ•™å¸ˆå¼ºåˆ¶æ¯”ä¾‹
        teacher_forcing_ratio *= 0.999  # è¡°å‡
```

**å¯¹æˆ‘ä»¬çš„åº”ç”¨** (åœ¨äº¤æ›¿æ¶æ„ä¸­):
```python
class AlternatingSTModelWithSampling(nn.Module):
    def forward(self, x, teacher_forcing_ratio=0.0):
        # Stage 1
        temp_out = self.temporal_encoder_1(x)
        spat_out = self.spatial_encoder_1(temp_out)
        fused = self.fusion_1(temp_out, spat_out)
        decoded = self.decoder(fused)
        
        # è®¡åˆ’é‡‡æ ·: ä»¥æ¦‚ç‡ p ä½¿ç”¨è§£ç ç»“æœ,å¦åˆ™ä½¿ç”¨åŸå§‹è¾“å…¥
        if self.training and random.random() > teacher_forcing_ratio:
            stage2_input = decoded  # ä½¿ç”¨æ¨¡å‹è§£ç çš„ç»“æœ
        else:
            stage2_input = x  # ä½¿ç”¨çœŸå®è¾“å…¥ (æ•™å¸ˆå¼ºåˆ¶)
        
        # Stage 2
        temp_out_2 = self.temporal_encoder_2(stage2_input)
        spat_out_2 = self.spatial_encoder_2(temp_out_2)
        final_out = self.fusion_2(temp_out_2, spat_out_2)
        
        return final_out
```

**ä¼˜åŠ¿**:
- âœ… æé«˜æ¨¡å‹é²æ£’æ€§ (è®­ç»ƒæ—¶è§è¿‡è‡ªå·±çš„é”™è¯¯)
- âœ… å‡å°‘è®­ç»ƒ-æµ‹è¯•å·®å¼‚
- âš ï¸ ä½†å¯èƒ½å¢åŠ è®­ç»ƒä¸ç¨³å®šæ€§

---

### 4. æ›´é²æ£’çš„æŸå¤±å‡½æ•°

**HimNet çš„æŸå¤±è®¾è®¡**:
```python
class HuberLoss(nn.Module):
    """
    Huber Loss: ç»“åˆ MAE å’Œ MSE çš„ä¼˜ç‚¹
    - å°è¯¯å·®: ä½¿ç”¨ L2 (å¹³æ»‘æ¢¯åº¦)
    - å¤§è¯¯å·®: ä½¿ç”¨ L1 (å¯¹å¼‚å¸¸å€¼é²æ£’)
    """
    def __init__(self, delta=1.0):
        super().__init__()
        self.delta = delta
    
    def forward(self, pred, true):
        error = torch.abs(pred - true)
        
        # å°è¯¯å·®: 0.5 * error^2
        quadratic = 0.5 * error ** 2
        
        # å¤§è¯¯å·®: delta * (error - 0.5*delta)
        linear = self.delta * (error - 0.5 * self.delta)
        
        # åˆ†æ®µå‡½æ•°
        loss = torch.where(error <= self.delta, quadratic, linear)
        return loss.mean()
```

**æ··åˆæŸå¤±æ–¹æ¡ˆ**:
```python
class HybridLoss(nn.Module):
    """
    æ··åˆæŸå¤±: Huber + MAE + MAPE
    """
    def __init__(self, alpha=0.6, beta=0.3, gamma=0.1):
        super().__init__()
        self.huber = HuberLoss(delta=1.0)
        self.alpha = alpha  # Huber æƒé‡
        self.beta = beta    # MAE æƒé‡
        self.gamma = gamma  # MAPE æƒé‡
    
    def forward(self, pred, true, null_val=0.0):
        # æ©ç 
        mask = (true != null_val).float()
        
        # Huber Loss
        huber = self.huber(pred * mask, true * mask)
        
        # MAE
        mae = torch.abs(pred - true) * mask
        mae = mae.sum() / mask.sum()
        
        # MAPE
        mape = torch.abs((pred - true) / (true + 1e-5)) * mask
        mape = mape.sum() / mask.sum()
        
        return self.alpha * huber + self.beta * mae + self.gamma * mape
```

---

## ğŸ”¥ æ¨èçš„æ”¹è¿›ä¼˜å…ˆçº§

### â­â­â­â­â­ æœ€é«˜ä¼˜å…ˆçº§: å¼‚è´¨æ€§èŠ‚ç‚¹åµŒå…¥
**å®æ–½éš¾åº¦**: âš¡ ä½ (åªéœ€ä¿®æ”¹ SpatialEncoder)  
**é¢„æœŸæ”¶ç›Š**: ğŸ“ˆ 5-10% MAE é™ä½  
**é£é™©**: âš ï¸ ä½ (å‚æ•°å¢åŠ å°‘)

**è¡ŒåŠ¨**: åˆ›å»º `alternating_st_heterogeneous.py`

---

### â­â­â­â­ é«˜ä¼˜å…ˆçº§: Huber Loss
**å®æ–½éš¾åº¦**: âš¡âš¡ ä½ (åªéœ€ä¿®æ”¹æŸå¤±å‡½æ•°)  
**é¢„æœŸæ”¶ç›Š**: ğŸ“ˆ 3-5% é²æ£’æ€§æå‡  
**é£é™©**: âš ï¸ æä½

**è¡ŒåŠ¨**: åœ¨ `basicts/losses/losses.py` æ·»åŠ  `HuberLoss`

---

### â­â­â­ ä¸­ç­‰ä¼˜å…ˆçº§: GCN + Transformer æ··åˆ
**å®æ–½éš¾åº¦**: âš¡âš¡âš¡ ä¸­ (éœ€è¦å®ç° GCN å±‚)  
**é¢„æœŸæ”¶ç›Š**: ğŸ“ˆ 5-8% (åˆ©ç”¨é‚»æ¥çŸ©é˜µå…ˆéªŒ)  
**é£é™©**: âš ï¸âš ï¸ ä¸­ (å¯èƒ½è¿‡æ‹Ÿåˆ)

**è¡ŒåŠ¨**: åˆ›å»º `HybridSpatialEncoder`

---

### â­â­ ä½ä¼˜å…ˆçº§: è®¡åˆ’é‡‡æ ·
**å®æ–½éš¾åº¦**: âš¡âš¡âš¡âš¡ é«˜ (éœ€è¦ä¿®æ”¹è®­ç»ƒé€»è¾‘)  
**é¢„æœŸæ”¶ç›Š**: ğŸ“ˆ 2-5% (å‡å°‘è®­ç»ƒ-æµ‹è¯•å·®å¼‚)  
**é£é™©**: âš ï¸âš ï¸âš ï¸ é«˜ (å¯èƒ½è®­ç»ƒä¸ç¨³å®š)

**è¡ŒåŠ¨**: æœ€åå°è¯•,éœ€è¦å¤§é‡è°ƒå‚

---

## ğŸ“ å®æ–½å»ºè®®

### å¿«é€ŸéªŒè¯æ–¹æ¡ˆ (1-2 å¤©)
1. âœ… **æ·»åŠ  Huber Loss** (1 å°æ—¶)
2. âœ… **å¼‚è´¨æ€§èŠ‚ç‚¹åµŒå…¥** (3-4 å°æ—¶)
3. âœ… **å¯¹æ¯”å®éªŒ** (PEMS03, 10 epochs)

**é¢„æœŸ**: MAE ä» 5.4 é™è‡³ **4.8-5.0**

### å®Œæ•´æ”¹è¿›æ–¹æ¡ˆ (1 å‘¨)
1. âœ… Huber Loss
2. âœ… å¼‚è´¨æ€§èŠ‚ç‚¹åµŒå…¥
3. âœ… GCN + Transformer æ··åˆç©ºé—´ç¼–ç 
4. âœ… å®Œæ•´è®­ç»ƒ (150 epochs, 4 ä¸ªæ•°æ®é›†)

**é¢„æœŸ**: MAE ä» 5.4 é™è‡³ **4.2-4.5** (è¾¾åˆ° SOTA æ°´å¹³)

---

## ğŸ¯ æ ¸å¿ƒç»“è®º

### HimNet ç»™æˆ‘ä»¬çš„å¯ç¤º:
1. **ç®€æ´æ¶æ„ + å…³é”®åˆ›æ–°** > å¤æ‚æ¶æ„
   - HimNet ä¹Ÿæ˜¯åŒç¼–ç å™¨ (ç®€å•)
   - ä½†å¼•å…¥èŠ‚ç‚¹å¼‚è´¨æ€§ (å…³é”®åˆ›æ–°)

2. **åˆ©ç”¨é¢†åŸŸçŸ¥è¯†**
   - é‚»æ¥çŸ©é˜µ (GCN)
   - èŠ‚ç‚¹ç±»å‹å·®å¼‚ (å…ƒåµŒå…¥)

3. **è®­ç»ƒæŠ€å·§å¾ˆé‡è¦**
   - æŸå¤±å‡½æ•°é€‰æ‹© (Huber)
   - é‡‡æ ·ç­–ç•¥ (Scheduled Sampling)

### æˆ‘ä»¬çš„ä¼˜åŠ¿:
- âœ… **äº¤æ›¿æ¶æ„ç†è®ºä¸Šä¼˜äºå¹¶è¡Œ** (æœ‰ä¿¡æ¯æµåŠ¨)
- âœ… **å‚æ•°æ›´å°‘** (1.23M vs 3.2M)
- âœ… **å·²éªŒè¯çš„åŸºçº¿** (MAE 5.4)

### ä¸‹ä¸€æ­¥:
**ä¸éœ€è¦é‡æ–°è®¾è®¡æ¶æ„!**  
åªéœ€åœ¨ Phase 1 åŸºç¡€ä¸Šå¢åŠ :
1. å¼‚è´¨æ€§èŠ‚ç‚¹åµŒå…¥ (å°ä¿®æ”¹)
2. Huber Loss (å°ä¿®æ”¹)
3. (å¯é€‰) GCN æ··åˆ (ä¸­ç­‰ä¿®æ”¹)

é¢„æœŸ: **ä» MAE 5.4 â†’ 4.2-4.5** (20-25% æå‡)
