"""
ç®€åŒ–ç‰ˆPatchEmbeddingæµ‹è¯•è„šæœ¬ï¼ˆä¸ä¾èµ–torchï¼‰
"""

def test_patch_embedding_logic():
    """æµ‹è¯•patch embeddingçš„é€»è¾‘å’Œç»´åº¦å˜æ¢"""
    
    print("ğŸ§ª æµ‹è¯•ç®€åŒ–ç‰ˆPatchEmbeddingé€»è¾‘")
    print("=" * 50)
    
    # æ¨¡æ‹Ÿæ‚¨çš„æ•°æ®ç»´åº¦
    B, L, N, C = 4, 864, 358, 1
    patch_size = 12
    embed_dim = 96
    
    print(f"ğŸ“Š è¾“å…¥å‚æ•°:")
    print(f"  - Batch size (B): {B}")
    print(f"  - åºåˆ—é•¿åº¦ (L): {L}")  
    print(f"  - èŠ‚ç‚¹æ•° (N): {N}")
    print(f"  - ç‰¹å¾ç»´åº¦ (C): {C}")
    print(f"  - Patch size: {patch_size}")
    print(f"  - Embed dim: {embed_dim}")
    
    # è®¡ç®—patchæ•°é‡
    if L % patch_size != 0:
        print(f"âŒ é”™è¯¯: åºåˆ—é•¿åº¦{L}ä¸èƒ½è¢«patch_size{patch_size}æ•´é™¤")
        return False
    
    P = L // patch_size
    print(f"  - è®¡ç®—å‡ºçš„patchæ•°é‡ (P): {P}")
    
    # æ¨¡æ‹Ÿç»´åº¦å˜æ¢è¿‡ç¨‹
    print(f"\nğŸ”„ ç»´åº¦å˜æ¢è¿‡ç¨‹:")
    print(f"  1. åŸå§‹è¾“å…¥: ({B}, {L}, {N}, {C})")
    
    # Step 1: æ·»åŠ Kç»´åº¦
    step1_shape = (B, L, N, 1, C)
    print(f"  2. æ·»åŠ Kç»´åº¦: {step1_shape}")
    
    # Step 2: ç»´åº¦é‡æ’ä¸ºConv3dæ ¼å¼
    step2_shape = (B, C, L, N, 1)
    print(f"  3. é‡æ’ä¸ºConv3dæ ¼å¼: {step2_shape}")
    
    # Step 3: Conv3d patch embedding
    # kernel_size=(patch_size, 1, 1), stride=(patch_size, 1, 1)
    step3_shape = (B, embed_dim, P, N, 1)
    print(f"  4. Conv3d patch embedding: {step3_shape}")
    
    # Step 4: ç§»é™¤Kç»´åº¦
    final_shape = (B, embed_dim, P, N)
    print(f"  5. æœ€ç»ˆè¾“å‡º: {final_shape}")
    
    # éªŒè¯è®¡ç®—
    print(f"\nâœ… éªŒè¯ç»“æœ:")
    print(f"  - æ—¶é—´å‹ç¼©æ¯”: {L} -> {P} (å‹ç¼©{L//P}å€)")
    print(f"  - ç‰¹å¾æ‰©å±•: {C} -> {embed_dim} (æ‰©å±•{embed_dim//C}å€)")
    print(f"  - ç©ºé—´ç»´åº¦ä¸å˜: {N}")
    print(f"  - Batchç»´åº¦ä¸å˜: {B}")
    
    # è®¡ç®—å‚æ•°å’Œè®¡ç®—é‡
    print(f"\nğŸ“Š æ¨¡å‹åˆ†æ:")
    conv_params = C * embed_dim * patch_size * 1 * 1
    print(f"  - Conv3då‚æ•°é‡: {conv_params:,}")
    
    input_elements = B * L * N * C
    output_elements = B * embed_dim * P * N
    print(f"  - è¾“å…¥å…ƒç´ æ•°: {input_elements:,}")
    print(f"  - è¾“å‡ºå…ƒç´ æ•°: {output_elements:,}")
    print(f"  - è¾“å‡º/è¾“å…¥æ¯”ä¾‹: {output_elements/input_elements:.2f}")
    
    return True

def demonstrate_usage():
    """æ¼”ç¤ºåœ¨AGPSTä¸­çš„ä½¿ç”¨æ–¹å¼"""
    
    print(f"\nğŸ”§ åœ¨AGPSTæ¨¡å‹ä¸­çš„ä½¿ç”¨:")
    print("=" * 50)
    
    print("""
# 1. åˆ›å»ºPatchEmbeddingå±‚
patch_embedding = PatchEmbedding(
    patch_size=12,       # 12ä¸ªæ—¶é—´æ­¥ä¸º1ä¸ªpatch (1å°æ—¶)
    in_channel=1,        # è¾“å…¥ç‰¹å¾ç»´åº¦ (æµé‡å€¼)
    embed_dim=96,        # åµŒå…¥ç»´åº¦
    norm_layer=nn.LayerNorm(96)  # å¯é€‰çš„å½’ä¸€åŒ–
)

# 2. åœ¨æ¨¡å‹forwardä¸­ä½¿ç”¨
def encoding(self, long_term_history, ...):
    # è¾“å…¥: (B=4, L=864, N=358, C=1)
    patches = self.patch_embedding(long_term_history)
    # è¾“å‡º: (B=4, embed_dim=96, P=72, N=358)
    
    # è½¬æ¢ä¸ºAdaptiveGraphLearnerè¾“å…¥æ ¼å¼
    B, D, P, N = patches.shape
    patches_for_graph = patches.permute(0, 2, 3, 1)  # (B, P, N, D)
    
    # ä½¿ç”¨åŠ¨æ€å›¾å­¦ä¹ 
    enhanced_patches, learned_adj = self.dynamic_graph_conv(patches_for_graph)
    
    # è½¬å›transformeræ ¼å¼ç»§ç»­å¤„ç†
    patches = enhanced_patches.permute(0, 3, 1, 2)  # (B, D, P, N)
    patches = patches.permute(0, 2, 1, 3)  # (B, P, D, N) -> (B, N, P, D)
    
    # ç»§ç»­transformerç¼–ç ...
""")

def compare_with_multiscale():
    """å¯¹æ¯”å¤šå°ºåº¦å’Œå•å°ºåº¦çš„åŒºåˆ«"""
    
    print(f"\nğŸ“‹ å¤šå°ºåº¦ vs å•å°ºåº¦å¯¹æ¯”:")
    print("=" * 50)
    
    B, L, N, C = 4, 864, 358, 1
    embed_dim = 96
    
    print("å¤šå°ºåº¦ç‰ˆæœ¬:")
    patch_sizes = [6, 12, 24]
    for p_size in patch_sizes:
        P = L // p_size
        params = C * (embed_dim // len(patch_sizes)) * p_size
        print(f"  - Patch size {p_size}: P={P}, å‚æ•°={params}")
    
    total_multi_params = sum(C * (embed_dim // len(patch_sizes)) * p for p in patch_sizes)
    print(f"  æ€»å‚æ•°: {total_multi_params}")
    
    print(f"\nå•å°ºåº¦ç‰ˆæœ¬:")
    patch_size = 12
    P = L // patch_size  
    params = C * embed_dim * patch_size
    print(f"  - Patch size {patch_size}: P={P}, å‚æ•°={params}")
    
    print(f"\nä¼˜åŠ¿åˆ†æ:")
    print(f"  âœ… å•å°ºåº¦æ›´ç®€æ´: ä»£ç è¡Œæ•°å‡å°‘çº¦70%")
    print(f"  âœ… å‚æ•°æ•ˆç‡æ›´é«˜: {params} vs {total_multi_params}")
    print(f"  âœ… è®¡ç®—æ›´å¿«: åªéœ€1æ¬¡å·ç§¯ vs {len(patch_sizes)}æ¬¡å·ç§¯")
    print(f"  âœ… å†…å­˜å ç”¨æ›´å°‘: æ— éœ€å­˜å‚¨å¤šä¸ªå°ºåº¦çš„ä¸­é—´ç»“æœ")
    print(f"  âœ… è°ƒè¯•æ›´å®¹æ˜“: ç»´åº¦å˜æ¢è·¯å¾„æ›´æ¸…æ™°")

if __name__ == "__main__":
    print("ğŸš€ ç®€åŒ–ç‰ˆPatchEmbeddingåˆ†æ\n")
    
    if test_patch_embedding_logic():
        demonstrate_usage()
        compare_with_multiscale()
        
        print(f"\nğŸ‰ ç®€åŒ–å®Œæˆ!")
        print("ä¸»è¦æ”¹è¿›:")
        print("1. âœ… ç§»é™¤äº†å¤æ‚çš„å¤šå°ºåº¦é€»è¾‘")
        print("2. âœ… ç›´æ¥é€‚é…(B,L,N,C)è¾“å…¥æ ¼å¼") 
        print("3. âœ… è¾“å‡ºæ ¼å¼æ›´é€‚åˆAdaptiveGraphLearner")
        print("4. âœ… ä»£ç æ›´æ˜“ç†è§£å’Œç»´æŠ¤")
        print("5. âœ… è®¡ç®—æ•ˆç‡æ›´é«˜")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥")