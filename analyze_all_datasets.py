"""
æ‰¹é‡å™ªå£°åˆ†æ - å¯¹æ‰€æœ‰æ•°æ®é›†è¿›è¡Œå™ªå£°åˆ†æ
"""
import numpy as np
import matplotlib.pyplot as plt
from scipy import signal, stats
from scipy.fft import fft, fftfreq
import os
import glob
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False


def load_dataset(dataset_name, mode='train'):
    """åŠ è½½æ•°æ®é›†"""
    data_path = f'datasets/{dataset_name}/{mode}_data.npy'
    
    if not os.path.exists(data_path):
        print(f"  âš ï¸  æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return None
    
    data = np.load(data_path)
    print(f"  âœ… åŠ è½½æ•°æ®: {data_path}")
    print(f"     å½¢çŠ¶: {data.shape} (T={data.shape[0]}, N={data.shape[1]})")
    
    return data


def estimate_snr(data, window_size=50):
    """
    ä¼°è®¡ä¿¡å™ªæ¯” (Signal-to-Noise Ratio)
    ä½¿ç”¨æ»‘åŠ¨çª—å£ä¼°è®¡å±€éƒ¨ä¿¡å·å’Œå™ªå£°
    """
    T, N = data.shape
    snr_values = np.zeros(N)
    
    for node in range(N):
        time_series = data[:, node]
        
        # ä½¿ç”¨ç§»åŠ¨å¹³å‡ä½œä¸ºä¿¡å·
        signal_estimate = np.convolve(time_series, np.ones(window_size)/window_size, mode='same')
        
        # å™ªå£° = åŸå§‹æ•°æ® - ä¿¡å·
        noise = time_series - signal_estimate
        
        # è®¡ç®—ä¿¡å·åŠŸç‡å’Œå™ªå£°åŠŸç‡
        signal_power = np.mean(signal_estimate ** 2)
        noise_power = np.mean(noise ** 2)
        
        # SNR (dB)
        if noise_power > 0:
            snr_values[node] = 10 * np.log10(signal_power / noise_power)
        else:
            snr_values[node] = 100  # éå¸¸å¹²å‡€
    
    return snr_values


def analyze_frequency_spectrum(data, sample_rate=1.0, num_nodes=10):
    """åˆ†æé¢‘è°± - æ£€æµ‹é«˜é¢‘å™ªå£°"""
    T, N = data.shape
    
    # éšæœºé€‰æ‹©å‡ ä¸ªèŠ‚ç‚¹
    sample_nodes = np.random.choice(N, min(num_nodes, N), replace=False)
    
    all_freqs = []
    all_power = []
    
    for node in sample_nodes:
        time_series = data[:, node]
        
        # FFT
        yf = fft(time_series)
        xf = fftfreq(T, 1/sample_rate)[:T//2]
        power = 2.0/T * np.abs(yf[:T//2])
        
        all_freqs.append(xf)
        all_power.append(power)
    
    # å¹³å‡é¢‘è°±
    avg_power = np.mean(all_power, axis=0)
    
    return xf, avg_power


def detect_outliers(data, method='iqr'):
    """
    æ£€æµ‹å¼‚å¸¸å€¼
    method: 'iqr' (å››åˆ†ä½è·) æˆ– 'zscore' (Zåˆ†æ•°)
    """
    T, N = data.shape
    
    if method == 'iqr':
        # IQRæ–¹æ³•
        q1 = np.percentile(data, 25, axis=0)
        q3 = np.percentile(data, 75, axis=0)
        iqr = q3 - q1
        
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        
        outliers = (data < lower_bound) | (data > upper_bound)
        
    elif method == 'zscore':
        # Z-scoreæ–¹æ³•
        mean = np.mean(data, axis=0)
        std = np.std(data, axis=0)
        z_scores = np.abs((data - mean) / (std + 1e-8))
        
        outliers = z_scores > 3
    
    outlier_ratio = (outliers.sum() / outliers.size) * 100
    
    return outliers, outlier_ratio


def analyze_autocorrelation(data, max_lag=50):
    """åˆ†æè‡ªç›¸å…³å‡½æ•°"""
    T, N = data.shape
    
    # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„è‡ªç›¸å…³
    all_acf = []
    
    for node in range(N):
        time_series = data[:, node]
        
        # æ ‡å‡†åŒ–
        ts_norm = (time_series - np.mean(time_series)) / (np.std(time_series) + 1e-8)
        
        # è®¡ç®—è‡ªç›¸å…³
        acf = np.correlate(ts_norm, ts_norm, mode='full')[len(ts_norm)-1:]
        acf = acf[:max_lag+1] / acf[0]
        
        all_acf.append(acf)
    
    # å¹³å‡è‡ªç›¸å…³
    avg_acf = np.mean(all_acf, axis=0)
    
    return avg_acf


def plot_comprehensive_analysis(data, dataset_name, save_dir='figure'):
    """
    ç”Ÿæˆç»¼åˆå™ªå£°åˆ†ææŠ¥å‘Šï¼ˆ4åˆ1å›¾è¡¨ï¼‰
    
    Args:
        data: æ•°æ®æ•°ç»„
        dataset_name: æ•°æ®é›†åç§°
        save_dir: ä¿å­˜ç›®å½•
    """
    fig = plt.figure(figsize=(16, 10))
    
    # 1. SNRåˆ†å¸ƒ
    ax1 = plt.subplot(2, 2, 1)
    snr_values = estimate_snr(data)
    
    # åˆ›å»ºçƒ­å›¾æ˜¾ç¤º
    T, N = data.shape
    n_rows = int(np.sqrt(N))
    n_cols = int(np.ceil(N / n_rows))
    snr_grid = np.full((n_rows, n_cols), np.nan)
    snr_grid.flat[:N] = snr_values
    
    im1 = ax1.imshow(snr_grid, cmap='RdYlGn', aspect='auto', vmin=0, vmax=30)
    ax1.set_title(f'{dataset_name} - å„èŠ‚ç‚¹ä¿¡å™ªæ¯”(SNR)åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    ax1.set_xlabel('åˆ—ç´¢å¼•', fontsize=10)
    ax1.set_ylabel('è¡Œç´¢å¼•', fontsize=10)
    plt.colorbar(im1, ax=ax1, label='SNR (dB)')
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    avg_snr = np.mean(snr_values)
    ax1.text(0.02, 0.98, f'å¹³å‡SNR: {avg_snr:.2f} dB', 
             transform=ax1.transAxes, fontsize=10,
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    # 2. é¢‘è°±åˆ†æ
    ax2 = plt.subplot(2, 2, 2)
    freqs, power = analyze_frequency_spectrum(data)
    
    ax2.semilogy(freqs, power, linewidth=1.5, color='steelblue')
    ax2.set_title(f'{dataset_name} - å¹³å‡åŠŸç‡è°±å¯†åº¦', fontsize=12, fontweight='bold')
    ax2.set_xlabel('é¢‘ç‡', fontsize=10)
    ax2.set_ylabel('åŠŸç‡ (å¯¹æ•°å°ºåº¦)', fontsize=10)
    ax2.grid(True, alpha=0.3)
    
    # æ ‡è®°é«˜é¢‘åŒºåŸŸ
    high_freq_threshold = freqs.max() * 0.3
    high_freq_idx = freqs > high_freq_threshold
    high_freq_power = power[high_freq_idx].sum()
    total_power = power.sum()
    high_freq_ratio = high_freq_power / total_power * 100
    
    ax2.axvline(high_freq_threshold, color='red', linestyle='--', linewidth=1.5, 
                label=f'é«˜é¢‘åŒº (>{high_freq_threshold:.2f})')
    ax2.text(0.02, 0.98, f'é«˜é¢‘èƒ½é‡å æ¯”: {high_freq_ratio:.2f}%', 
             transform=ax2.transAxes, fontsize=10,
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    ax2.legend(fontsize=9)
    
    # 3. å¼‚å¸¸å€¼æ£€æµ‹
    ax3 = plt.subplot(2, 2, 3)
    outliers, outlier_ratio = detect_outliers(data, method='zscore')
    
    # æ˜¾ç¤ºå¼‚å¸¸å€¼åˆ†å¸ƒ
    outlier_counts = outliers.sum(axis=0)
    ax3.bar(range(N), outlier_counts, color='coral', alpha=0.7, edgecolor='black')
    ax3.set_title(f'{dataset_name} - å„èŠ‚ç‚¹å¼‚å¸¸å€¼æ•°é‡', fontsize=12, fontweight='bold')
    ax3.set_xlabel('èŠ‚ç‚¹ç´¢å¼•', fontsize=10)
    ax3.set_ylabel('å¼‚å¸¸å€¼æ•°é‡', fontsize=10)
    ax3.grid(True, alpha=0.3, axis='y')
    
    ax3.text(0.02, 0.98, f'æ€»å¼‚å¸¸å€¼æ¯”ä¾‹: {outlier_ratio:.2f}%', 
             transform=ax3.transAxes, fontsize=10,
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    # 4. è‡ªç›¸å…³åˆ†æ
    ax4 = plt.subplot(2, 2, 4)
    acf = analyze_autocorrelation(data)
    lags = range(len(acf))
    
    ax4.plot(lags, acf, linewidth=2, color='steelblue', marker='o', markersize=4)
    ax4.axhline(0, color='black', linestyle='-', linewidth=0.8)
    ax4.axhline(0.5, color='red', linestyle='--', linewidth=1, alpha=0.5, label='ä¸­ç­‰ç›¸å…³')
    ax4.set_title(f'{dataset_name} - å¹³å‡è‡ªç›¸å…³å‡½æ•°', fontsize=12, fontweight='bold')
    ax4.set_xlabel('æ»åé‡', fontsize=10)
    ax4.set_ylabel('ç›¸å…³ç³»æ•°', fontsize=10)
    ax4.grid(True, alpha=0.3)
    ax4.legend(fontsize=9)
    
    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªä½ç›¸å…³çš„æ»å
    low_corr_lag = np.argmax(np.abs(acf[1:]) < 0.3) + 1 if any(np.abs(acf[1:]) < 0.3) else len(acf)
    ax4.text(0.02, 0.98, f'å¿«é€Ÿè¡°å‡æ»å: {low_corr_lag}', 
             transform=ax4.transAxes, fontsize=10,
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    
    # ä¿å­˜æ–‡ä»¶ååŒ…å«æ•°æ®é›†åç§°
    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, f'noise_analysis_{dataset_name}.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\n  ğŸ“Š åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {save_path}")
    plt.close()
    
    return {
        'dataset': dataset_name,
        'avg_snr': avg_snr,
        'high_freq_ratio': high_freq_ratio,
        'outlier_ratio': outlier_ratio,
        'low_corr_lag': low_corr_lag
    }


def generate_recommendation(metrics):
    """æ ¹æ®æŒ‡æ ‡ç”Ÿæˆå»ºè®®"""
    dataset = metrics['dataset']
    avg_snr = metrics['avg_snr']
    high_freq_ratio = metrics['high_freq_ratio']
    outlier_ratio = metrics['outlier_ratio']
    
    print(f"\n  ğŸ“‹ ã€{dataset}ã€‘å™ªå£°åˆ†ææŠ¥å‘Š:")
    print(f"     â€¢ å¹³å‡ä¿¡å™ªæ¯”: {avg_snr:.2f} dB")
    print(f"     â€¢ é«˜é¢‘èƒ½é‡å æ¯”: {high_freq_ratio:.2f}%")
    print(f"     â€¢ å¼‚å¸¸å€¼æ¯”ä¾‹: {outlier_ratio:.2f}%")
    
    # è¯„åˆ†ç³»ç»Ÿ
    score = 0
    
    if avg_snr < 15:
        score += 2
    elif avg_snr < 20:
        score += 1
    
    if high_freq_ratio > 15:
        score += 2
    elif high_freq_ratio > 10:
        score += 1
    
    if outlier_ratio > 5:
        score += 2
    elif outlier_ratio > 2:
        score += 1
    
    print(f"     â€¢ å™ªå£°è¯„åˆ†: {score}/6", end="")
    
    # æ¨èæ–¹æ¡ˆ
    if score >= 4:
        print(" ğŸ”´ ä¸¥é‡")
        print(f"     ğŸ’¡ æ¨è: ä½¿ç”¨æ³¨æ„åŠ›å»å™ª (denoise_type='attention')")
    elif score >= 2:
        print(" ğŸŸ¡ ä¸­ç­‰")
        print(f"     ğŸ’¡ æ¨è: ä½¿ç”¨å·ç§¯å»å™ª (denoise_type='conv')")
    else:
        print(" ğŸŸ¢ è‰¯å¥½")
        print(f"     ğŸ’¡ æ¨è: å¯ä»¥ä¸ä½¿ç”¨å»å™ªï¼Œæˆ–åšå¯¹æ¯”å®éªŒ")
    
    return score


def find_all_datasets():
    """æŸ¥æ‰¾æ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†"""
    datasets_dir = 'datasets'
    
    if not os.path.exists(datasets_dir):
        print(f"âŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {datasets_dir}")
        return []
    
    # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«train_data.npyçš„å­ç›®å½•
    dataset_names = []
    for item in os.listdir(datasets_dir):
        item_path = os.path.join(datasets_dir, item)
        if os.path.isdir(item_path):
            train_file = os.path.join(item_path, 'train_data.npy')
            if os.path.exists(train_file):
                dataset_names.append(item)
    
    return sorted(dataset_names)


def create_summary_report(all_metrics, save_dir='figure'):
    """åˆ›å»ºæ±‡æ€»å¯¹æ¯”æŠ¥å‘Š"""
    if not all_metrics:
        return
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 10))
    
    datasets = [m['dataset'] for m in all_metrics]
    snrs = [m['avg_snr'] for m in all_metrics]
    high_freqs = [m['high_freq_ratio'] for m in all_metrics]
    outliers = [m['outlier_ratio'] for m in all_metrics]
    
    # 1. SNRå¯¹æ¯”
    ax1 = axes[0, 0]
    bars1 = ax1.bar(datasets, snrs, color='steelblue', alpha=0.7, edgecolor='black')
    ax1.set_ylabel('å¹³å‡SNR (dB)', fontsize=11)
    ax1.set_title('å„æ•°æ®é›†å¹³å‡ä¿¡å™ªæ¯”å¯¹æ¯”', fontsize=12, fontweight='bold')
    ax1.grid(True, alpha=0.3, axis='y')
    ax1.tick_params(axis='x', rotation=45)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar in bars1:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}', ha='center', va='bottom', fontsize=9)
    
    # 2. é«˜é¢‘èƒ½é‡å¯¹æ¯”
    ax2 = axes[0, 1]
    bars2 = ax2.bar(datasets, high_freqs, color='coral', alpha=0.7, edgecolor='black')
    ax2.set_ylabel('é«˜é¢‘èƒ½é‡å æ¯” (%)', fontsize=11)
    ax2.set_title('å„æ•°æ®é›†é«˜é¢‘èƒ½é‡å æ¯”å¯¹æ¯”', fontsize=12, fontweight='bold')
    ax2.grid(True, alpha=0.3, axis='y')
    ax2.tick_params(axis='x', rotation=45)
    
    for bar in bars2:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}%', ha='center', va='bottom', fontsize=9)
    
    # 3. å¼‚å¸¸å€¼æ¯”ä¾‹å¯¹æ¯”
    ax3 = axes[1, 0]
    bars3 = ax3.bar(datasets, outliers, color='lightgreen', alpha=0.7, edgecolor='black')
    ax3.set_ylabel('å¼‚å¸¸å€¼æ¯”ä¾‹ (%)', fontsize=11)
    ax3.set_title('å„æ•°æ®é›†å¼‚å¸¸å€¼æ¯”ä¾‹å¯¹æ¯”', fontsize=12, fontweight='bold')
    ax3.grid(True, alpha=0.3, axis='y')
    ax3.tick_params(axis='x', rotation=45)
    
    for bar in bars3:
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.2f}%', ha='center', va='bottom', fontsize=9)
    
    # 4. ç»¼åˆè¯„åˆ†é›·è¾¾å›¾
    ax4 = axes[1, 1]
    
    # å½’ä¸€åŒ–æŒ‡æ ‡ç”¨äºé›·è¾¾å›¾
    snrs_norm = [(30 - s) / 30 * 100 for s in snrs]  # åè½¬ï¼Œè¶Šä½è¶Šå¥½
    
    x = np.arange(len(datasets))
    width = 0.25
    
    ax4.bar(x - width, snrs_norm, width, label='SNRæŒ‡æ ‡', alpha=0.7)
    ax4.bar(x, high_freqs, width, label='é«˜é¢‘å™ªå£°', alpha=0.7)
    ax4.bar(x + width, outliers, width, label='å¼‚å¸¸å€¼', alpha=0.7)
    
    ax4.set_ylabel('æŒ‡æ ‡å€¼', fontsize=11)
    ax4.set_title('å™ªå£°ç»¼åˆæŒ‡æ ‡å¯¹æ¯”', fontsize=12, fontweight='bold')
    ax4.set_xticks(x)
    ax4.set_xticklabels(datasets, rotation=45)
    ax4.legend(fontsize=9)
    ax4.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    
    # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    summary_path = os.path.join(save_dir, 'noise_analysis_summary.png')
    plt.savefig(summary_path, dpi=300, bbox_inches='tight')
    print(f"\nğŸ“Š æ±‡æ€»å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {summary_path}")
    plt.close()


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "ğŸ”¬ " + "="*58 + " ğŸ”¬")
    print("ğŸ”¬  æ‰¹é‡å™ªå£°åˆ†æå·¥å…· - åˆ†ææ‰€æœ‰æ•°æ®é›†")
    print("ğŸ”¬ " + "="*58 + " ğŸ”¬\n")
    
    # æŸ¥æ‰¾æ‰€æœ‰æ•°æ®é›†
    print("ğŸ” æŸ¥æ‰¾å¯ç”¨æ•°æ®é›†...")
    dataset_names = find_all_datasets()
    
    if not dataset_names:
        print("\nâŒ æœªæ‰¾åˆ°ä»»ä½•æ•°æ®é›†ï¼")
        print("ğŸ’¡ è¯·ç¡®ä¿ datasets/ ç›®å½•ä¸‹æœ‰åŒ…å« train_data.npy çš„å­ç›®å½•")
        return
    
    print(f"\nâœ… æ‰¾åˆ° {len(dataset_names)} ä¸ªæ•°æ®é›†:")
    for i, name in enumerate(dataset_names, 1):
        print(f"   {i}. {name}")
    
    # åˆ†ææ¯ä¸ªæ•°æ®é›†
    all_metrics = []
    
    print("\n" + "="*60)
    print("ğŸ“Š å¼€å§‹æ‰¹é‡åˆ†æ...")
    print("="*60)
    
    for i, dataset_name in enumerate(dataset_names, 1):
        print(f"\n[{i}/{len(dataset_names)}] åˆ†ææ•°æ®é›†: {dataset_name}")
        print("-" * 60)
        
        # åŠ è½½æ•°æ®
        data = load_dataset(dataset_name, mode='train')
        
        if data is None:
            print(f"  âš ï¸  è·³è¿‡ {dataset_name}")
            continue
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        metrics = plot_comprehensive_analysis(data, dataset_name)
        
        # ç”Ÿæˆå»ºè®®
        score = generate_recommendation(metrics)
        
        all_metrics.append(metrics)
    
    # åˆ›å»ºæ±‡æ€»æŠ¥å‘Š
    if all_metrics:
        print("\n" + "="*60)
        print("ğŸ“Š ç”Ÿæˆæ±‡æ€»å¯¹æ¯”æŠ¥å‘Š...")
        print("="*60)
        create_summary_report(all_metrics)
    
    # æ‰“å°æ€»ç»“
    print("\n" + "="*60)
    print("âœ… æ‰¹é‡åˆ†æå®Œæˆï¼")
    print("="*60)
    
    print(f"\nğŸ“‚ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   â€¢ ä¸ªåˆ«æ•°æ®é›†æŠ¥å‘Š: figure/noise_analysis_<æ•°æ®é›†å>.png")
    print(f"   â€¢ æ±‡æ€»å¯¹æ¯”æŠ¥å‘Š: figure/noise_analysis_summary.png")
    
    print("\nğŸ’¡ å»ºè®®é…ç½®æ€»ç»“:")
    print("-" * 60)
    for metrics in all_metrics:
        dataset = metrics['dataset']
        avg_snr = metrics['avg_snr']
        
        if avg_snr < 15:
            denoise = "attention"
            icon = "ğŸ”´"
        elif avg_snr < 20:
            denoise = "conv"
            icon = "ğŸŸ¡"
        else:
            denoise = "None (å¯é€‰)"
            icon = "ğŸŸ¢"
        
        print(f"  {icon} {dataset:15s} â†’ denoise_type: '{denoise}'")
    
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("   1. æŸ¥çœ‹å„æ•°æ®é›†çš„è¯¦ç»†åˆ†ææŠ¥å‘Š")
    print("   2. æŸ¥çœ‹æ±‡æ€»å¯¹æ¯”æŠ¥å‘Šäº†è§£æ•´ä½“æƒ…å†µ")
    print("   3. æ ¹æ®å»ºè®®é…ç½®å„æ•°æ®é›†çš„å»å™ªå‚æ•°")
    print("   4. è¿è¡Œå¯¹æ¯”å®éªŒéªŒè¯å»å™ªæ•ˆæœ\n")


if __name__ == '__main__':
    main()
