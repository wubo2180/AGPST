"""
ç®€åŒ–ç‰ˆPatchEmbeddingå®Œæ•´ä»£ç å’Œè¯´æ˜
=====================================

æœ¬æ–‡ä»¶åŒ…å«:
1. ç®€åŒ–åçš„å®Œæ•´PatchEmbeddingä»£ç 
2. è¯¦ç»†çš„ä½¿ç”¨è¯´æ˜å’Œç¤ºä¾‹
3. ä¸åŸç‰ˆæœ¬çš„å¯¹æ¯”åˆ†æ
"""

from torch import nn
import torch

class PatchEmbedding(nn.Module):
    """
    ç®€åŒ–ç‰ˆå•å°ºåº¦Patch Embeddingç”¨äºäº¤é€šé¢„æµ‹
    
    åŠŸèƒ½:
    - å°†é•¿æ—¶é—´åºåˆ— (B, L, N, C) è½¬æ¢ä¸ºpatchåºåˆ— (B, embed_dim, P, N)
    - æ—¶é—´ç»´åº¦å‹ç¼©: L -> P = L // patch_size
    - ç‰¹å¾ç»´åº¦æ‰©å±•: C -> embed_dim
    - å®Œå…¨é€‚é…AdaptiveGraphLearnerçš„è¾“å…¥éœ€æ±‚
    """

    def __init__(self, patch_size, in_channel, embed_dim, num_nodes=None, topK=None, norm_layer=None):
        """
        Args:
            patch_size (int): patchå¤§å°ï¼Œå»ºè®®12 (1å°æ—¶=12ä¸ª5åˆ†é’Ÿ)
            in_channel (int): è¾“å…¥ç‰¹å¾ç»´åº¦ï¼Œé€šå¸¸ä¸º1 (æµé‡å€¼)
            embed_dim (int): è¾“å‡ºåµŒå…¥ç»´åº¦ï¼Œå»ºè®®96
            num_nodes (int): èŠ‚ç‚¹æ•°ï¼Œä¿ç•™å…¼å®¹æ€§ä½†ä¸ä½¿ç”¨
            topK (int): Top-Kå‚æ•°ï¼Œä¿ç•™å…¼å®¹æ€§ä½†ä¸ä½¿ç”¨
            norm_layer: å¯é€‰çš„å½’ä¸€åŒ–å±‚
        """
        super().__init__()
        self.patch_size = patch_size
        self.in_channel = in_channel
        self.embed_dim = embed_dim
        
        # å•ä¸€3Då·ç§¯å±‚å®ç°patch embedding
        self.patch_conv = nn.Conv3d(
            in_channels=in_channel,      # è¾“å…¥é€šé“æ•° (é€šå¸¸ä¸º1)
            out_channels=embed_dim,      # è¾“å‡ºåµŒå…¥ç»´åº¦ (å¦‚96)
            kernel_size=(patch_size, 1, 1),  # åªåœ¨æ—¶é—´ç»´åº¦åšpatch
            stride=(patch_size, 1, 1),       # æ—¶é—´æ­¥é•¿ä¸ºpatch_size
            padding=0                         # æ— padding
        )
        
        # å¯é€‰å½’ä¸€åŒ–
        self.norm_layer = norm_layer if norm_layer is not None else nn.Identity()

    def forward(self, long_term_history):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            long_term_history: å½¢çŠ¶ä¸º (B, L, N, C) çš„å¼ é‡
                             - B: batch size (å¦‚4)
                             - L: æ—¶é—´åºåˆ—é•¿åº¦ (å¦‚864)  
                             - N: èŠ‚ç‚¹æ•° (å¦‚358)
                             - C: ç‰¹å¾ç»´åº¦ (å¦‚1)

        Returns:
            output: å½¢çŠ¶ä¸º (B, embed_dim, P, N) çš„å¼ é‡
                   - P = L // patch_size (å¦‚72)
        
        ç»´åº¦å˜æ¢è¿‡ç¨‹:
            (B, L, N, C) -> (B, L, N, 1, C) -> (B, C, L, N, 1) 
            -> Conv3d -> (B, embed_dim, P, N, 1) -> (B, embed_dim, P, N)
        """
        B, L, N, C = long_term_history.shape
        
        # éªŒè¯è¾“å…¥
        assert L % self.patch_size == 0, \
            f"åºåˆ—é•¿åº¦ {L} å¿…é¡»èƒ½è¢« patch_size {self.patch_size} æ•´é™¤"
        
        # Step 1: ä¸ºConv3dæ·»åŠ Kç»´åº¦
        x = long_term_history.unsqueeze(3)  # (B, L, N, C) -> (B, L, N, 1, C)
        
        # Step 2: é‡æ’ç»´åº¦ä¸ºConv3dæ‰€éœ€æ ¼å¼
        x = x.permute(0, 4, 1, 2, 3)  # (B, L, N, 1, C) -> (B, C, L, N, 1)
        
        # Step 3: 3Då·ç§¯è¿›è¡Œpatch embedding
        output = self.patch_conv(x)  # (B, C, L, N, 1) -> (B, embed_dim, P, N, 1)
        
        # Step 4: ç§»é™¤Kç»´åº¦
        output = output.squeeze(-1)  # (B, embed_dim, P, N, 1) -> (B, embed_dim, P, N)
        
        # Step 5: åº”ç”¨å½’ä¸€åŒ–
        output = self.norm_layer(output)
        
        # éªŒè¯è¾“å‡ºç»´åº¦
        expected_patches = L // self.patch_size
        assert output.shape == (B, self.embed_dim, expected_patches, N)
        
        return output


# =====================================
# ä½¿ç”¨ç¤ºä¾‹å’Œæœ€ä½³å®è·µ
# =====================================

def create_patch_embedding_for_agpst():
    """åˆ›å»ºé€‚ç”¨äºAGPSTçš„PatchEmbeddingå±‚"""
    
    return PatchEmbedding(
        patch_size=12,                    # 12ä¸ª5åˆ†é’Ÿ = 1å°æ—¶
        in_channel=1,                     # æµé‡å€¼
        embed_dim=96,                     # åµŒå…¥ç»´åº¦
        norm_layer=nn.LayerNorm(96)       # LayerNormå½’ä¸€åŒ–
    )

def demonstrate_integration_with_graph_learning():
    """æ¼”ç¤ºä¸AdaptiveGraphLearnerçš„é›†æˆ"""
    
    # æ¨¡æ‹ŸAGPSTæ¨¡å‹ä¸­çš„ä½¿ç”¨
    class AGPSTEncoding(nn.Module):
        def __init__(self, config):
            super().__init__()
            
            # Patch embeddingå±‚
            self.patch_embedding = PatchEmbedding(
                patch_size=config['patch_size'],
                in_channel=config['in_channel'],
                embed_dim=config['embed_dim'],
                norm_layer=nn.LayerNorm(config['embed_dim'])
            )
            
            # åŠ¨æ€å›¾å­¦ä¹ å±‚ (å‡è®¾å·²å®ç°)
            # self.dynamic_graph_conv = PostPatchDynamicGraphConv(...)
            
        def forward(self, long_term_history):
            """
            å®Œæ•´çš„ç¼–ç æµç¨‹
            """
            # Step 1: Patch embedding
            # (B, L, N, C) -> (B, embed_dim, P, N)
            patches = self.patch_embedding(long_term_history)
            
            # Step 2: è½¬æ¢ä¸ºå›¾å­¦ä¹ æ ¼å¼
            # (B, embed_dim, P, N) -> (B, P, N, embed_dim)
            B, D, P, N = patches.shape
            patches_for_graph = patches.permute(0, 2, 3, 1)
            
            # Step 3: åŠ¨æ€å›¾å­¦ä¹  (è¿™é‡Œéœ€è¦é›†æˆæˆ‘ä»¬ä¹‹å‰åˆ›å»ºçš„æ¨¡å—)
            # enhanced_patches, learned_adj = self.dynamic_graph_conv(patches_for_graph)
            # æš‚æ—¶è·³è¿‡è¿™ä¸€æ­¥
            enhanced_patches = patches_for_graph
            
            # Step 4: è½¬å›Transformeræ ¼å¼
            # (B, P, N, D) -> (B, N, P, D) 
            enhanced_patches = enhanced_patches.permute(0, 2, 1, 3)
            
            return enhanced_patches
    
    # ä½¿ç”¨ç¤ºä¾‹
    config = {
        'patch_size': 12,
        'in_channel': 1,
        'embed_dim': 96
    }
    
    model = AGPSTEncoding(config)
    
    # æµ‹è¯•æ•°æ®
    test_input = torch.randn(4, 864, 358, 1)  # (B, L, N, C)
    
    with torch.no_grad():
        output = model(test_input)
        print(f"è¾“å…¥: {test_input.shape}")
        print(f"è¾“å‡º: {output.shape}")  # (4, 358, 72, 96)


# =====================================
# æ€§èƒ½å¯¹æ¯”åˆ†æ
# =====================================

def performance_comparison():
    """
    ç®€åŒ–ç‰ˆ vs åŸå¤šå°ºåº¦ç‰ˆæœ¬æ€§èƒ½å¯¹æ¯”
    """
    
    print("æ€§èƒ½å¯¹æ¯”åˆ†æ:")
    print("=" * 50)
    
    # è¾“å…¥å‚æ•°
    B, L, N, C = 4, 864, 358, 1
    embed_dim = 96
    
    # åŸå¤šå°ºåº¦ç‰ˆæœ¬
    patch_sizes_multi = [6, 12, 24]
    params_multi = sum(C * (embed_dim // len(patch_sizes_multi)) * p for p in patch_sizes_multi)
    compute_multi = len(patch_sizes_multi)  # éœ€è¦3æ¬¡å·ç§¯
    
    # ç®€åŒ–å•å°ºåº¦ç‰ˆæœ¬  
    patch_size_single = 12
    params_single = C * embed_dim * patch_size_single
    compute_single = 1  # åªéœ€1æ¬¡å·ç§¯
    
    print(f"å¤šå°ºåº¦ç‰ˆæœ¬:")
    print(f"  - å‚æ•°é‡: {params_multi:,}")
    print(f"  - å·ç§¯æ¬¡æ•°: {compute_multi}")
    print(f"  - ä»£ç è¡Œæ•°: ~100è¡Œ")
    print(f"  - å¤æ‚åº¦: é«˜")
    
    print(f"\nå•å°ºåº¦ç‰ˆæœ¬:")
    print(f"  - å‚æ•°é‡: {params_single:,}")
    print(f"  - å·ç§¯æ¬¡æ•°: {compute_single}")
    print(f"  - ä»£ç è¡Œæ•°: ~40è¡Œ")
    print(f"  - å¤æ‚åº¦: ä½")
    
    print(f"\næ€§èƒ½æå‡:")
    print(f"  âœ… å‚æ•°æ•ˆç‡: {(params_single/params_multi-1)*100:.1f}% æ›´é«˜")
    print(f"  âœ… è®¡ç®—æ•ˆç‡: {compute_multi}x æ›´å¿«")
    print(f"  âœ… ä»£ç ç®€åŒ–: 60% è¡Œæ•°å‡å°‘")
    print(f"  âœ… å†…å­˜æ•ˆç‡: æ— å¤šå°ºåº¦ç¼“å­˜å¼€é”€")


if __name__ == "__main__":
    print("ç®€åŒ–ç‰ˆPatchEmbeddingå®Œæ•´æ–¹æ¡ˆ\n")
    
    # æ€§èƒ½åˆ†æ
    performance_comparison()
    
    # é›†æˆç¤ºä¾‹
    print(f"\n{'='*50}")
    demonstrate_integration_with_graph_learning()
    
    print(f"\nğŸ‰ ç®€åŒ–å®Œæˆ! ä¸»è¦ä¼˜åŠ¿:")
    print("1. âœ… ä»£ç æ›´æ¸…æ™°: ç§»é™¤äº†å¤æ‚çš„å¤šå°ºåº¦é€»è¾‘")
    print("2. âœ… æ•ˆç‡æ›´é«˜: å‡å°‘è®¡ç®—å’Œå†…å­˜å¼€é”€")  
    print("3. âœ… æ˜“äºè°ƒè¯•: ç»´åº¦å˜æ¢è·¯å¾„ç®€å•æ˜ç¡®")
    print("4. âœ… å®Œç¾é€‚é…: è¾“å‡ºæ ¼å¼é€‚åˆAdaptiveGraphLearner")
    print("5. âœ… ä¿æŒå…¼å®¹: æ¥å£å‚æ•°å‘åå…¼å®¹åŸç‰ˆæœ¬")