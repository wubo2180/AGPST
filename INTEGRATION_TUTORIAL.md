# å¦‚ä½•å°†æ–°çš„è‡ªé€‚åº”å›¾æ–¹æ³•é›†æˆåˆ°æ‚¨çš„æ¨¡å‹ä¸­

## ğŸ¯ å¿«é€Ÿé›†æˆï¼ˆ3 æ­¥å®Œæˆï¼‰

### æ­¥éª¤ 1: ä¿®æ”¹ `model.py` çš„ `__init__` æ–¹æ³•

åœ¨ `basicts/mask/model.py` æ–‡ä»¶ä¸­ï¼Œæ‰¾åˆ° `pretrain_model` çš„ `__init__` æ–¹æ³•ï¼š

**åŸæ¥çš„ä»£ç ** (ç¬¬ 18-46 è¡Œ):
```python
class pretrain_model(nn.Module):
    def __init__(self, num_nodes, dim, topK, adaptive, epochs, patch_size, 
                 in_channel, embed_dim, num_heads, mlp_ratio, dropout, mask_ratio, 
                 encoder_depth, decoder_depth, patch_sizes=None, mode="pre-train") -> None:
        super().__init__()
        # ...
        
        # åŸæ¥çš„æ–¹æ³•: ç®€å•çš„çŸ©é˜µä¹˜æ³•
        self.nodevec1 = nn.Parameter(torch.randn(num_nodes, dim), requires_grad=True)
        self.nodevec2 = nn.Parameter(torch.randn(dim, num_nodes), requires_grad=True)
        
        # ...
```

**æ–°çš„ä»£ç ** (æ¨è - Multi-Head æ–¹æ³•):
```python
class pretrain_model(nn.Module):
    def __init__(self, num_nodes, dim, topK, adaptive, epochs, patch_size, 
                 in_channel, embed_dim, num_heads, mlp_ratio, dropout, mask_ratio, 
                 encoder_depth, decoder_depth, patch_sizes=None, mode="pre-train",
                 graph_type='multihead', graph_num_heads=4) -> None:  # æ–°å¢å‚æ•°
        super().__init__()
        # ...
        
        # å¯¼å…¥æ–°çš„è‡ªé€‚åº”å›¾æ¨¡å—
        from .adaptive_graph import AdaptiveGraphFactory
        
        # åˆ›å»ºè‡ªé€‚åº”å›¾
        self.adaptive_graph = AdaptiveGraphFactory.create(
            graph_type=graph_type,        # 'multihead', 'dynamic', 'hyperbolic', etc.
            num_nodes=num_nodes,
            embed_dim=dim,
            num_heads=graph_num_heads     # ä»… multihead ä½¿ç”¨
        )
        
        # å¦‚æœéœ€è¦å…¼å®¹æ—§çš„ checkpointï¼Œä¿ç•™åŸæ¥çš„å‚æ•°
        # self.nodevec1 = nn.Parameter(torch.randn(num_nodes, dim), requires_grad=True)
        # self.nodevec2 = nn.Parameter(torch.randn(dim, num_nodes), requires_grad=True)
        
        # ...
```

---

### æ­¥éª¤ 2: ä¿®æ”¹ `forward` æ–¹æ³•

åœ¨åŒä¸€ä¸ªæ–‡ä»¶ä¸­ï¼Œæ‰¾åˆ° `forward` æ–¹æ³•ï¼ˆç¬¬ 130-150 è¡Œï¼‰:

**åŸæ¥çš„ä»£ç **:
```python
def forward(self, history_data: torch.Tensor, epoch):
    # åŸæ¥çš„æ–¹æ³•
    adp = F.softmax(F.relu(torch.mm(self.nodevec1, self.nodevec2)), dim=1)
    
    values, indices = torch.topk(adp, self.topK)
    # ...
```

**æ–°çš„ä»£ç **:
```python
def forward(self, history_data: torch.Tensor, epoch):
    # æ–°æ–¹æ³•: ä½¿ç”¨è‡ªé€‚åº”å›¾æ¨¡å—
    adp = self.adaptive_graph()  # è‡ªåŠ¨è°ƒç”¨å¯¹åº”çš„å›¾æ„å»ºæ–¹æ³•
    
    values, indices = torch.topk(adp, self.topK)
    # å…¶ä½™ä»£ç å®Œå…¨ä¸å˜
    # ...
```

**å¦‚æœä½¿ç”¨ Dynamic æ–¹æ³•** (éœ€è¦è¾“å…¥ç‰¹å¾):
```python
def forward(self, history_data: torch.Tensor, epoch):
    # Dynamic æ–¹æ³•éœ€è¦ä¼ å…¥è¾“å…¥ç‰¹å¾
    if hasattr(self.adaptive_graph, 'dynamic_encoder'):  # åˆ¤æ–­æ˜¯å¦æ˜¯ Dynamic
        adp = self.adaptive_graph(history_data)
    else:
        adp = self.adaptive_graph()
    
    values, indices = torch.topk(adp, self.topK)
    # ...
```

---

### æ­¥éª¤ 3: æ›´æ–°é…ç½®æ–‡ä»¶

åœ¨ `parameters/PEMS03_multiscale.yaml` ä¸­æ·»åŠ æ–°çš„é…ç½®é¡¹:

```yaml
# ... åŸæœ‰é…ç½® ...

# è‡ªé€‚åº”å›¾é…ç½® (æ–°å¢)
graph_type: 'multihead'      # å¯é€‰: simple, multihead, dynamic, hyperbolic, sparse
graph_num_heads: 4           # ä»… multihead ä½¿ç”¨
graph_topk: 10               # ä»… sparse ä½¿ç”¨
graph_feature_dim: 64        # ä»… dynamic ä½¿ç”¨

# mask_args ä¸­æ·»åŠ è¿™äº›å‚æ•°
mask_args:
  num_nodes: 358
  dim: 10
  topK: 6
  # ... å…¶ä»–åŸæœ‰å‚æ•° ...
  
  # æ–°å¢
  graph_type: ${graph_type}
  graph_num_heads: ${graph_num_heads}
```

---

## ğŸ”„ å®Œæ•´çš„ä¿®æ”¹ç¤ºä¾‹

### ç¤ºä¾‹ 1: ä½¿ç”¨ Multi-Head æ–¹æ³•ï¼ˆæ¨èï¼‰

**æ–‡ä»¶**: `basicts/mask/model.py`

```python
import os
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from .adaptive_graph import MultiHeadAdaptiveGraph  # æ–°å¢å¯¼å…¥

class pretrain_model(nn.Module):
    def __init__(self, num_nodes, dim, topK, adaptive, epochs, patch_size, 
                 in_channel, embed_dim, num_heads, mlp_ratio, dropout, mask_ratio, 
                 encoder_depth, decoder_depth, patch_sizes=None, mode="pre-train",
                 graph_type='multihead', graph_num_heads=4) -> None:
        super().__init__()
        assert topK < num_nodes
        
        # ... å…¶ä»–åˆå§‹åŒ–ä»£ç  ...
        
        # ============ ä¿®æ”¹è¿™é‡Œ ============
        # æ—§æ–¹æ³•:
        # self.nodevec1 = nn.Parameter(torch.randn(num_nodes, dim), requires_grad=True)
        # self.nodevec2 = nn.Parameter(torch.randn(dim, num_nodes), requires_grad=True)
        
        # æ–°æ–¹æ³•: Multi-Head Adaptive Graph
        self.adaptive_graph = MultiHeadAdaptiveGraph(
            num_nodes=num_nodes,
            embed_dim=dim,
            num_heads=graph_num_heads
        )
        # ===============================
        
        # ... å…¶ä»–åˆå§‹åŒ–ä»£ç ä¿æŒä¸å˜ ...
    
    def forward(self, history_data: torch.Tensor, epoch):
        # ============ ä¿®æ”¹è¿™é‡Œ ============
        # æ—§æ–¹æ³•:
        # adp = F.softmax(F.relu(torch.mm(self.nodevec1, self.nodevec2)), dim=1)
        
        # æ–°æ–¹æ³•:
        adp = self.adaptive_graph()
        # ===============================
        
        values, indices = torch.topk(adp, self.topK)
        
        # å…¶ä½™ä»£ç å®Œå…¨ä¸å˜
        K = self.topK
        B, L, N, C = history_data.shape
        history_data_khop = history_data.transpose(1, 2).reshape((B, N, -1))
        history_data_khop = history_data_khop[:, indices, :]
        history_data_khop = history_data_khop.reshape((B, N, K, L, C))
        history_data_khop = history_data_khop.permute(0, 3, 1, 2, 4)
        
        if self.mode == "pre-train":
            hidden_states_unmasked, unmasked_token_index, masked_token_index = self.encoding(history_data_khop, epoch, adp)
            reconstruction_full = self.decoding(hidden_states_unmasked, masked_token_index, adp)
            reconstruction_masked_tokens, label_masked_tokens = self.get_reconstructed_masked_tokens(
                reconstruction_full, history_data.permute(0, 2, 3, 1), 
                unmasked_token_index, masked_token_index
            )
            return reconstruction_masked_tokens, label_masked_tokens
        else:
            hidden_states_full, _, _ = self.encoding(history_data_khop, epoch, adp, mask=False)
            return hidden_states_full
```

---

### ç¤ºä¾‹ 2: ä½¿ç”¨ Dynamic æ–¹æ³•ï¼ˆæ€§èƒ½æœ€ä½³ï¼‰

```python
from .adaptive_graph import DynamicAdaptiveGraph  # æ–°å¢å¯¼å…¥

class pretrain_model(nn.Module):
    def __init__(self, num_nodes, dim, topK, adaptive, epochs, patch_size, 
                 in_channel, embed_dim, num_heads, mlp_ratio, dropout, mask_ratio, 
                 encoder_depth, decoder_depth, patch_sizes=None, mode="pre-train",
                 graph_feature_dim=64) -> None:
        super().__init__()
        
        # Dynamic Adaptive Graph
        self.adaptive_graph = DynamicAdaptiveGraph(
            num_nodes=num_nodes,
            embed_dim=dim,
            feature_dim=graph_feature_dim  # é€šå¸¸è®¾ä¸º in_channel æˆ– embed_dim
        )
    
    def forward(self, history_data: torch.Tensor, epoch):
        # Dynamic æ–¹æ³•éœ€è¦ä¼ å…¥è¾“å…¥ç‰¹å¾
        adp = self.adaptive_graph(history_data)  # ä¼ å…¥ history_data
        
        values, indices = torch.topk(adp, self.topK)
        # ... å…¶ä½™ä»£ç ä¸å˜ ...
```

---

### ç¤ºä¾‹ 3: ä½¿ç”¨ Hyperbolic æ–¹æ³•ï¼ˆå±‚æ¬¡ç½‘ç»œï¼‰

```python
from .adaptive_graph import HyperbolicAdaptiveGraph  # æ–°å¢å¯¼å…¥

class pretrain_model(nn.Module):
    def __init__(self, num_nodes, dim, topK, adaptive, epochs, patch_size, 
                 in_channel, embed_dim, num_heads, mlp_ratio, dropout, mask_ratio, 
                 encoder_depth, decoder_depth, patch_sizes=None, mode="pre-train",
                 graph_curv=1.0) -> None:
        super().__init__()
        
        # Hyperbolic Adaptive Graph
        self.adaptive_graph = HyperbolicAdaptiveGraph(
            num_nodes=num_nodes,
            embed_dim=dim,
            curv=graph_curv  # æ›²ç‡å‚æ•°
        )
    
    def forward(self, history_data: torch.Tensor, epoch):
        adp = self.adaptive_graph()
        
        values, indices = torch.topk(adp, self.topK)
        # ... å…¶ä½™ä»£ç ä¸å˜ ...
```

---

## ğŸ“‹ æ›´æ–° `main.py`

åœ¨ `main.py` ä¸­ï¼Œæ›´æ–° `pretrain` å‡½æ•°ä»¥ä¼ é€’æ–°å‚æ•°:

```python
def pretrain(config, args):
    print('### start pre-training ... ###')
    # ...
    
    model = pretrain_model(
        config['num_nodes'], 
        config['dim'], 
        config['topK'], 
        config['adaptive'], 
        config['pretrain_epochs'], 
        config['patch_size'], 
        config['in_channel'], 
        config['embed_dim'], 
        config['num_heads'], 
        config['mlp_ratio'], 
        config['dropout'], 
        config['mask_ratio'], 
        config['encoder_depth'], 
        config['decoder_depth'],
        
        # æ–°å¢å‚æ•°
        graph_type=config.get('graph_type', 'multihead'),
        graph_num_heads=config.get('graph_num_heads', 4)
    )
    
    # ...
```

---

## âš™ï¸ é…ç½®æ–‡ä»¶ç¤ºä¾‹

### `parameters/PEMS03_multihead.yaml` (æ¨è)

```yaml
description: 'Multi-Head Adaptive Graph'
model_name: 'AGPST-MultiHead'
dataset_name: "PEMS03"

# ... å…¶ä»–é…ç½® ...

# è‡ªé€‚åº”å›¾é…ç½®
graph_type: 'multihead'
graph_num_heads: 4

num_nodes: 358
dim: 10
topK: 6
# ...
```

### `parameters/PEMS03_dynamic.yaml` (é«˜æ€§èƒ½)

```yaml
description: 'Dynamic Adaptive Graph'
model_name: 'AGPST-Dynamic'
dataset_name: "PEMS03"

# è‡ªé€‚åº”å›¾é…ç½®
graph_type: 'dynamic'
graph_feature_dim: 64

num_nodes: 358
dim: 10
# ...
```

### `parameters/PEMS03_hyperbolic.yaml` (å±‚æ¬¡ç½‘ç»œ)

```yaml
description: 'Hyperbolic Adaptive Graph'
model_name: 'AGPST-Hyperbolic'
dataset_name: "PEMS03"

# è‡ªé€‚åº”å›¾é…ç½®
graph_type: 'hyperbolic'
graph_curv: 1.0

num_nodes: 358
dim: 10
# ...
```

---

## ğŸ§ª æµ‹è¯•ä¸åŒæ–¹æ³•

åˆ›å»ºä¸€ä¸ªå®éªŒè„šæœ¬ `test_adaptive_graphs.sh`:

```bash
#!/bin/bash

# æµ‹è¯•åŸå§‹æ–¹æ³• (baseline)
echo "Testing Simple (baseline)..."
python main.py --config parameters/PEMS03_multiscale.yaml \
    --pretrain_epochs 10 --finetune_epochs 10

# æµ‹è¯• Multi-Head
echo "Testing Multi-Head..."
python main.py --config parameters/PEMS03_multihead.yaml \
    --pretrain_epochs 10 --finetune_epochs 10

# æµ‹è¯• Dynamic
echo "Testing Dynamic..."
python main.py --config parameters/PEMS03_dynamic.yaml \
    --pretrain_epochs 10 --finetune_epochs 10

# æµ‹è¯• Hyperbolic
echo "Testing Hyperbolic..."
python main.py --config parameters/PEMS03_hyperbolic.yaml \
    --pretrain_epochs 10 --finetune_epochs 10

echo "All tests completed! Check SwanLab for results."
```

---

## ğŸ“Š åœ¨ SwanLab ä¸­å¯¹æ¯”ç»“æœ

è¿è¡Œä¸åŒæ–¹æ³•åï¼Œåœ¨ SwanLab Dashboard ä¸­:

1. **å¯¹æ¯” MAE/RMSE/MAPE**
   - æŸ¥çœ‹ä¸åŒæ–¹æ³•çš„æ€§èƒ½å·®å¼‚
   
2. **åˆ†æè®­ç»ƒæ›²çº¿**
   - è§‚å¯Ÿæ”¶æ•›é€Ÿåº¦
   - æ£€æŸ¥ç¨³å®šæ€§

3. **å¯è§†åŒ–é‚»æ¥çŸ©é˜µ**
   ```python
   # åœ¨ main.py ä¸­æ·»åŠ 
   if epoch == 0:
       adp_vis = model.adaptive_graph().detach().cpu().numpy()
       swanlab.log({"adaptive_graph": swanlab.Image(adp_vis)})
   ```

---

## âœ… æ£€æŸ¥æ¸…å•

åœ¨é›†æˆæ–°æ–¹æ³•åï¼Œç¡®ä¿:

- [ ] å¯¼å…¥äº†æ­£ç¡®çš„æ¨¡å— (`from .adaptive_graph import ...`)
- [ ] åœ¨ `__init__` ä¸­åˆ›å»ºäº† `self.adaptive_graph`
- [ ] åœ¨ `forward` ä¸­æ›¿æ¢äº† `adp` çš„è®¡ç®—
- [ ] æ›´æ–°äº†é…ç½®æ–‡ä»¶
- [ ] æ›´æ–°äº† `main.py` ä¼ é€’æ–°å‚æ•°
- [ ] ä»£ç æ²¡æœ‰è¯­æ³•é”™è¯¯ (`python -m py_compile basicts/mask/model.py`)
- [ ] è¿è¡Œä¸€ä¸ªå°å®éªŒæµ‹è¯• (`--pretrain_epochs 1`)

---

## ğŸš€ å¼€å§‹å®éªŒï¼

```bash
# 1. æ£€æŸ¥ä»£ç 
python -m py_compile basicts/mask/model.py
python -m py_compile basicts/mask/adaptive_graph.py

# 2. å¿«é€Ÿæµ‹è¯•
python main.py --config parameters/PEMS03_multihead.yaml \
    --pretrain_epochs 1 --finetune_epochs 1

# 3. å®Œæ•´è®­ç»ƒ
python main.py --config parameters/PEMS03_multihead.yaml \
    --pretrain_epochs 100 --finetune_epochs 100

# 4. æŸ¥çœ‹ç»“æœ
swanlab watch
```

---

## ğŸ’¡ æ•…éšœæ’é™¤

### é—®é¢˜ 1: å¯¼å…¥é”™è¯¯
```
ModuleNotFoundError: No module named 'adaptive_graph'
```
**è§£å†³**: ç¡®ä¿ `adaptive_graph.py` åœ¨ `basicts/mask/` ç›®å½•ä¸‹

### é—®é¢˜ 2: å½¢çŠ¶ä¸åŒ¹é…
```
RuntimeError: size mismatch
```
**è§£å†³**: æ£€æŸ¥ `embed_dim` æ˜¯å¦èƒ½è¢« `num_heads` æ•´é™¤ï¼ˆä»… Multi-Headï¼‰

### é—®é¢˜ 3: å†…å­˜æº¢å‡º (Dynamic æ–¹æ³•)
```
CUDA out of memory
```
**è§£å†³**: å‡å° batch size æˆ–ä½¿ç”¨ `sparse` æ–¹æ³•

---

å¥½äº†ï¼ç°åœ¨æ‚¨æœ‰äº† **7 ç§å…ˆè¿›çš„åŠ¨æ€é‚»æ¥çŸ©é˜µæ„å»ºæ–¹æ³•**ï¼Œå¯ä»¥æ ¹æ®æ‚¨çš„éœ€æ±‚é€‰æ‹©ä½¿ç”¨ã€‚

**æ¨èé¡ºåº**:
1. å…ˆè¯• **Multi-Head** (ç®€å•+æœ‰æ•ˆ)
2. å†è¯• **Hyperbolic** (é€‚åˆäº¤é€šç½‘ç»œ)
3. æœ€åè¯• **Dynamic** (æ€§èƒ½æœ€ä½³ä½†å¼€é”€å¤§)

ç¥å®éªŒé¡ºåˆ©ï¼ğŸ‰
