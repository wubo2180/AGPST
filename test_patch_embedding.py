"""
æµ‹è¯•ä¿®æ”¹åçš„PatchEmbeddingæ˜¯å¦æ­£ç¡®å¤„ç†(B, L, N, C)æ ¼å¼çš„æ•°æ®
"""
import torch
import torch.nn as nn
import sys
import os

# æ·»åŠ è·¯å¾„ä»¥ä¾¿å¯¼å…¥æ¨¡å—
sys.path.append('.')

from basicts.mask.patch import PatchEmbedding

def test_patch_embedding():
    """æµ‹è¯•PatchEmbeddingå¯¹(B, L, N, C)æ•°æ®çš„å¤„ç†"""
    
    # æ¨¡æ‹Ÿæ‚¨çš„å®é™…æ•°æ®
    B, L, N, C = 4, 864, 358, 1
    patch_size = 12
    embed_dim = 96
    
    print("ğŸ§ª æµ‹è¯•PatchEmbeddingä¿®æ”¹ç‰ˆæœ¬")
    print(f"è¾“å…¥æ•°æ®ç»´åº¦: (B={B}, L={L}, N={N}, C={C})")
    print(f"Patché…ç½®: patch_size={patch_size}, embed_dim={embed_dim}")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = torch.randn(B, L, N, C)
    print(f"âœ… æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_data.shape}")
    
    # æµ‹è¯•å•å°ºåº¦patch embedding
    print("\nğŸ“Š æµ‹è¯•å•å°ºåº¦Patch Embedding:")
    single_scale_patch = PatchEmbedding(
        patch_size=patch_size,
        in_channel=C,
        embed_dim=embed_dim,
        num_nodes=N,
        topK=6,  # è¿™ä¸ªå‚æ•°ç°åœ¨ä¸å½±å“ç»“æœ
        norm_layer=None,
        patch_sizes=None  # å•å°ºåº¦
    )
    
    try:
        output_single = single_scale_patch(test_data)
        expected_patches = L // patch_size  # 864 // 12 = 72
        print(f"âœ… å•å°ºåº¦è¾“å‡ºå½¢çŠ¶: {output_single.shape}")
        print(f"âœ… é¢„æœŸå½¢çŠ¶: (B={B}, embed_dim={embed_dim}, P={expected_patches}, N={N})")
        assert output_single.shape == (B, embed_dim, expected_patches, N)
        print("âœ… å•å°ºåº¦æµ‹è¯•é€šè¿‡ï¼")
    except Exception as e:
        print(f"âŒ å•å°ºåº¦æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•å¤šå°ºåº¦patch embedding
    print("\nğŸ“Š æµ‹è¯•å¤šå°ºåº¦Patch Embedding:")
    patch_sizes = [6, 12, 24]  # å¤šå°ºåº¦
    multi_scale_patch = PatchEmbedding(
        patch_size=patch_size,  # ä¸»patch size
        in_channel=C,
        embed_dim=embed_dim,
        num_nodes=N,
        topK=6,
        norm_layer=None,
        patch_sizes=patch_sizes  # å¤šå°ºåº¦
    )
    
    try:
        output_multi = multi_scale_patch(test_data)
        min_patches = min([L // p for p in patch_sizes])  # æœ€å°patchæ•°
        print(f"âœ… å¤šå°ºåº¦è¾“å‡ºå½¢çŠ¶: {output_multi.shape}")
        print(f"âœ… é¢„æœŸå½¢çŠ¶: (B={B}, embed_dim={embed_dim}, P={min_patches}, N={N})")
        assert output_multi.shape == (B, embed_dim, min_patches, N)
        print("âœ… å¤šå°ºåº¦æµ‹è¯•é€šè¿‡ï¼")
    except Exception as e:
        print(f"âŒ å¤šå°ºåº¦æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•ä¸åŒpatch sizeçš„å½±å“
    print("\nğŸ“Š æµ‹è¯•ä¸åŒPatch Size:")
    for p_size in [6, 12, 24]:
        patch_layer = PatchEmbedding(
            patch_size=p_size,
            in_channel=C,
            embed_dim=embed_dim,
            num_nodes=N,
            topK=6,
            norm_layer=None
        )
        
        try:
            output = patch_layer(test_data)
            expected_p = L // p_size
            print(f"  Patch size {p_size}: è¾“å‡º {output.shape}, é¢„æœŸpatches={expected_p}")
            assert output.shape == (B, embed_dim, expected_p, N)
        except Exception as e:
            print(f"  âŒ Patch size {p_size} æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼PatchEmbeddingå·²æˆåŠŸé€‚é…(B, L, N, C)æ ¼å¼")
    return True

def demonstrate_usage():
    """æ¼”ç¤ºå¦‚ä½•åœ¨å®é™…æ¨¡å‹ä¸­ä½¿ç”¨"""
    print("\nğŸ”§ æ¼”ç¤ºå®é™…ä½¿ç”¨æ–¹æ³•:")
    
    # PEMS03æ•°æ®é›†é…ç½®
    config = {
        'B': 4,           # batch size
        'L': 864,         # æ—¶é—´åºåˆ—é•¿åº¦
        'N': 358,         # èŠ‚ç‚¹æ•° 
        'C': 1,           # ç‰¹å¾ç»´åº¦
        'patch_size': 12, # patchå¤§å°
        'embed_dim': 96,  # åµŒå…¥ç»´åº¦
        'patch_sizes': [6, 12, 24]  # å¤šå°ºåº¦
    }
    
    # åˆ›å»ºpatch embeddingå±‚
    patch_embedding = PatchEmbedding(
        patch_size=config['patch_size'],
        in_channel=config['C'],
        embed_dim=config['embed_dim'],
        num_nodes=config['N'],
        topK=6,  # åœ¨æ–°ç‰ˆæœ¬ä¸­è¿™ä¸ªå‚æ•°ä¸å½±å“è¾“å‡ºç»´åº¦
        norm_layer=nn.LayerNorm(config['embed_dim']),  # å¯ä»¥æ·»åŠ normalization
        patch_sizes=config['patch_sizes']
    )
    
    # æ¨¡æ‹Ÿæ•°æ®
    traffic_data = torch.randn(config['B'], config['L'], config['N'], config['C'])
    print(f"åŸå§‹äº¤é€šæ•°æ®: {traffic_data.shape}")
    
    # Patch embedding
    patches = patch_embedding(traffic_data)
    print(f"PatchåµŒå…¥å: {patches.shape}")
    
    # ä¸ºAdaptiveGraphLearnerå‡†å¤‡æ•°æ®
    # éœ€è¦è½¬æ¢ä¸º (B, P, N, D) æ ¼å¼
    B, D, P, N = patches.shape
    patches_for_graph = patches.permute(0, 2, 3, 1)  # (B, P, N, D)
    print(f"å›¾å­¦ä¹ è¾“å…¥æ ¼å¼: {patches_for_graph.shape}")
    
    print("\nğŸ“‹ æ•°æ®æµç¨‹æ€»ç»“:")
    print(f"1. åŸå§‹æ•°æ®: (B={config['B']}, L={config['L']}, N={config['N']}, C={config['C']})")
    print(f"2. PatchåµŒå…¥: (B={B}, D={D}, P={P}, N={N})")
    print(f"3. å›¾å­¦ä¹ è¾“å…¥: (B={B}, P={P}, N={N}, D={D})")
    print(f"4. å‹ç¼©æ¯”: æ—¶é—´ç»´åº¦ä»{config['L']}å‹ç¼©åˆ°{P} (å‹ç¼©{config['L']//P}å€)")

if __name__ == "__main__":
    print("ğŸš€ æµ‹è¯•ä¿®æ”¹åçš„PatchEmbeddingæ¨¡å—\n")
    
    # è¿è¡Œæµ‹è¯•
    if test_patch_embedding():
        demonstrate_usage()
    else:
        print("âŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ")