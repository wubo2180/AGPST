"""
æ—¶ç©ºå™ªå£°åˆ†æï¼ˆæ–‡æœ¬ç‰ˆæœ¬ï¼‰ - ä¸éœ€è¦matplotlib
"""
import numpy as np
import pickle
import os


def load_dataset(dataset_name, mode='train'):
    """åŠ è½½æ•°æ®é›†"""
    data_path = f'datasets/{dataset_name}/{mode}_data.npy'
    
    if not os.path.exists(data_path):
        return None
    
    data = np.load(data_path)
    return data


def load_adjacency_matrix(dataset_name):
    """åŠ è½½é‚»æ¥çŸ©é˜µ"""
    adj_path = f'datasets/{dataset_name}/adj_mx.pkl'
    
    if not os.path.exists(adj_path):
        return None
    
    try:
        with open(adj_path, 'rb') as f:
            pkl_data = pickle.load(f, encoding='latin1')
        
        if isinstance(pkl_data, np.ndarray):
            adj_mx = pkl_data
        else:
            return None
        
        # å¦‚æœæ˜¯è·ç¦»çŸ©é˜µï¼Œè½¬æ¢ä¸ºäºŒè¿›åˆ¶é‚»æ¥çŸ©é˜µ
        if adj_mx.max() > 1.0:
            threshold = np.percentile(adj_mx[adj_mx > 0], 25)
            adj_mx = (adj_mx > 0) & (adj_mx <= threshold)
            adj_mx = adj_mx.astype(np.float32)
        
        return adj_mx
    
    except Exception as e:
        return None


def generate_adjacency_from_data(data, k_neighbors=5):
    """ä»æ•°æ®ç”Ÿæˆé‚»æ¥çŸ©é˜µ"""
    T, N = data.shape
    corr_matrix = np.corrcoef(data.T)
    dist_matrix = 1 - np.abs(corr_matrix)
    
    adj_matrix = np.zeros((N, N))
    for i in range(N):
        neighbors = np.argsort(dist_matrix[i, :])[1:k_neighbors+1]
        adj_matrix[i, neighbors] = 1
        adj_matrix[neighbors, i] = 1
    
    return adj_matrix


def detect_temporal_outliers(data, method='zscore'):
    """æ£€æµ‹æ—¶é—´ç»´åº¦å¼‚å¸¸å€¼"""
    T, N = data.shape
    
    if method == 'zscore':
        mean = np.mean(data, axis=0)
        std = np.std(data, axis=0)
        z_scores = np.abs((data - mean) / (std + 1e-8))
        outliers = z_scores > 3
    
    return outliers


def detect_spatial_outliers(data, adj_matrix, threshold=3.0):
    """æ£€æµ‹ç©ºé—´ç»´åº¦å¼‚å¸¸å€¼"""
    T, N = data.shape
    spatial_outliers = np.zeros((T, N), dtype=bool)
    
    for t in range(T):
        snapshot = data[t, :]
        
        for i in range(N):
            neighbors = np.where(adj_matrix[i, :] > 0)[0]
            
            if len(neighbors) == 0:
                continue
            
            neighbor_vals = snapshot[neighbors]
            avg_neighbor = np.mean(neighbor_vals)
            std_neighbor = np.std(neighbor_vals)
            
            if std_neighbor < 1e-6:
                std_neighbor = np.std(snapshot)
            
            if std_neighbor > 0:
                z_score = abs(snapshot[i] - avg_neighbor) / std_neighbor
                if z_score > threshold:
                    spatial_outliers[t, i] = True
    
    return spatial_outliers


def compute_spatial_autocorrelation(data, adj_matrix):
    """è®¡ç®—Moran's Iç©ºé—´è‡ªç›¸å…³ç³»æ•°"""
    T, N = data.shape
    moran_values = []
    
    W = np.sum(adj_matrix)
    if W == 0:
        return np.zeros(T)
    
    for t in range(T):
        snapshot = data[t, :]
        mean_val = np.mean(snapshot)
        deviations = snapshot - mean_val
        
        numerator = 0
        for i in range(N):
            for j in range(N):
                if adj_matrix[i, j] > 0:
                    numerator += adj_matrix[i, j] * deviations[i] * deviations[j]
        
        denominator = np.sum(deviations ** 2)
        
        if denominator > 0:
            moran_i = (N / W) * (numerator / denominator)
        else:
            moran_i = 0
        
        moran_values.append(moran_i)
    
    return np.array(moran_values)


def analyze_dataset(dataset_name):
    """åˆ†æå•ä¸ªæ•°æ®é›†"""
    print(f"\n{'='*70}")
    print(f"ğŸ“Š åˆ†ææ•°æ®é›†: {dataset_name}")
    print(f"{'='*70}")
    
    # åŠ è½½æ•°æ®
    data = load_dataset(dataset_name, mode='train')
    if data is None:
        print(f"âŒ æ— æ³•åŠ è½½æ•°æ®")
        return None
    
    T, N = data.shape
    print(f"âœ… æ•°æ®å½¢çŠ¶: (T={T}, N={N})")
    
    # åŠ è½½æˆ–ç”Ÿæˆé‚»æ¥çŸ©é˜µ
    adj_matrix = load_adjacency_matrix(dataset_name)
    if adj_matrix is None:
        print(f"ğŸ’¡ ä½¿ç”¨æ•°æ®ç”Ÿæˆk-è¿‘é‚»é‚»æ¥çŸ©é˜µ (k=5)")
        adj_matrix = generate_adjacency_from_data(data, k_neighbors=5)
    else:
        print(f"âœ… åŠ è½½é‚»æ¥çŸ©é˜µ: {adj_matrix.shape}")
    
    edge_count = (adj_matrix > 0).sum() / 2
    density = edge_count / (N * (N-1) / 2) * 100 if N > 1 else 0
    print(f"   è¾¹æ•°: {int(edge_count)}, å¯†åº¦: {density:.2f}%")
    
    # 1. æ—¶é—´ç»´åº¦å¼‚å¸¸å€¼
    print(f"\nğŸ” æ£€æµ‹æ—¶é—´ç»´åº¦å¼‚å¸¸å€¼...")
    temporal_outliers = detect_temporal_outliers(data, method='zscore')
    temporal_ratio = temporal_outliers.sum() / temporal_outliers.size * 100
    temporal_counts = temporal_outliers.sum(axis=0)
    
    print(f"   å¼‚å¸¸å€¼æ¯”ä¾‹: {temporal_ratio:.2f}%")
    print(f"   å¹³å‡æ¯èŠ‚ç‚¹: {temporal_counts.mean():.1f}ä¸ª")
    print(f"   æœ€å¤šèŠ‚ç‚¹: {temporal_counts.max()}ä¸ª, æœ€å°‘: {temporal_counts.min()}ä¸ª")
    
    # 2. ç©ºé—´ç»´åº¦å¼‚å¸¸å€¼
    print(f"\nğŸŒ æ£€æµ‹ç©ºé—´ç»´åº¦å¼‚å¸¸å€¼...")
    spatial_outliers = detect_spatial_outliers(data, adj_matrix, threshold=3.0)
    spatial_ratio = spatial_outliers.sum() / spatial_outliers.size * 100
    spatial_counts = spatial_outliers.sum(axis=0)
    
    print(f"   å¼‚å¸¸å€¼æ¯”ä¾‹: {spatial_ratio:.2f}%")
    print(f"   å¹³å‡æ¯èŠ‚ç‚¹: {spatial_counts.mean():.1f}ä¸ª")
    print(f"   æœ€å¤šèŠ‚ç‚¹: {spatial_counts.max()}ä¸ª, æœ€å°‘: {spatial_counts.min()}ä¸ª")
    
    # 3. æ—¶ç©ºäº¤å‰å¼‚å¸¸å€¼
    print(f"\nğŸ”„ åˆ†ææ—¶ç©ºäº¤å‰å¼‚å¸¸å€¼...")
    spatiotemporal_outliers = temporal_outliers & spatial_outliers
    st_ratio = spatiotemporal_outliers.sum() / spatiotemporal_outliers.size * 100
    
    print(f"   äº¤å‰å¼‚å¸¸æ¯”ä¾‹: {st_ratio:.2f}%")
    print(f"   å æ—¶é—´å¼‚å¸¸çš„æ¯”ä¾‹: {st_ratio/temporal_ratio*100:.1f}%" if temporal_ratio > 0 else "   N/A")
    print(f"   å ç©ºé—´å¼‚å¸¸çš„æ¯”ä¾‹: {st_ratio/spatial_ratio*100:.1f}%" if spatial_ratio > 0 else "   N/A")
    
    # 4. ç©ºé—´è‡ªç›¸å…³
    print(f"\nğŸ“ˆ è®¡ç®—ç©ºé—´è‡ªç›¸å…³...")
    moran_i = compute_spatial_autocorrelation(data, adj_matrix)
    avg_moran = np.mean(moran_i)
    std_moran = np.std(moran_i)
    
    print(f"   å¹³å‡ Moran's I: {avg_moran:.3f} Â± {std_moran:.3f}")
    print(f"   æœ€å°å€¼: {moran_i.min():.3f}, æœ€å¤§å€¼: {moran_i.max():.3f}")
    
    # 5. è¯Šæ–­å’Œå»ºè®®
    print(f"\n{'='*70}")
    print(f"ğŸ¯ å™ªå£°ç±»å‹è¯Šæ–­")
    print(f"{'='*70}")
    
    if temporal_ratio > spatial_ratio * 2:
        noise_type = "æ—¶é—´å™ªå£°ä¸»å¯¼"
        icon = "ğŸ”´"
        recommendation = {
            'type': 'temporal',
            'use_denoising': True,
            'denoise_type': 'attention' if temporal_ratio > 5 else 'conv',
            'use_advanced_graph': False
        }
    elif spatial_ratio > temporal_ratio * 2:
        noise_type = "ç©ºé—´å™ªå£°ä¸»å¯¼"
        icon = "ğŸ”µ"
        recommendation = {
            'type': 'spatial',
            'use_denoising': temporal_ratio > 2,
            'denoise_type': 'conv',
            'use_advanced_graph': True,
            'graph_heads': 4
        }
    else:
        noise_type = "æ—¶ç©ºè€¦åˆå™ªå£°"
        icon = "ğŸŸ£"
        recommendation = {
            'type': 'spatiotemporal',
            'use_denoising': True,
            'denoise_type': 'attention',
            'use_advanced_graph': True,
            'graph_heads': 4
        }
    
    print(f"\n{icon} ã€{noise_type}ã€‘")
    print(f"   æ—¶é—´å¼‚å¸¸: {temporal_ratio:.2f}%")
    print(f"   ç©ºé—´å¼‚å¸¸: {spatial_ratio:.2f}%")
    print(f"   æ¯”å€¼: {temporal_ratio/spatial_ratio:.2f}" if spatial_ratio > 0 else "   æ¯”å€¼: inf")
    
    print(f"\nğŸ’¡ é…ç½®å»ºè®®:")
    print(f"   use_denoising: {recommendation['use_denoising']}")
    if recommendation['use_denoising']:
        print(f"   denoise_type: '{recommendation['denoise_type']}'")
    print(f"   use_advanced_graph: {recommendation['use_advanced_graph']}")
    if recommendation.get('graph_heads'):
        print(f"   graph_heads: {recommendation['graph_heads']}")
    
    print(f"\nğŸŒ ç©ºé—´ç»“æ„è¯„ä¼°:")
    if avg_moran > 0.7:
        print(f"   âœ… ç©ºé—´è‡ªç›¸å…³å¼º ({avg_moran:.3f}) - ç©ºé—´ç»“æ„è‰¯å¥½")
    elif avg_moran > 0.4:
        print(f"   ğŸŸ¡ ç©ºé—´è‡ªç›¸å…³ä¸­ç­‰ ({avg_moran:.3f}) - ç©ºé—´ç»“æ„å¯æ¥å—")
    else:
        print(f"   ğŸ”´ ç©ºé—´è‡ªç›¸å…³å¼± ({avg_moran:.3f}) - ç©ºé—´ç»“æ„æ··ä¹±")
        print(f"      å»ºè®®: ä½¿ç”¨è‡ªé€‚åº”å›¾å­¦ä¹ é‡æ–°å­¦ä¹ é‚»æ¥çŸ©é˜µ")
    
    return {
        'dataset': dataset_name,
        'temporal_outlier_ratio': temporal_ratio,
        'spatial_outlier_ratio': spatial_ratio,
        'spatiotemporal_outlier_ratio': st_ratio,
        'avg_spatial_autocorrelation': avg_moran,
        'noise_type': noise_type,
        'recommendation': recommendation
    }


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "ğŸŒ " + "="*68 + " ğŸŒ")
    print("ğŸŒ  æ—¶ç©ºè”åˆå™ªå£°åˆ†æå·¥å…·ï¼ˆæ–‡æœ¬ç‰ˆæœ¬ï¼‰")
    print("ğŸŒ " + "="*68 + " ğŸŒ")
    
    datasets = []
    datasets_dir = 'datasets'
    
    if os.path.exists(datasets_dir):
        for item in os.listdir(datasets_dir):
            item_path = os.path.join(datasets_dir, item)
            if os.path.isdir(item_path):
                train_file = os.path.join(item_path, 'train_data.npy')
                if os.path.exists(train_file):
                    datasets.append(item)
    
    datasets = sorted(datasets)
    
    if not datasets:
        print("\nâŒ æœªæ‰¾åˆ°ä»»ä½•æ•°æ®é›†ï¼")
        return
    
    print(f"\nâœ… æ‰¾åˆ° {len(datasets)} ä¸ªæ•°æ®é›†: {', '.join(datasets)}")
    
    # åˆ†ææ‰€æœ‰æ•°æ®é›†
    all_metrics = []
    
    for dataset_name in datasets:
        metrics = analyze_dataset(dataset_name)
        if metrics:
            all_metrics.append(metrics)
    
    # æ±‡æ€»å¯¹æ¯”
    if len(all_metrics) > 1:
        print(f"\n{'='*70}")
        print(f"ğŸ“Š æ±‡æ€»å¯¹æ¯”")
        print(f"{'='*70}")
        
        print(f"\n{'æ•°æ®é›†':<15} {'å™ªå£°ç±»å‹':<12} {'æ—¶é—´%':<8} {'ç©ºé—´%':<8} {'Moran':<8} {'å»ºè®®'}")
        print(f"{'-'*70}")
        
        for m in all_metrics:
            dataset = m['dataset']
            noise_type = m['noise_type'][:4]
            temp = m['temporal_outlier_ratio']
            spat = m['spatial_outlier_ratio']
            moran = m['avg_spatial_autocorrelation']
            rec = m['recommendation']
            
            if rec['use_denoising'] and rec['use_advanced_graph']:
                suggest = "å»å™ª+å›¾å­¦ä¹ "
            elif rec['use_denoising']:
                suggest = f"å»å™ª({rec['denoise_type']})"
            elif rec['use_advanced_graph']:
                suggest = "å›¾å­¦ä¹ "
            else:
                suggest = "æ— éœ€ç‰¹æ®Šå¤„ç†"
            
            print(f"{dataset:<15} {noise_type:<12} {temp:>6.2f}% {spat:>6.2f}% {moran:>7.3f} {suggest}")
    
    print(f"\n{'='*70}")
    print(f"âœ… åˆ†æå®Œæˆï¼")
    print(f"{'='*70}")
    
    print(f"\nğŸ’¡ æ€»ç»“:")
    print(f"   â€¢ æ—¶é—´å™ªå£°ä¸»å¯¼çš„æ•°æ®é›†é€‚åˆä½¿ç”¨å»å™ªæ¨¡å—")
    print(f"   â€¢ ç©ºé—´å™ªå£°ä¸»å¯¼çš„æ•°æ®é›†é€‚åˆä½¿ç”¨åŠ¨æ€å›¾å­¦ä¹ ")
    print(f"   â€¢ æ—¶ç©ºè€¦åˆå™ªå£°éœ€è¦ä¸¤è€…ç»“åˆ")
    print(f"   â€¢ ä½ç©ºé—´è‡ªç›¸å…³(Moran's I<0.4)å»ºè®®ä½¿ç”¨è‡ªé€‚åº”å›¾å­¦ä¹ \n")


if __name__ == '__main__':
    main()
