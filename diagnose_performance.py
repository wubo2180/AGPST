"""
æ€§èƒ½è¯Šæ–­è„šæœ¬
æ£€æŸ¥æ¨¡å‹è¾“å‡ºã€æ¢¯åº¦ã€å­¦ä¹ ç‡ç­‰å…³é”®æŒ‡æ ‡
"""

import torch
import sys
sys.path.append('.')

from basicts.mask.model import AGPSTModel

def diagnose_model():
    print("=" * 60)
    print("æ€§èƒ½è¯Šæ–­ - Encoder-Decoder æ¶æ„")
    print("=" * 60)
    
    # æ¨¡å‹é…ç½®
    config = {
        'num_nodes': 358,
        'dim': 10,
        'topK': 10,
        'in_channel': 1,
        'embed_dim': 96,
        'num_heads': 4,
        'mlp_ratio': 4,
        'dropout': 0.1,
        'encoder_depth': 4,
        'decoder_depth': 1,  # â­ ä¿®å¤å: 1å±‚
        'use_denoising': False,
        'denoise_type': 'conv',
        'use_advanced_graph': True,
        'graph_heads': 4,
        'pred_len': 12
    }
    
    print("\nå½“å‰é…ç½®:")
    print(f"  Encoderæ·±åº¦: {config['encoder_depth']}")
    print(f"  Decoderæ·±åº¦: {config['decoder_depth']} â­ (ä¿®å¤å)")
    print(f"  å­¦ä¹ ç‡å»ºè®®: 0.0003 â­ (ä¿®å¤å)")
    print(f"  æ‰¹æ¬¡å¤§å°å»ºè®®: 64 â­ (ä¿®å¤å)")
    print()
    
    # åˆ›å»ºæ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = AGPSTModel(**config).to(device)
    
    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    encoder_params = sum(p.numel() for p in model.encoder.parameters())
    decoder_params = sum(p.numel() for p in model.decoder.parameters())
    projection_params = sum(p.numel() for p in model.output_projection.parameters())
    
    print("å‚æ•°ç»Ÿè®¡:")
    print(f"  æ€»å‚æ•°: {total_params:,}")
    print(f"  Encoder: {encoder_params:,} ({encoder_params/total_params*100:.1f}%)")
    print(f"  Decoder: {decoder_params:,} ({decoder_params/total_params*100:.1f}%)")
    print(f"  Projection: {projection_params:,} ({projection_params/total_params*100:.1f}%)")
    print()
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size = 32
    history = torch.randn(batch_size, 12, 358, 1).to(device)
    target = torch.randn(batch_size, 12, 358, 1).to(device)
    
    print("=" * 60)
    print("å‰å‘ä¼ æ’­æµ‹è¯•")
    print("=" * 60)
    
    model.train()
    prediction = model(history)
    
    print(f"è¾“å…¥å½¢çŠ¶: {history.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {prediction.shape}")
    print()
    
    # æ£€æŸ¥è¾“å‡ºèŒƒå›´
    print("è¾“å‡ºç»Ÿè®¡:")
    print(f"  é¢„æµ‹å€¼èŒƒå›´: [{prediction.min().item():.4f}, {prediction.max().item():.4f}]")
    print(f"  é¢„æµ‹å€¼å‡å€¼: {prediction.mean().item():.4f}")
    print(f"  é¢„æµ‹å€¼æ ‡å‡†å·®: {prediction.std().item():.4f}")
    print(f"  ç›®æ ‡å€¼èŒƒå›´: [{target.min().item():.4f}, {target.max().item():.4f}]")
    print()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
    if torch.isnan(prediction).any():
        print("âŒ è­¦å‘Š: è¾“å‡ºåŒ…å« NaN!")
    elif torch.isinf(prediction).any():
        print("âŒ è­¦å‘Š: è¾“å‡ºåŒ…å« Inf!")
    else:
        print("âœ… è¾“å‡ºå€¼æ­£å¸¸")
    print()
    
    # æµ‹è¯•æ¢¯åº¦
    print("=" * 60)
    print("æ¢¯åº¦æµ‹è¯•")
    print("=" * 60)
    
    loss = torch.nn.functional.mse_loss(prediction, target)
    print(f"æŸå¤±å€¼: {loss.item():.6f}")
    
    loss.backward()
    
    # æ£€æŸ¥å…³é”®ç»„ä»¶çš„æ¢¯åº¦
    print("\nå…³é”®ç»„ä»¶æ¢¯åº¦èŒƒæ•°:")
    
    components = {
        'future_queries': model.future_queries,
        'encoder_pos_embed': model.encoder_pos_embed,
        'decoder_pos_embed': model.decoder_pos_embed,
    }
    
    for name, param in components.items():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            print(f"  {name:20s}: {grad_norm:.6f}")
        else:
            print(f"  {name:20s}: No gradient")
    
    # æ£€æŸ¥æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦èŒƒå›´
    print("\næ¢¯åº¦èŒƒå›´åˆ†æ:")
    grad_norms = []
    large_grads = []
    tiny_grads = []
    no_grads = []
    
    for name, param in model.named_parameters():
        if param.requires_grad:
            if param.grad is not None:
                grad_norm = param.grad.norm().item()
                grad_norms.append(grad_norm)
                
                if grad_norm > 10.0:
                    large_grads.append((name, grad_norm))
                elif grad_norm < 1e-6:
                    tiny_grads.append((name, grad_norm))
            else:
                no_grads.append(name)
    
    if grad_norms:
        print(f"  å¹³å‡æ¢¯åº¦èŒƒæ•°: {sum(grad_norms)/len(grad_norms):.6f}")
        print(f"  æœ€å¤§æ¢¯åº¦èŒƒæ•°: {max(grad_norms):.6f}")
        print(f"  æœ€å°æ¢¯åº¦èŒƒæ•°: {min(grad_norms):.6f}")
    
    if large_grads:
        print(f"\nâš ï¸  è¿‡å¤§çš„æ¢¯åº¦ (> 10.0): {len(large_grads)} ä¸ª")
        for name, norm in large_grads[:3]:
            print(f"    - {name}: {norm:.4f}")
    
    if tiny_grads:
        print(f"\nâš ï¸  è¿‡å°çš„æ¢¯åº¦ (< 1e-6): {len(tiny_grads)} ä¸ª")
        for name, norm in tiny_grads[:3]:
            print(f"    - {name}: {norm:.8f}")
    
    if no_grads:
        print(f"\nâš ï¸  æœªè®¡ç®—æ¢¯åº¦: {len(no_grads)} ä¸ªå‚æ•°")
    
    if not large_grads and not tiny_grads:
        print("\nâœ… æ¢¯åº¦èŒƒå›´æ­£å¸¸")
    
    print()
    
    # è¾“å‡ºæŠ•å½±å±‚åˆ†æ
    print("=" * 60)
    print("è¾“å‡ºæŠ•å½±å±‚åˆ†æ")
    print("=" * 60)
    
    print("\nå½“å‰ç»“æ„:")
    for i, layer in enumerate(model.output_projection):
        print(f"  Layer {i}: {layer}")
    
    print(f"\næŠ•å½±å±‚å‚æ•°é‡: {projection_params:,}")
    print(f"å æ€»å‚æ•°æ¯”ä¾‹: {projection_params/total_params*100:.2f}%")
    
    # æ£€æŸ¥æŠ•å½±å±‚çš„è¾“å‡º
    test_input = torch.randn(32, 12, 96).to(device)
    test_output = model.output_projection(test_input)
    print(f"\næŠ•å½±å±‚æµ‹è¯•:")
    print(f"  è¾“å…¥: {test_input.shape}")
    print(f"  è¾“å‡º: {test_output.shape}")
    print(f"  è¾“å‡ºèŒƒå›´: [{test_output.min().item():.4f}, {test_output.max().item():.4f}]")
    
    print()
    
    # å»ºè®®
    print("=" * 60)
    print("ä¼˜åŒ–å»ºè®®")
    print("=" * 60)
    
    print("\nâœ… å·²ä¿®å¤çš„é—®é¢˜:")
    print("  1. è¾“å‡ºæŠ•å½±å±‚: å¢å¼ºä¸º 96â†’96â†’48â†’1 (å¸¦ LayerNorm + GELU)")
    print("  2. æœªæ¥æŸ¥è¯¢åˆå§‹åŒ–: æ”¹ç”¨ Xavier åˆå§‹åŒ–")
    print("  3. è§£ç å™¨æ·±åº¦: ä» 2 å±‚å‡å°‘åˆ° 1 å±‚")
    print()
    
    print("ğŸ“ é…ç½®æ–‡ä»¶å»ºè®® (parameters/PEMS03.yaml):")
    print("  decoder_depth: 1     # â­ å·²ä¿®æ”¹")
    print("  lr: 0.0003           # â­ å·²ä¿®æ”¹ (ä» 0.001)")
    print("  batch_size: 64       # â­ å·²ä¿®æ”¹ (ä» 32)")
    print()
    
    print("ğŸ¯ é¢„æœŸæ€§èƒ½æå‡:")
    print("  å½“å‰ MAE: ~22.03")
    print("  ä¿®å¤åé¢„æœŸ: ~16-18 (ç¬¬ä¸€é˜¶æ®µ)")
    print("  æœ€ç»ˆç›®æ ‡: ~14.5-15 (å…¨éƒ¨ä¼˜åŒ–)")
    print()
    
    print("ğŸš€ ä¸‹ä¸€æ­¥:")
    print("  1. ä½¿ç”¨ä¿®å¤åçš„é…ç½®é‡æ–°è®­ç»ƒ")
    print("  2. ç›‘æ§è®­ç»ƒæŸå¤±æ›²çº¿æ˜¯å¦ç¨³å®šä¸‹é™")
    print("  3. å¦‚æœæ€§èƒ½ä»ä¸ä½³ï¼Œç»§ç»­åº”ç”¨è¿›é˜¶ä¼˜åŒ–")
    print()
    
    print("=" * 60)
    print("è¯Šæ–­å®Œæˆ!")
    print("=" * 60)


if __name__ == '__main__':
    diagnose_model()
