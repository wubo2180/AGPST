"""
å¿«é€Ÿæµ‹è¯•ä¸åŒç©ºé—´ç¼–ç å™¨çš„è„šæœ¬

ç”¨æ³•:
    python test_spatial_encoders.py --encoder hybrid --epochs 10
"""

import torch
import torch.nn as nn
import numpy as np
import argparse
import time

# å¯¼å…¥æ¨¡å‹
import sys
sys.path.append('.')
from basicts.mask.alternating_st import AlternatingSTModel


def create_dummy_data(batch_size=8, num_nodes=358, seq_len=12):
    """åˆ›å»ºè™šæ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•"""
    # è¾“å…¥: (B, T, N, 1)
    history_data = torch.randn(batch_size, seq_len, num_nodes, 1)
    
    # é‚»æ¥çŸ©é˜µ: ç®€å•çš„ç¯å½¢å›¾ (æ¯ä¸ªèŠ‚ç‚¹è¿æ¥å‰åèŠ‚ç‚¹)
    adj_mx = np.zeros((num_nodes, num_nodes))
    for i in range(num_nodes):
        # è¿æ¥å‰ä¸€ä¸ªèŠ‚ç‚¹
        adj_mx[i, (i - 1) % num_nodes] = 1
        # è¿æ¥åä¸€ä¸ªèŠ‚ç‚¹
        adj_mx[i, (i + 1) % num_nodes] = 1
        # è‡ªç¯
        adj_mx[i, i] = 1
    
    # å½’ä¸€åŒ–: D^(-1/2) A D^(-1/2)
    rowsum = np.array(adj_mx.sum(1))
    d_inv_sqrt = np.power(rowsum, -0.5).flatten()
    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.
    d_mat_inv_sqrt = np.diag(d_inv_sqrt)
    adj_normalized = adj_mx.dot(d_mat_inv_sqrt).T.dot(d_mat_inv_sqrt)
    adj_tensor = torch.FloatTensor(adj_normalized)
    
    return history_data, adj_tensor


def test_encoder(encoder_type='hybrid', num_epochs=10, device='cuda'):
    """
    æµ‹è¯•æŒ‡å®šç¼–ç å™¨çš„æ€§èƒ½
    
    Args:
        encoder_type: 'transformer', 'gcn', 'chebnet', 'gat', 'hybrid'
        num_epochs: è®­ç»ƒè½®æ•°
        device: 'cuda' or 'cpu'
    """
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•ç©ºé—´ç¼–ç å™¨: {encoder_type.upper()}")
    print(f"{'='*60}\n")
    
    # åˆ›å»ºæ¨¡å‹
    model = AlternatingSTModel(
        num_nodes=358,
        in_steps=12,
        out_steps=12,
        input_dim=1,
        embed_dim=64,  # å‡å°ä»¥åŠ é€Ÿæµ‹è¯•
        num_heads=4,
        temporal_depth_1=2,
        spatial_depth_1=1 if encoder_type == 'hybrid' else 2,
        temporal_depth_2=2,
        spatial_depth_2=1 if encoder_type == 'hybrid' else 2,
        fusion_type='gated',
        dropout=0.05,
        use_denoising=True,
        denoise_type='conv',
        spatial_encoder_type=encoder_type,
        gnn_K=3  # ChebNet çš„ K å€¼
    ).to(device)
    
    # ç»Ÿè®¡å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°:")
    print(f"   æ€»å‚æ•°: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"   å‚æ•°å¤§å°: {total_params * 4 / 1024 / 1024:.2f} MB\n")
    
    # åˆ›å»ºè™šæ‹Ÿæ•°æ®
    history_data, adj_mx = create_dummy_data()
    history_data = history_data.to(device)
    adj_mx = adj_mx.to(device) if encoder_type != 'transformer' else None
    
    # è™šæ‹Ÿç›®æ ‡ (ç”¨äºè®¡ç®—æŸå¤±)
    target = torch.randn(8, 12, 358, 1).to(device)
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.L1Loss()
    
    # è®­ç»ƒå¾ªç¯
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...\n")
    times = []
    losses = []
    
    for epoch in range(num_epochs):
        start_time = time.time()
        
        # å‰å‘ä¼ æ’­
        model.train()
        optimizer.zero_grad()
        
        if adj_mx is not None:
            prediction = model(history_data, adj_mx=adj_mx)
        else:
            prediction = model(history_data)
        
        # è®¡ç®—æŸå¤±
        loss = criterion(prediction, target)
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        epoch_time = time.time() - start_time
        times.append(epoch_time)
        losses.append(loss.item())
        
        print(f"Epoch [{epoch+1:2d}/{num_epochs}] | "
              f"Loss: {loss.item():.4f} | "
              f"Time: {epoch_time:.3f}s")
    
    # ç»Ÿè®¡ç»“æœ
    avg_time = np.mean(times[1:])  # è·³è¿‡ç¬¬ä¸€ä¸ª epoch (é¢„çƒ­)
    final_loss = losses[-1]
    
    print(f"\n{'='*60}")
    print(f"ğŸ“ˆ æµ‹è¯•ç»“æœ:")
    print(f"   ç¼–ç å™¨: {encoder_type.upper()}")
    print(f"   å¹³å‡è®­ç»ƒæ—¶é—´: {avg_time:.3f} s/epoch")
    print(f"   æœ€ç»ˆæŸå¤±: {final_loss:.4f}")
    print(f"   æ€»å‚æ•°é‡: {total_params:,}")
    print(f"{'='*60}\n")
    
    return {
        'encoder': encoder_type,
        'avg_time': avg_time,
        'final_loss': final_loss,
        'params': total_params
    }


def main():
    parser = argparse.ArgumentParser(description='æµ‹è¯•ä¸åŒçš„ç©ºé—´ç¼–ç å™¨')
    parser.add_argument('--encoder', type=str, default='all',
                       choices=['all', 'transformer', 'gcn', 'chebnet', 'gat', 'hybrid'],
                       help='ç©ºé—´ç¼–ç å™¨ç±»å‹')
    parser.add_argument('--epochs', type=int, default=10,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--device', type=str, default='cuda',
                       choices=['cuda', 'cpu'],
                       help='è®¾å¤‡ç±»å‹')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥ CUDA å¯ç”¨æ€§
    if args.device == 'cuda' and not torch.cuda.is_available():
        print("âš ï¸  CUDA ä¸å¯ç”¨,åˆ‡æ¢åˆ° CPU")
        args.device = 'cpu'
    
    # æµ‹è¯•ç¼–ç å™¨
    if args.encoder == 'all':
        # æµ‹è¯•æ‰€æœ‰ç¼–ç å™¨
        encoders = ['transformer', 'gcn', 'chebnet', 'gat', 'hybrid']
        results = []
        
        for encoder in encoders:
            try:
                result = test_encoder(encoder, args.epochs, args.device)
                results.append(result)
            except Exception as e:
                print(f"âŒ {encoder.upper()} æµ‹è¯•å¤±è´¥: {e}\n")
        
        # æ‰“å°å¯¹æ¯”ç»“æœ
        if results:
            print("\n" + "="*80)
            print("ğŸ† ç»¼åˆå¯¹æ¯”ç»“æœ")
            print("="*80)
            print(f"{'ç¼–ç å™¨':<15} {'å¹³å‡æ—¶é—´ (s)':<15} {'æœ€ç»ˆæŸå¤±':<15} {'å‚æ•°é‡':<15}")
            print("-"*80)
            
            for r in results:
                print(f"{r['encoder']:<15} {r['avg_time']:<15.3f} "
                      f"{r['final_loss']:<15.4f} {r['params']:<15,}")
            
            print("="*80)
            
            # æ‰¾å‡ºæœ€ä¼˜
            fastest = min(results, key=lambda x: x['avg_time'])
            best_loss = min(results, key=lambda x: x['final_loss'])
            smallest = min(results, key=lambda x: x['params'])
            
            print(f"\nâš¡ æœ€å¿«: {fastest['encoder'].upper()} ({fastest['avg_time']:.3f} s/epoch)")
            print(f"ğŸ¯ æŸå¤±æœ€ä½: {best_loss['encoder'].upper()} ({best_loss['final_loss']:.4f})")
            print(f"ğŸ’¾ å‚æ•°æœ€å°‘: {smallest['encoder'].upper()} ({smallest['params']:,})")
            print("="*80 + "\n")
    else:
        # æµ‹è¯•å•ä¸ªç¼–ç å™¨
        test_encoder(args.encoder, args.epochs, args.device)


if __name__ == '__main__':
    main()
