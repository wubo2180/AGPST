"""
åœ¨AGPSTæ¨¡å‹ä¸­é›†æˆPostPatchDynamicGraphConvçš„å®Œæ•´æ–¹æ¡ˆ
=========================================================

è¿™ä¸ªæ–‡ä»¶å±•ç¤ºäº†å¦‚ä½•æ­£ç¡®åœ°åœ¨model.pyä¸­é›†æˆåŠ¨æ€å›¾å­¦ä¹ æ¨¡å—
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from .post_patch_adaptive_graph import PostPatchDynamicGraphConv


class ImprovedPretrainModel(nn.Module):
    """æ”¹è¿›ç‰ˆçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œé›†æˆäº†åŠ¨æ€å›¾å­¦ä¹ """
    
    def __init__(self, num_nodes, dim, topK, adaptive, epochs, patch_size, 
                 in_channel, embed_dim, num_heads, mlp_ratio, 
                 dropout, mask_ratio, encoder_depth, decoder_depth,
                 patch_sizes=None, mode="pre-train"):
        super().__init__()
        
        # ä¿æŒåŸæœ‰å‚æ•°
        self.adaptive = adaptive
        self.lamda = 0.8
        self.epochs = epochs
        self.embed_dim = embed_dim
        self.patch_size = patch_size
        self.topK = topK
        self.mask_ratio = mask_ratio
        self.selected_feature = 0
        self.mode = mode
        
        # é™æ€å›¾å‚æ•° (å¤‡ç”¨)
        self.nodevec1 = nn.Parameter(torch.randn(num_nodes, dim), requires_grad=True)
        self.nodevec2 = nn.Parameter(torch.randn(dim, num_nodes), requires_grad=True)
        
        # è§„èŒƒåŒ–å±‚
        self.encoder_norm = nn.LayerNorm(embed_dim)
        self.decoder_norm = nn.LayerNorm(embed_dim)
        
        # ğŸ¯ æ ¸å¿ƒç»„ä»¶
        # 1. ç®€åŒ–çš„Patch Embedding
        self.patch_embedding = PatchEmbedding(
            patch_size=patch_size,
            in_channel=in_channel,
            embed_dim=embed_dim,
            norm_layer=nn.LayerNorm(embed_dim)
        )
        
        # 2. ğŸ”¥ åŠ¨æ€å›¾å­¦ä¹ æ¨¡å—
        self.dynamic_graph_conv = PostPatchDynamicGraphConv(
            embed_dim=embed_dim,
            num_nodes=num_nodes,
            node_dim=dim,
            num_heads=4,
            topk=topK,
            dropout=dropout
        )
        
        # 3. ä½ç½®ç¼–ç 
        self.positional_encoding = PositionalEncoding()
        
        # 4. Transformerç¼–ç å™¨
        self.encoder = TransformerLayers(embed_dim, encoder_depth, mlp_ratio, num_heads, dropout)
        
        # 5. è§£ç å™¨ç»„ä»¶
        self.enc_2_dec_emb = nn.Linear(embed_dim, embed_dim, bias=True)
        self.mask_token = nn.Parameter(torch.zeros(1, 1, 1, embed_dim))
        self.decoder = TransformerLayers(embed_dim, decoder_depth, mlp_ratio, num_heads, dropout)
        
        # 6. è¾“å‡ºå±‚
        self.output_layer = nn.Linear(embed_dim, patch_size)
        
        # åˆå§‹åŒ–
        self.initialize_weights()
        
        # ä½ç½®ç¼–ç çŸ©é˜µ
        self.pos_mat = None
        
    def initialize_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        nn.init.trunc_normal_(self.mask_token, std=0.02)
    
    def encoding(self, long_term_history, epoch, mask=True):
        """
        æ”¹è¿›çš„ç¼–ç è¿‡ç¨‹ï¼Œé›†æˆåŠ¨æ€å›¾å­¦ä¹ 
        
        Args:
            long_term_history: (B, L, N, K, C) æˆ– (B, L, N, C)
            epoch: å½“å‰è®­ç»ƒè½®æ•°
            mask: æ˜¯å¦ä½¿ç”¨masking
        """
        
        # å¤„ç†è¾“å…¥ç»´åº¦
        if len(long_term_history.shape) == 5:
            # åŸæ ¼å¼: (B, L, N, K, C) -> å–ç¬¬ä¸€ä¸ªKç»´åº¦
            B, L, N, K, C = long_term_history.shape
            history_data = long_term_history[:, :, :, 0, :]  # (B, L, N, C)
        else:
            # æ–°æ ¼å¼: (B, L, N, C)
            history_data = long_term_history
            B, L, N, C = history_data.shape
        
        if mask:
            # === è®­ç»ƒæ¨¡å¼ (with masking) ===
            
            # Step 1: Patch Embedding
            # (B, L, N, C) -> (B, embed_dim, P, N)
            patches = self.patch_embedding(history_data)
            batch_size, embed_dim, num_time, num_nodes = patches.shape
            
            print(f"ğŸ“Š Patch embedding è¾“å‡º: {patches.shape}")
            
            # Step 2: è½¬æ¢ä¸ºåŠ¨æ€å›¾å­¦ä¹ æ ¼å¼
            # (B, embed_dim, P, N) -> (B, P, N, embed_dim)
            patches_for_graph = patches.permute(0, 2, 3, 1)
            
            # Step 3: ğŸ¯ åŠ¨æ€å›¾å­¦ä¹ ä¸å›¾å·ç§¯
            enhanced_patches, learned_adj = self.dynamic_graph_conv(patches_for_graph)
            print(f"ğŸ”— åŠ¨æ€å›¾å­¦ä¹ å®Œæˆï¼Œé‚»æ¥çŸ©é˜µ: {learned_adj.shape}")
            
            # Step 4: è½¬æ¢ä¸ºTransformeræ ¼å¼
            # (B, P, N, embed_dim) -> (B, N, P, embed_dim)
            patches = enhanced_patches.permute(0, 2, 1, 3)
            
            # Step 5: ä½ç½®ç¼–ç 
            patches, self.pos_mat = self.positional_encoding(patches)
            
            # Step 6: è‡ªé€‚åº”masking
            if self.adaptive:
                mask_ratio = self.mask_ratio * pow((epoch + 1) / self.epochs, self.lamda)
            else:
                mask_ratio = self.mask_ratio
            
            # Step 7: ç”Ÿæˆmask
            from .maskgenerator import MaskGenerator
            Maskg = MaskGenerator(patches.shape[2], mask_ratio)
            unmasked_token_index, masked_token_index = Maskg.uniform_rand()
            
            # Step 8: Transformerç¼–ç 
            encoder_input = patches[:, :, unmasked_token_index, :]
            hidden_states_unmasked = self.encoder(encoder_input)
            hidden_states_unmasked = self.encoder_norm(hidden_states_unmasked)
            
            return hidden_states_unmasked, unmasked_token_index, masked_token_index, learned_adj
        
        else:
            # === æ¨ç†æ¨¡å¼ (without masking) ===
            
            # ç±»ä¼¼çš„å¤„ç†æµç¨‹ï¼Œä½†ä¸è¿›è¡Œmasking
            patches = self.patch_embedding(history_data)
            batch_size, embed_dim, num_time, num_nodes = patches.shape
            
            patches_for_graph = patches.permute(0, 2, 3, 1)
            enhanced_patches, learned_adj = self.dynamic_graph_conv(patches_for_graph)
            patches = enhanced_patches.permute(0, 2, 1, 3)
            
            patches, self.pos_mat = self.positional_encoding(patches)
            
            hidden_states_unmasked = self.encoder(patches)
            hidden_states_unmasked = self.encoder_norm(hidden_states_unmasked)
            
            return hidden_states_unmasked, None, None, learned_adj
    
    def decoding(self, hidden_states_unmasked, masked_token_index, learned_adj=None):
        """
        è§£ç è¿‡ç¨‹ - å¯ä»¥é€‰æ‹©ä½¿ç”¨å­¦ä¹ åˆ°çš„é‚»æ¥çŸ©é˜µ
        """
        batch_size, num_nodes, num_time, _ = hidden_states_unmasked.shape
        
        if masked_token_index is not None:
            unmasked_token_index = [i for i in range(len(masked_token_index) + num_time) 
                                  if i not in masked_token_index]
            
            # å¤„ç†masked tokens
            hidden_states_masked = self.pos_mat[:, :, masked_token_index, :]
            hidden_states_masked += self.mask_token.expand(
                batch_size, num_nodes, len(masked_token_index), self.embed_dim
            )
            
            # æ·»åŠ ä½ç½®ç¼–ç åˆ°unmasked tokens
            hidden_states_unmasked += self.pos_mat[:, :, unmasked_token_index, :]
            
            # æ‹¼æ¥
            hidden_states_full = torch.cat([hidden_states_unmasked, hidden_states_masked], dim=2)
        else:
            hidden_states_full = hidden_states_unmasked
        
        # Transformerè§£ç 
        hidden_states_full = self.decoder(hidden_states_full)
        hidden_states_full = self.decoder_norm(hidden_states_full)
        
        # è¾“å‡ºå±‚
        reconstruction_full = self.output_layer(hidden_states_full)
        
        return reconstruction_full
    
    def forward(self, history_data, epoch):
        """å‰å‘ä¼ æ’­"""
        
        # å¤„ç†è¾“å…¥æ•°æ®æ ¼å¼
        if len(history_data.shape) == 4:
            # å¦‚æœè¾“å…¥æ˜¯(B, L, N, C)ï¼Œéœ€è¦æ„å»ºKç»´åº¦
            B, L, N, C = history_data.shape
            K = self.topK
            
            # æ„å»ºé™æ€é‚»æ¥çŸ©é˜µ (å¤‡ç”¨)
            static_adp = F.softmax(F.relu(torch.mm(self.nodevec1, self.nodevec2)), dim=1)
            values, indices = torch.topk(static_adp, K)
            
            # æ„å»ºK-hopæ•°æ®
            history_data_khop = history_data.transpose(1, 2).reshape(B, N, -1)  # (B, N, L*C)
            history_data_khop = history_data_khop[:, indices, :]  # (B, N, K, L*C)
            history_data_khop = history_data_khop.reshape(B, N, K, L, C)
            history_data_khop = history_data_khop.permute(0, 3, 1, 2, 4)  # (B, L, N, K, C)
        else:
            history_data_khop = history_data
        
        if self.mode == "pre-train":
            # é¢„è®­ç»ƒæ¨¡å¼
            hidden_states_unmasked, unmasked_token_index, masked_token_index, learned_adj = \
                self.encoding(history_data_khop, epoch, mask=True)
            
            reconstruction_full = self.decoding(hidden_states_unmasked, masked_token_index, learned_adj)
            
            # æå–masked tokensç”¨äºæŸå¤±è®¡ç®—
            reconstruction_masked_tokens, label_masked_tokens = self.get_reconstructed_masked_tokens(
                reconstruction_full, history_data.permute(0, 2, 3, 1), 
                unmasked_token_index, masked_token_index
            )
            
            return reconstruction_masked_tokens, label_masked_tokens
        else:
            # æ¨ç†æ¨¡å¼
            hidden_states_full, _, _, learned_adj = self.encoding(history_data_khop, epoch, mask=False)
            return hidden_states_full


# =====================================
# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
# =====================================

def test_improved_model():
    """æµ‹è¯•æ”¹è¿›ç‰ˆæ¨¡å‹"""
    
    # æ¨¡å‹é…ç½®
    config = {
        'num_nodes': 358,
        'dim': 10,
        'topK': 6,
        'adaptive': True,
        'epochs': 100,
        'patch_size': 12,
        'in_channel': 1,
        'embed_dim': 96,
        'num_heads': 4,
        'mlp_ratio': 4,
        'dropout': 0.1,
        'mask_ratio': 0.25,
        'encoder_depth': 4,
        'decoder_depth': 1
    }
    
    # åˆ›å»ºæ¨¡å‹
    model = ImprovedPretrainModel(**config)
    
    # æµ‹è¯•æ•°æ®
    B, L, N, C = 4, 864, 358, 1
    test_data = torch.randn(B, L, N, C)
    
    print(f"ğŸ§ª æµ‹è¯•æ”¹è¿›ç‰ˆAGPSTæ¨¡å‹")
    print(f"è¾“å…¥æ•°æ®: {test_data.shape}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    model.eval()
    with torch.no_grad():
        try:
            output = model(test_data, epoch=0)
            print(f"âœ… æ¨¡å‹è¾“å‡º: {[o.shape for o in output] if isinstance(output, tuple) else output.shape}")
            print("âœ… åŠ¨æ€å›¾å­¦ä¹ é›†æˆæˆåŠŸï¼")
            return True
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            return False


if __name__ == "__main__":
    print("ğŸš€ æµ‹è¯•PostPatchDynamicGraphConvé›†æˆ\n")
    
    if test_improved_model():
        print(f"\nğŸ‰ é›†æˆå®Œæˆï¼ä¸»è¦æ”¹è¿›:")
        print("1. âœ… é€‚é…ç®€åŒ–çš„PatchEmbeddingè¾“å‡ºæ ¼å¼")
        print("2. âœ… åœ¨patch embeddingåä½¿ç”¨åŠ¨æ€å›¾å­¦ä¹ ")  
        print("3. âœ… ä¿æŒä¸åŸæ¨¡å‹çš„æ¥å£å…¼å®¹æ€§")
        print("4. âœ… è®¡ç®—æ•ˆç‡ä¼˜åŒ–ï¼š72ä¸ªpatch vs 864ä¸ªæ—¶é—´æ­¥")
        print("5. âœ… æä¾›å­¦ä¹ åˆ°çš„é‚»æ¥çŸ©é˜µç”¨äºåˆ†æ")
    else:
        print("âŒ é›†æˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ")