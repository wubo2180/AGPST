# ğŸ”„ äº¤æ›¿æ—¶ç©ºç¼–ç è§£ç æ¶æ„è®¾è®¡

## ğŸ¯ æ ¸å¿ƒæ€æƒ³

ä¼ ç»Ÿ Transformer: æ—¶ç©ºæ··åˆç¼–ç  â†’ è§£ç  â†’ è¾“å‡º
**æ–°æ¶æ„**: åˆ†ç¦»æ—¶ç©º â†’ èåˆ â†’ è§£ç  â†’ å†ç¼–ç  â†’ èåˆ â†’ è¾“å‡º

### ä¼˜åŠ¿
1. **æ˜¾å¼åˆ†ç¦»æ—¶ç©ºä¾èµ–** - æ—¶é—´å’Œç©ºé—´å„è‡ªå»ºæ¨¡
2. **å¤šå±‚æŠ½è±¡** - é€šè¿‡äº¤æ›¿ç¼–ç è§£ç é€æ­¥æŠ½è±¡
3. **ç‰¹å¾ç²¾ç‚¼** - è§£ç åå†ç¼–ç å¯ä»¥ç²¾ç‚¼ç‰¹å¾
4. **ç±»ä¼¼ U-Net** - ç¼–ç å™¨-è§£ç å™¨-å†ç¼–ç å™¨çš„æ²™æ¼ç»“æ„

---

## ğŸ“ æ¶æ„è®¾è®¡

```
è¾“å…¥: (B, 12, N, 1)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      ç¬¬ä¸€å±‚: åˆ†ç¦»æ—¶ç©ºç¼–ç                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ—¶é—´ç¼–ç å™¨ (Temporal Encoder)          â”‚
â”‚    - å¯¹æ¯ä¸ªèŠ‚ç‚¹ç‹¬ç«‹ç¼–ç æ—¶é—´åºåˆ—          â”‚
â”‚    - (B, N, T, C) â†’ (B, N, T, D)       â”‚
â”‚                                         â”‚
â”‚  ç©ºé—´ç¼–ç å™¨ (Spatial Encoder)           â”‚
â”‚    - å¯¹æ¯ä¸ªæ—¶é—´æ­¥ç‹¬ç«‹ç¼–ç ç©ºé—´ä¾èµ–        â”‚
â”‚    - (B, T, N, C) â†’ (B, T, N, D)       â”‚
â”‚                                         â”‚
â”‚  æ—¶ç©ºèåˆ (Fusion)                      â”‚
â”‚    - èåˆæ—¶é—´å’Œç©ºé—´ç‰¹å¾                  â”‚
â”‚    - é—¨æ§æœºåˆ¶æˆ–æ³¨æ„åŠ›èåˆ                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ fused_features (B, T, N, D)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      ç¬¬äºŒå±‚: è§£ç  (ä¸­é—´è¡¨ç¤º)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ—¶ç©ºè§£ç å™¨ (ST Decoder)                â”‚
â”‚    - å°†èåˆç‰¹å¾è§£ç å›æ—¶ç©ºç»´åº¦            â”‚
â”‚    - å¯èƒ½æ”¹å˜ç»´åº¦ (Tâ†’T', Nâ†’N')          â”‚
â”‚    - (B, T, N, D) â†’ (B, T', N', D')    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ decoded_features
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      ç¬¬ä¸‰å±‚: å†æ¬¡åˆ†ç¦»æ—¶ç©ºç¼–ç  (ç²¾ç‚¼)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ—¶é—´å†ç¼–ç å™¨ (Temporal Re-Encoder)     â”‚
â”‚    - å¯¹è§£ç åçš„æ—¶é—´åºåˆ—å†æ¬¡ç¼–ç           â”‚
â”‚                                         â”‚
â”‚  ç©ºé—´å†ç¼–ç å™¨ (Spatial Re-Encoder)      â”‚
â”‚    - å¯¹è§£ç åçš„ç©ºé—´ç»“æ„å†æ¬¡ç¼–ç           â”‚
â”‚                                         â”‚
â”‚  æœ€ç»ˆèåˆ (Final Fusion)                â”‚
â”‚    - èåˆç²¾ç‚¼åçš„æ—¶ç©ºç‰¹å¾                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ refined_features
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      ç¬¬å››å±‚: é¢„æµ‹å¤´                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  è¾“å‡ºæŠ•å½± (Output Projection)           â”‚
â”‚    - æ˜ å°„åˆ°é¢„æµ‹ç»´åº¦                      â”‚
â”‚    - (B, T', N', D') â†’ (B, 12, N, 1)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ è¯¦ç»†å®ç°æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: å®Œå…¨åˆ†ç¦»æ—¶ç©º (æ¨è) â­â­â­â­â­

```python
class AlternatingSTEncoder(nn.Module):
    """äº¤æ›¿æ—¶ç©ºç¼–ç è§£ç æ¶æ„"""
    
    def __init__(self, num_nodes, embed_dim, num_heads, dropout, 
                 temporal_depth=2, spatial_depth=2):
        super().__init__()
        self.num_nodes = num_nodes
        self.embed_dim = embed_dim
        
        # ============ ç¬¬ä¸€å±‚: åˆ†ç¦»æ—¶ç©ºç¼–ç  ============
        
        # æ—¶é—´ç¼–ç å™¨ (å¯¹æ¯ä¸ªèŠ‚ç‚¹)
        self.temporal_encoder_1 = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=embed_dim,
                nhead=num_heads,
                dim_feedforward=embed_dim * 4,
                dropout=dropout,
                batch_first=True
            ),
            num_layers=temporal_depth
        )
        
        # ç©ºé—´ç¼–ç å™¨ (å¯¹æ¯ä¸ªæ—¶é—´æ­¥)
        self.spatial_encoder_1 = DynamicGraphConv(
            embed_dim=embed_dim,
            num_nodes=num_nodes,
            # ... å…¶ä»–å‚æ•°
        )
        
        # æ—¶ç©ºèåˆå±‚ 1
        self.fusion_1 = FusionModule(embed_dim)
        
        # ============ ç¬¬äºŒå±‚: è§£ç  ============
        
        self.st_decoder = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(
                d_model=embed_dim,
                nhead=num_heads,
                dim_feedforward=embed_dim * 4,
                dropout=dropout,
                batch_first=True
            ),
            num_layers=2
        )
        
        # å¯å­¦ä¹ çš„è§£ç æŸ¥è¯¢
        self.decoder_queries = nn.Parameter(torch.randn(1, 12, embed_dim))
        
        # ============ ç¬¬ä¸‰å±‚: å†æ¬¡åˆ†ç¦»æ—¶ç©ºç¼–ç  ============
        
        # æ—¶é—´å†ç¼–ç å™¨
        self.temporal_encoder_2 = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=embed_dim,
                nhead=num_heads,
                dim_feedforward=embed_dim * 4,
                dropout=dropout,
                batch_first=True
            ),
            num_layers=temporal_depth
        )
        
        # ç©ºé—´å†ç¼–ç å™¨
        self.spatial_encoder_2 = DynamicGraphConv(
            embed_dim=embed_dim,
            num_nodes=num_nodes,
        )
        
        # æ—¶ç©ºèåˆå±‚ 2
        self.fusion_2 = FusionModule(embed_dim)
        
        # ============ ç¬¬å››å±‚: é¢„æµ‹å¤´ ============
        
        self.output_projection = nn.Sequential(
            nn.Linear(embed_dim, embed_dim),
            nn.LayerNorm(embed_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim, 1)
        )
    
    def forward(self, x):
        """
        Args:
            x: (B, T, N, C)
        Returns:
            prediction: (B, pred_len, N, 1)
        """
        B, T, N, C = x.shape
        
        # ============ ç¬¬ä¸€å±‚: åˆ†ç¦»æ—¶ç©ºç¼–ç  ============
        
        # æ—¶é—´ç¼–ç : å¯¹æ¯ä¸ªèŠ‚ç‚¹ç‹¬ç«‹ç¼–ç 
        x_temporal = x.permute(0, 2, 1, 3)  # (B, N, T, C)
        x_temporal = x_temporal.reshape(B * N, T, C)
        temporal_features = self.temporal_encoder_1(x_temporal)  # (B*N, T, D)
        temporal_features = temporal_features.reshape(B, N, T, self.embed_dim)
        
        # ç©ºé—´ç¼–ç : å¯¹æ¯ä¸ªæ—¶é—´æ­¥ç‹¬ç«‹ç¼–ç 
        x_spatial = x  # (B, T, N, C)
        spatial_features = []
        for t in range(T):
            xt = x_spatial[:, t, :, :]  # (B, N, C)
            xt = xt.unsqueeze(2)  # (B, N, 1, C)
            spatial_t, _, _ = self.spatial_encoder_1(xt)  # (B, N, 1, D)
            spatial_features.append(spatial_t.squeeze(2))
        spatial_features = torch.stack(spatial_features, dim=1)  # (B, T, N, D)
        
        # æ—¶ç©ºèåˆ
        # temporal_features: (B, N, T, D)
        # spatial_features: (B, T, N, D)
        # éœ€è¦å¯¹é½ç»´åº¦
        temporal_features = temporal_features.permute(0, 2, 1, 3)  # (B, T, N, D)
        fused_features_1 = self.fusion_1(temporal_features, spatial_features)  # (B, T, N, D)
        
        # ============ ç¬¬äºŒå±‚: è§£ç  ============
        
        # å‡†å¤‡è§£ç å™¨æŸ¥è¯¢
        queries = self.decoder_queries.expand(B, -1, -1)  # (B, pred_len, D)
        
        # å‡†å¤‡è®°å¿† (å°†æ—¶ç©ºç‰¹å¾å±•å¹³)
        memory = fused_features_1.reshape(B, T * N, self.embed_dim)  # (B, T*N, D)
        
        # è§£ç 
        decoded = self.st_decoder(queries, memory)  # (B, pred_len, D)
        
        # æ‰©å±•åˆ°ç©ºé—´ç»´åº¦
        decoded = decoded.unsqueeze(2).expand(-1, -1, N, -1)  # (B, pred_len, N, D)
        
        # ============ ç¬¬ä¸‰å±‚: å†æ¬¡åˆ†ç¦»æ—¶ç©ºç¼–ç  ============
        
        pred_len = decoded.size(1)
        
        # æ—¶é—´å†ç¼–ç 
        x_temporal_2 = decoded.permute(0, 2, 1, 3)  # (B, N, pred_len, D)
        x_temporal_2 = x_temporal_2.reshape(B * N, pred_len, self.embed_dim)
        temporal_features_2 = self.temporal_encoder_2(x_temporal_2)  # (B*N, pred_len, D)
        temporal_features_2 = temporal_features_2.reshape(B, N, pred_len, self.embed_dim)
        
        # ç©ºé—´å†ç¼–ç 
        spatial_features_2 = []
        for t in range(pred_len):
            xt = decoded[:, t, :, :]  # (B, N, D)
            xt = xt.unsqueeze(2)  # (B, N, 1, D)
            spatial_t, _, _ = self.spatial_encoder_2(xt)  # (B, N, 1, D)
            spatial_features_2.append(spatial_t.squeeze(2))
        spatial_features_2 = torch.stack(spatial_features_2, dim=1)  # (B, pred_len, N, D)
        
        # æœ€ç»ˆèåˆ
        temporal_features_2 = temporal_features_2.permute(0, 2, 1, 3)  # (B, pred_len, N, D)
        fused_features_2 = self.fusion_2(temporal_features_2, spatial_features_2)  # (B, pred_len, N, D)
        
        # ============ ç¬¬å››å±‚: é¢„æµ‹ ============
        
        prediction = self.output_projection(fused_features_2)  # (B, pred_len, N, 1)
        
        return prediction
```

---

### èåˆæ¨¡å—è®¾è®¡

#### é€‰é¡¹ 1: é—¨æ§èåˆ (æ¨è)

```python
class FusionModule(nn.Module):
    """é—¨æ§æ—¶ç©ºèåˆ"""
    
    def __init__(self, embed_dim):
        super().__init__()
        # é—¨æ§æœºåˆ¶
        self.temporal_gate = nn.Linear(embed_dim * 2, embed_dim)
        self.spatial_gate = nn.Linear(embed_dim * 2, embed_dim)
        
        # ç‰¹å¾å˜æ¢
        self.temporal_transform = nn.Linear(embed_dim, embed_dim)
        self.spatial_transform = nn.Linear(embed_dim, embed_dim)
        
        self.layer_norm = nn.LayerNorm(embed_dim)
    
    def forward(self, temporal_features, spatial_features):
        """
        Args:
            temporal_features: (B, T, N, D)
            spatial_features: (B, T, N, D)
        Returns:
            fused: (B, T, N, D)
        """
        # æ‹¼æ¥
        concat = torch.cat([temporal_features, spatial_features], dim=-1)  # (B, T, N, 2D)
        
        # é—¨æ§æƒé‡
        temporal_weight = torch.sigmoid(self.temporal_gate(concat))  # (B, T, N, D)
        spatial_weight = torch.sigmoid(self.spatial_gate(concat))    # (B, T, N, D)
        
        # å½’ä¸€åŒ–æƒé‡
        total_weight = temporal_weight + spatial_weight + 1e-8
        temporal_weight = temporal_weight / total_weight
        spatial_weight = spatial_weight / total_weight
        
        # åŠ æƒèåˆ
        temporal_transformed = self.temporal_transform(temporal_features)
        spatial_transformed = self.spatial_transform(spatial_features)
        
        fused = temporal_weight * temporal_transformed + spatial_weight * spatial_transformed
        fused = self.layer_norm(fused)
        
        return fused
```

#### é€‰é¡¹ 2: äº¤å‰æ³¨æ„åŠ›èåˆ

```python
class CrossAttentionFusion(nn.Module):
    """äº¤å‰æ³¨æ„åŠ›æ—¶ç©ºèåˆ"""
    
    def __init__(self, embed_dim, num_heads=4):
        super().__init__()
        self.temporal_to_spatial = nn.MultiheadAttention(
            embed_dim, num_heads, batch_first=True
        )
        self.spatial_to_temporal = nn.MultiheadAttention(
            embed_dim, num_heads, batch_first=True
        )
        self.fusion = nn.Linear(embed_dim * 2, embed_dim)
    
    def forward(self, temporal_features, spatial_features):
        """
        Args:
            temporal_features: (B, T, N, D)
            spatial_features: (B, T, N, D)
        """
        B, T, N, D = temporal_features.shape
        
        # å±•å¹³æ—¶ç©ºç»´åº¦
        temporal_flat = temporal_features.reshape(B, T * N, D)
        spatial_flat = spatial_features.reshape(B, T * N, D)
        
        # äº¤å‰æ³¨æ„åŠ›
        t2s, _ = self.temporal_to_spatial(
            temporal_flat, spatial_flat, spatial_flat
        )  # (B, T*N, D)
        
        s2t, _ = self.spatial_to_temporal(
            spatial_flat, temporal_flat, temporal_flat
        )  # (B, T*N, D)
        
        # æ‹¼æ¥èåˆ
        fused_flat = torch.cat([t2s, s2t], dim=-1)  # (B, T*N, 2D)
        fused_flat = self.fusion(fused_flat)  # (B, T*N, D)
        
        # é‡å¡‘
        fused = fused_flat.reshape(B, T, N, D)
        
        return fused
```

---

## ğŸ¯ æ¶æ„å˜ä½“

### å˜ä½“ 1: å•æ¬¡å¾ªç¯ (è½»é‡)

```
è¾“å…¥ â†’ æ—¶ç©ºç¼–ç  â†’ èåˆ â†’ è§£ç  â†’ å†ç¼–ç  â†’ èåˆ â†’ è¾“å‡º
```

**å‚æ•°é…ç½®**:
```yaml
temporal_depth: 2
spatial_depth: 1
decoder_depth: 2
```

---

### å˜ä½“ 2: å¤šæ¬¡å¾ªç¯ (å¼ºå¤§)

```
è¾“å…¥ â†’ [æ—¶ç©ºç¼–ç  â†’ èåˆ â†’ è§£ç ] Ã— N â†’ å†ç¼–ç  â†’ èåˆ â†’ è¾“å‡º
```

**å®ç°**:
```python
for i in range(num_cycles):
    # æ—¶ç©ºç¼–ç 
    temporal = temporal_encoder[i](x)
    spatial = spatial_encoder[i](x)
    
    # èåˆ
    fused = fusion[i](temporal, spatial)
    
    # è§£ç  (å¦‚æœä¸æ˜¯æœ€åä¸€è½®)
    if i < num_cycles - 1:
        x = decoder[i](fused)
```

---

### å˜ä½“ 3: U-Net é£æ ¼ (å¸¦è·³è·ƒè¿æ¥)

```python
# ç¼–ç é˜¶æ®µ
enc1 = encode_layer_1(x)
enc2 = encode_layer_2(enc1)

# è§£ç é˜¶æ®µ
dec2 = decode_layer_2(enc2)
dec1 = decode_layer_1(dec2 + enc1)  # è·³è·ƒè¿æ¥

# å†ç¼–ç 
output = encode_layer_3(dec1 + x)  # è·³è·ƒè¿æ¥
```

---

## ğŸ“Š ä¸ç°æœ‰æ¶æ„å¯¹æ¯”

| ç‰¹æ€§ | å• Encoder-Decoder | äº¤æ›¿æ—¶ç©ºæ¶æ„ |
|------|-------------------|-------------|
| **æ—¶ç©ºå»ºæ¨¡** | æ··åˆ | æ˜¾å¼åˆ†ç¦» |
| **æŠ½è±¡å±‚æ¬¡** | å•å±‚ | å¤šå±‚ (ç¼–ç â†’è§£ç â†’å†ç¼–ç ) |
| **ç‰¹å¾ç²¾ç‚¼** | ä¸€æ¬¡æ€§ | å¾ªç¯ç²¾ç‚¼ |
| **å‚æ•°é‡** | ä¸­ | è¾ƒå¤§ |
| **è¡¨è¾¾èƒ½åŠ›** | ä¸­-å¼º | å¾ˆå¼º |
| **è®¡ç®—å¤æ‚åº¦** | O(NÃ—TÂ²Ã—D) | O(NÃ—TÂ²Ã—D + TÃ—NÂ²Ã—D) |

---

## ğŸ’¡ ä¼˜åŒ–å»ºè®®

### 1. å‡å°‘è®¡ç®—é‡

**é—®é¢˜**: ç©ºé—´ç¼–ç éœ€è¦å¯¹æ¯ä¸ªæ—¶é—´æ­¥å•ç‹¬è®¡ç®—
**è§£å†³**: æ‰¹å¤„ç†æ—¶é—´ç»´åº¦

```python
# æ—§ç‰ˆ: å¾ªç¯
for t in range(T):
    spatial_t = spatial_encoder(x[:, t, :, :])

# æ–°ç‰ˆ: æ‰¹å¤„ç†
x_batched = x.reshape(B * T, N, C)
spatial_all = spatial_encoder(x_batched)
spatial_features = spatial_all.reshape(B, T, N, D)
```

---

### 2. å…±äº«å‚æ•°

**ç­–ç•¥**: ç¬¬ä¸€å±‚å’Œç¬¬äºŒå±‚çš„ç¼–ç å™¨å…±äº«å‚æ•°

```python
# å…±äº«æ—¶é—´ç¼–ç å™¨
self.temporal_encoder = nn.TransformerEncoder(...)
# ç¬¬ä¸€å±‚å’Œç¬¬äºŒå±‚éƒ½ç”¨è¿™ä¸ª

temporal_features_1 = self.temporal_encoder(x1)
temporal_features_2 = self.temporal_encoder(x2)
```

**å¥½å¤„**:
- å‚æ•°é‡å‡å°‘ 50%
- å­¦ä¹ æ›´é€šç”¨çš„æ—¶é—´æ¨¡å¼

---

### 3. æ¸è¿›å¼ç»´åº¦

```python
# ç¬¬ä¸€å±‚: é«˜ç»´
temporal_encoder_1: D=96

# è§£ç : é™ç»´
decoder: D=96 â†’ D=64

# ç¬¬äºŒå±‚: æ¢å¤
temporal_encoder_2: D=64 â†’ D=96
```

---

## ğŸš€ å®ç°ä¼˜å…ˆçº§

### Phase 1: åŸºç¡€ç‰ˆæœ¬ (1-2å°æ—¶)
1. âœ… å®ç°åŸºæœ¬çš„æ—¶ç©ºåˆ†ç¦»ç¼–ç 
2. âœ… å®ç°é—¨æ§èåˆæ¨¡å—
3. âœ… å®ç°å•æ¬¡è§£ç -å†ç¼–ç 

### Phase 2: ä¼˜åŒ–ç‰ˆæœ¬ (3-4å°æ—¶)
4. âœ… æ·»åŠ è·³è·ƒè¿æ¥
5. âœ… æ‰¹å¤„ç†ä¼˜åŒ–
6. âœ… å‚æ•°å…±äº«

### Phase 3: é«˜çº§ç‰ˆæœ¬ (1å¤©)
7. âœ… å¤šæ¬¡å¾ªç¯
8. âœ… äº¤å‰æ³¨æ„åŠ›èåˆ
9. âœ… è‡ªé€‚åº”èåˆæƒé‡

---

## ğŸ“ é…ç½®ç¤ºä¾‹

```yaml
# parameters/PEMS03_alternating.yaml

MODEL:
  NAME: AlternatingSTEncoder
  PARAM:
    num_nodes: 358
    embed_dim: 96
    num_heads: 4
    dropout: 0.05
    
    # æ—¶ç©ºç¼–ç æ·±åº¦
    temporal_depth: 2
    spatial_depth: 1
    
    # è§£ç å™¨æ·±åº¦
    decoder_depth: 2
    
    # èåˆæ–¹å¼
    fusion_type: 'gated'  # 'gated' or 'cross_attention'
    
    # æ˜¯å¦å…±äº«å‚æ•°
    share_encoders: False
    
    # æ˜¯å¦ä½¿ç”¨è·³è·ƒè¿æ¥
    use_skip_connections: True
```

---

## âœ… æ€»ç»“

**æ ¸å¿ƒåˆ›æ–°**:
1. ğŸŒŸ **æ˜¾å¼åˆ†ç¦»æ—¶ç©º** - æ—¶é—´å’Œç©ºé—´å„è‡ªå»ºæ¨¡
2. ğŸŒŸ **äº¤æ›¿ç¼–ç è§£ç ** - ç¼–ç  â†’ è§£ç  â†’ å†ç¼–ç 
3. ğŸŒŸ **å¤šå±‚æŠ½è±¡** - é€æ­¥ç²¾ç‚¼ç‰¹å¾
4. ğŸŒŸ **é—¨æ§èåˆ** - è‡ªé€‚åº”æƒè¡¡æ—¶ç©ºä¿¡æ¯

**é¢„æœŸæ•ˆæœ**:
- æ›´å¼ºçš„æ—¶ç©ºå»ºæ¨¡èƒ½åŠ›
- æ›´å¥½çš„ç‰¹å¾è¡¨ç¤º
- å¯èƒ½çš„æ€§èƒ½æå‡: +5-10% MAE

**æŒ‘æˆ˜**:
- å‚æ•°é‡å¢åŠ  ~50%
- è®¡ç®—é‡å¢åŠ  ~30%
- éœ€è¦æ›´å¤šè®­ç»ƒæ—¶é—´

**å»ºè®®**: å…ˆå®ç°åŸºç¡€ç‰ˆæœ¬,éªŒè¯æ•ˆæœåå†ä¼˜åŒ–!
