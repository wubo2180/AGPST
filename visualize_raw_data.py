"""
å¯è§†åŒ–åŸå§‹æ•°æ® - å¿«é€Ÿæ£€æŸ¥æ•°æ®è´¨é‡
"""
import numpy as np
import matplotlib.pyplot as plt
import os
plt.rcParams['font.sans-serif'] = ['SimHei']


def load_dataset(dataset_name='PEMS03', mode='train'):
    """åŠ è½½æ•°æ®é›†"""
    data_path = f'datasets/{dataset_name}/{mode}_data.npy'
    
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        print(f"è¯·æ£€æŸ¥æ•°æ®é›†è·¯å¾„")
        return None
    
    data = np.load(data_path)
    print(f"âœ… åŠ è½½æ•°æ®: {data_path}")
    print(f"   æ•°æ®å½¢çŠ¶: {data.shape}")
    print(f"   æ•°æ®ç±»å‹: {data.dtype}")
    print(f"   æ•°å€¼èŒƒå›´: [{data.min():.2f}, {data.max():.2f}]")
    print(f"   å‡å€¼: {data.mean():.2f}")
    print(f"   æ ‡å‡†å·®: {data.std():.2f}")
    
    return data


def plot_time_series(data, num_samples=5, sample_nodes=None, save_path='figure/raw_data_time_series.png'):
    """ç»˜åˆ¶æ—¶é—´åºåˆ—"""
    T, N = data.shape
    
    if sample_nodes is None:
        # éšæœºé€‰æ‹©èŠ‚ç‚¹
        sample_nodes = np.random.choice(N, num_samples, replace=False)
    else:
        num_samples = len(sample_nodes)
    
    # åªæ˜¾ç¤ºå‰500ä¸ªæ—¶é—´æ­¥ä»¥ä¾¿æŸ¥çœ‹ç»†èŠ‚
    time_window = min(500, T)
    
    fig, axes = plt.subplots(num_samples, 1, figsize=(14, 2.5 * num_samples))
    if num_samples == 1:
        axes = [axes]
    
    for idx, node_id in enumerate(sample_nodes):
        ax = axes[idx]
        time_series = data[:time_window, node_id]
        
        ax.plot(time_series, linewidth=1.0, alpha=0.8, color='steelblue')
        ax.set_title(f'èŠ‚ç‚¹ {node_id} çš„æ—¶é—´åºåˆ—', fontsize=11, fontweight='bold')
        ax.set_xlabel('æ—¶é—´æ­¥', fontsize=10)
        ax.set_ylabel('æ•°å€¼', fontsize=10)
        ax.grid(True, alpha=0.3, linestyle='--')
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        mean_val = time_series.mean()
        std_val = time_series.std()
        ax.axhline(mean_val, color='red', linestyle='--', linewidth=1, alpha=0.7, label=f'å‡å€¼: {mean_val:.2f}')
        ax.fill_between(range(time_window), mean_val - std_val, mean_val + std_val, 
                        color='red', alpha=0.1, label=f'Â±1Ïƒ: {std_val:.2f}')
        ax.legend(loc='upper right', fontsize=9)
    
    plt.tight_layout()
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\nğŸ“Š æ—¶é—´åºåˆ—å›¾å·²ä¿å­˜: {save_path}")
    plt.close()


def plot_distribution(data, save_path='figure/raw_data_distribution.png'):
    """ç»˜åˆ¶æ•°æ®åˆ†å¸ƒ"""
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))
    
    # å…¨å±€åˆ†å¸ƒ
    ax1 = axes[0]
    ax1.hist(data.flatten(), bins=100, color='steelblue', alpha=0.7, edgecolor='black')
    ax1.set_title('å…¨å±€æ•°æ®åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    ax1.set_xlabel('æ•°å€¼', fontsize=10)
    ax1.set_ylabel('é¢‘æ•°', fontsize=10)
    ax1.grid(True, alpha=0.3)
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    mean_val = data.mean()
    std_val = data.std()
    ax1.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'å‡å€¼: {mean_val:.2f}')
    ax1.axvline(mean_val + std_val, color='orange', linestyle='--', linewidth=1, label=f'+1Ïƒ: {mean_val+std_val:.2f}')
    ax1.axvline(mean_val - std_val, color='orange', linestyle='--', linewidth=1, label=f'-1Ïƒ: {mean_val-std_val:.2f}')
    ax1.legend(fontsize=9)
    
    # æ¯ä¸ªèŠ‚ç‚¹çš„å‡å€¼å’Œæ ‡å‡†å·®
    ax2 = axes[1]
    node_means = data.mean(axis=0)
    node_stds = data.std(axis=0)
    
    ax2.scatter(node_means, node_stds, alpha=0.5, s=20, color='steelblue')
    ax2.set_title('å„èŠ‚ç‚¹ç»Ÿè®¡ç‰¹å¾', fontsize=12, fontweight='bold')
    ax2.set_xlabel('èŠ‚ç‚¹å‡å€¼', fontsize=10)
    ax2.set_ylabel('èŠ‚ç‚¹æ ‡å‡†å·®', fontsize=10)
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š æ•°æ®åˆ†å¸ƒå›¾å·²ä¿å­˜: {save_path}")
    plt.close()


def plot_correlation_heatmap(data, max_nodes=50, save_path='figure/raw_data_correlation.png'):
    """ç»˜åˆ¶èŠ‚ç‚¹ç›¸å…³æ€§çƒ­å›¾"""
    T, N = data.shape
    
    # åªæ˜¾ç¤ºéƒ¨åˆ†èŠ‚ç‚¹ä»¥ä¾¿æŸ¥çœ‹
    sample_nodes = min(max_nodes, N)
    node_indices = np.linspace(0, N-1, sample_nodes, dtype=int)
    
    # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
    data_sample = data[:, node_indices]
    corr_matrix = np.corrcoef(data_sample.T)
    
    fig, ax = plt.subplots(figsize=(10, 8))
    im = ax.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1, aspect='auto')
    
    ax.set_title(f'èŠ‚ç‚¹ç›¸å…³æ€§çƒ­å›¾ (å‰{sample_nodes}ä¸ªèŠ‚ç‚¹)', fontsize=12, fontweight='bold')
    ax.set_xlabel('èŠ‚ç‚¹ç´¢å¼•', fontsize=10)
    ax.set_ylabel('èŠ‚ç‚¹ç´¢å¼•', fontsize=10)
    
    # æ·»åŠ é¢œè‰²æ¡
    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    cbar.set_label('ç›¸å…³ç³»æ•°', fontsize=10)
    
    plt.tight_layout()
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š ç›¸å…³æ€§çƒ­å›¾å·²ä¿å­˜: {save_path}")
    plt.close()


def analyze_basic_stats(data):
    """åˆ†æåŸºæœ¬ç»Ÿè®¡ä¿¡æ¯"""
    print("\n" + "="*60)
    print("ğŸ“ˆ åŸºæœ¬ç»Ÿè®¡åˆ†æ")
    print("="*60)
    
    T, N = data.shape
    
    print(f"\næ•°æ®ç»´åº¦:")
    print(f"  æ—¶é—´æ­¥æ•°: {T}")
    print(f"  èŠ‚ç‚¹æ•°é‡: {N}")
    print(f"  æ€»æ ·æœ¬æ•°: {T * N:,}")
    
    print(f"\nå…¨å±€ç»Ÿè®¡:")
    print(f"  æœ€å°å€¼: {data.min():.4f}")
    print(f"  æœ€å¤§å€¼: {data.max():.4f}")
    print(f"  å‡å€¼: {data.mean():.4f}")
    print(f"  ä¸­ä½æ•°: {np.median(data):.4f}")
    print(f"  æ ‡å‡†å·®: {data.std():.4f}")
    print(f"  å˜å¼‚ç³»æ•°: {data.std()/data.mean():.4f}")
    
    # æ£€æµ‹å¼‚å¸¸å€¼ (ä½¿ç”¨3Ïƒå‡†åˆ™)
    mean_val = data.mean()
    std_val = data.std()
    outliers = np.abs(data - mean_val) > 3 * std_val
    outlier_ratio = outliers.sum() / data.size * 100
    
    print(f"\nå¼‚å¸¸å€¼æ£€æµ‹ (3Ïƒå‡†åˆ™):")
    print(f"  å¼‚å¸¸å€¼æ•°é‡: {outliers.sum():,}")
    print(f"  å¼‚å¸¸å€¼æ¯”ä¾‹: {outlier_ratio:.2f}%")
    
    if outlier_ratio > 5:
        print(f"  âš ï¸  è­¦å‘Š: å¼‚å¸¸å€¼æ¯”ä¾‹è¾ƒé«˜ï¼Œå»ºè®®ä½¿ç”¨å»å™ª")
    elif outlier_ratio > 1:
        print(f"  â„¹ï¸  æç¤º: æœ‰ä¸€å®šå¼‚å¸¸å€¼ï¼Œå¯ä»¥è€ƒè™‘å»å™ª")
    else:
        print(f"  âœ… å¼‚å¸¸å€¼æ¯”ä¾‹è¾ƒä½ï¼Œæ•°æ®è´¨é‡è‰¯å¥½")
    
    # æ£€æŸ¥æ•°æ®å˜åŒ–ç‡
    diff = np.diff(data, axis=0)
    change_rate = np.abs(diff).mean()
    
    print(f"\næ—¶é—´åºåˆ—ç‰¹å¾:")
    print(f"  å¹³å‡å˜åŒ–ç‡: {change_rate:.4f}")
    print(f"  æœ€å¤§å˜åŒ–: {np.abs(diff).max():.4f}")
    
    return {
        'outlier_ratio': outlier_ratio,
        'change_rate': change_rate,
        'std': std_val
    }


def generate_recommendation(stats):
    """æ ¹æ®ç»Ÿè®¡ä¿¡æ¯ç”Ÿæˆå»ºè®®"""
    print("\n" + "="*60)
    print("ğŸ’¡ å»å™ªå»ºè®®")
    print("="*60)
    
    outlier_ratio = stats['outlier_ratio']
    change_rate = stats['change_rate']
    std = stats['std']
    
    # ç»¼åˆè¯„åˆ†
    score = 0
    
    if outlier_ratio > 5:
        score += 3
        print(f"\nâŒ å¼‚å¸¸å€¼æ¯”ä¾‹é«˜ ({outlier_ratio:.2f}%) â†’ å¼ºçƒˆå»ºè®®å»å™ª")
    elif outlier_ratio > 1:
        score += 2
        print(f"\nâš ï¸  å¼‚å¸¸å€¼æ¯”ä¾‹ä¸­ç­‰ ({outlier_ratio:.2f}%) â†’ å»ºè®®å»å™ª")
    else:
        score += 0
        print(f"\nâœ… å¼‚å¸¸å€¼æ¯”ä¾‹ä½ ({outlier_ratio:.2f}%) â†’ æ•°æ®è´¨é‡å¥½")
    
    if std > 50:
        score += 2
        print(f"âš ï¸  æ ‡å‡†å·®è¾ƒå¤§ ({std:.2f}) â†’ å»ºè®®å»å™ª")
    elif std > 20:
        score += 1
        print(f"â„¹ï¸  æ ‡å‡†å·®ä¸­ç­‰ ({std:.2f}) â†’ å¯ä»¥è€ƒè™‘å»å™ª")
    else:
        print(f"âœ… æ ‡å‡†å·®è¾ƒå° ({std:.2f}) â†’ æ•°æ®ç¨³å®š")
    
    print("\n" + "-"*60)
    print("ğŸ“ æ¨èé…ç½®:")
    print("-"*60)
    
    if score >= 4:
        print("\nğŸ”´ å¼ºçƒˆå»ºè®®ä½¿ç”¨æ³¨æ„åŠ›å»å™ª:")
        print("```yaml")
        print("use_denoising: True")
        print("denoise_type: 'attention'")
        print("```")
    elif score >= 2:
        print("\nğŸŸ¡ å»ºè®®ä½¿ç”¨å·ç§¯å»å™ª:")
        print("```yaml")
        print("use_denoising: True")
        print("denoise_type: 'conv'")
        print("```")
    else:
        print("\nğŸŸ¢ æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå¯ä»¥ä¸ä½¿ç”¨å»å™ª:")
        print("```yaml")
        print("use_denoising: False")
        print("```")
        print("\nä½†ä¹Ÿå¯ä»¥å°è¯•è½»é‡çº§å»å™ªçœ‹æ˜¯å¦æœ‰æå‡:")
        print("```yaml")
        print("use_denoising: True")
        print("denoise_type: 'conv'")
        print("```")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "ğŸ” " + "="*58 + " ğŸ”")
    print("ğŸ”  åŸå§‹æ•°æ®å¯è§†åŒ–ä¸åˆ†æå·¥å…·")
    print("ğŸ” " + "="*58 + " ğŸ”\n")
    
    # åŠ è½½æ•°æ®
    dataset_name = 'PEMS03'
    mode = 'val'
    
    data = load_dataset(dataset_name, mode)
    
    if data is None:
        print("\nâŒ æ— æ³•åŠ è½½æ•°æ®ï¼Œç¨‹åºé€€å‡º")
        return
    
    # åŸºæœ¬ç»Ÿè®¡åˆ†æ
    stats = analyze_basic_stats(data)
    
    # å¯è§†åŒ–
    print("\n" + "="*60)
    print("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    print("="*60)
    
    # æ—¶é—´åºåˆ—å›¾ - é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§èŠ‚ç‚¹
    sample_nodes = [0, 50, 100, 150, 200]  # å¯ä»¥è‡ªå®šä¹‰
    plot_time_series(data, sample_nodes=sample_nodes)
    
    # æ•°æ®åˆ†å¸ƒå›¾
    plot_distribution(data)
    
    # ç›¸å…³æ€§çƒ­å›¾
    plot_correlation_heatmap(data)
    
    # ç”Ÿæˆå»ºè®®
    generate_recommendation(stats)
    
    print("\n" + "="*60)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("="*60)
    print("\nğŸ“‚ æ‰€æœ‰å›¾è¡¨å·²ä¿å­˜åˆ° 'figure/' ç›®å½•")
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("   1. æŸ¥çœ‹ç”Ÿæˆçš„å›¾è¡¨")
    print("   2. æ ¹æ®å»ºè®®é…ç½®å»å™ªå‚æ•°")
    print("   3. è¿è¡Œ analyze_noise.py è¿›è¡Œæ·±å…¥åˆ†æ")
    print("   4. å¼€å§‹è®­ç»ƒæ¨¡å‹\n")


if __name__ == '__main__':
    main()
