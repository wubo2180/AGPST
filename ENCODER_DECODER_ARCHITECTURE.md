# Encoder-Decoder æ¶æ„é‡æ„æ–‡æ¡£

## ğŸ¯ æ¶æ„æ¦‚è§ˆ

ä» **å•ç¼–ç å™¨ + MLP é¢„æµ‹å¤´** å‡çº§åˆ° **Encoder-Decoder æ¶æ„**

### æ ¸å¿ƒæ”¹è¿›
âœ… **æ˜ç¡®çš„å†å²-æœªæ¥å»ºæ¨¡** - ç¼–ç å™¨å¤„ç†å†å²ï¼Œè§£ç å™¨ç”Ÿæˆæœªæ¥  
âœ… **äº¤å‰æ³¨æ„åŠ›æœºåˆ¶** - è§£ç å™¨å¯ä»¥å…³æ³¨ç¼–ç å™¨çš„æ‰€æœ‰æ—¶é—´æ­¥  
âœ… **å¯å­¦ä¹ çš„æœªæ¥æŸ¥è¯¢** - ä»£è¡¨æœªæ¥æ—¶é—´æ­¥çš„è¯­ä¹‰  
âœ… **æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›** - ç»å…¸çš„åºåˆ—åˆ°åºåˆ—æ¶æ„  

---

## ğŸ“ æ–°æ¶æ„è¯¦è§£

```
è¾“å…¥: (B, 12, N, 1)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          ENCODER éƒ¨åˆ†                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  0. å»å™ªæ¨¡å— (Conv/Attention)        â”‚
â”‚     â†“                                â”‚
â”‚  1. æ—¶é—´ç‰¹å¾åµŒå…¥ (1 â†’ embed_dim)     â”‚
â”‚     â†“                                â”‚
â”‚  2. ç¼–ç å™¨ä½ç½®ç¼–ç                     â”‚
â”‚     â†“                                â”‚
â”‚  3. è‡ªé€‚åº”å›¾å­¦ä¹  + åŠ¨æ€å›¾å·ç§¯         â”‚
â”‚     â†“                                â”‚
â”‚  4. Transformer Encoder (Nå±‚)       â”‚
â”‚     â†“                                â”‚
â”‚  è¾“å‡º: encoder_memory (B*N, 12, D)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          DECODER éƒ¨åˆ†                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  5. å¯å­¦ä¹ çš„æœªæ¥æŸ¥è¯¢å‘é‡              â”‚
â”‚     future_queries (pred_len, D)    â”‚
â”‚     â†“                                â”‚
â”‚  6. è§£ç å™¨ä½ç½®ç¼–ç                     â”‚
â”‚     â†“                                â”‚
â”‚  7. Transformer Decoder (2å±‚)        â”‚
â”‚     - Self-Attention                â”‚
â”‚     - Cross-Attention (with memory) â”‚
â”‚     - FeedForward                   â”‚
â”‚     â†“                                â”‚
â”‚  8. è¾“å‡ºæŠ•å½±å±‚ (D â†’ 1)               â”‚
â”‚     â†“                                â”‚
â”‚  è¾“å‡º: prediction (B, pred_len, N, 1)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”‘ å…³é”®ç»„ä»¶è¯´æ˜

### 1. ç¼–ç å™¨ (Encoder)

**ä½œç”¨**: æå–å†å²æ•°æ®çš„æ—¶ç©ºç‰¹å¾

```python
# è¾“å…¥: (B, 12, N, 1)
# è¾“å‡º: (B*N, 12, embed_dim)

encoder_layer = nn.TransformerEncoderLayer(
    d_model=embed_dim,
    nhead=num_heads,
    dim_feedforward=embed_dim * mlp_ratio,
    dropout=dropout,
    batch_first=True
)
self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=encoder_depth)
```

**ç‰¹ç‚¹**:
- âœ… ä¿ç•™æ‰€æœ‰å†å²æ—¶é—´æ­¥çš„ä¿¡æ¯
- âœ… é€šè¿‡è‡ªæ³¨æ„åŠ›æ•è·æ—¶é—´ä¾èµ–
- âœ… æ¯ä¸ªèŠ‚ç‚¹ç‹¬ç«‹ç¼–ç ï¼Œç©ºé—´ä¾èµ–ç”±å›¾å·ç§¯å¤„ç†

---

### 2. å¯å­¦ä¹ çš„æœªæ¥æŸ¥è¯¢ (Future Queries)

**ä½œç”¨**: ä»£è¡¨æœªæ¥æ—¶é—´æ­¥çš„è¯­ä¹‰è¡¨ç¤º

```python
# å¯å­¦ä¹ å‚æ•°: (1, pred_len, embed_dim)
self.future_queries = nn.Parameter(torch.randn(1, pred_len, embed_dim))

# ä½¿ç”¨æ—¶æ‰©å±•åˆ°æ‰¹æ¬¡å’ŒèŠ‚ç‚¹ç»´åº¦:
queries = self.future_queries.expand(B * N, -1, -1)  # (B*N, pred_len, D)
```

**è®¾è®¡ç†å¿µ**:
- ğŸ“Œ æ¯ä¸ªæ—¶é—´æ­¥æœ‰ç‹¬ç«‹çš„æŸ¥è¯¢å‘é‡
- ğŸ“Œ é€šè¿‡è®­ç»ƒå­¦ä¹ åˆ°"æœªæ¥ç¬¬1æ­¥"ã€"æœªæ¥ç¬¬2æ­¥"ç­‰çš„æŠ½è±¡è¡¨ç¤º
- ğŸ“Œ ç±»ä¼¼äº BERT çš„ [CLS] tokenï¼Œæ˜¯å¯å­¦ä¹ çš„ä»»åŠ¡ç‰¹å®šè¡¨ç¤º

**è®­ç»ƒè¿‡ç¨‹**:
- åˆå§‹æ—¶åˆ»ï¼šéšæœºåˆå§‹åŒ–
- è®­ç»ƒä¸­ï¼šé€šè¿‡åå‘ä¼ æ’­å­¦ä¹ åˆ°æœ€ä¼˜çš„æŸ¥è¯¢æ¨¡å¼
- è®­ç»ƒåï¼šæ¯ä¸ªæŸ¥è¯¢å‘é‡ä»£è¡¨äº†"å¦‚ä½•ä»å†å²ä¸­æå–ä¿¡æ¯ä»¥é¢„æµ‹è¯¥æ—¶é—´æ­¥"

---

### 3. è§£ç å™¨ (Decoder)

**ä½œç”¨**: åŸºäºç¼–ç å™¨è®°å¿†ç”Ÿæˆæœªæ¥é¢„æµ‹

```python
decoder_layer = nn.TransformerDecoderLayer(
    d_model=embed_dim,
    nhead=num_heads,
    dim_feedforward=embed_dim * mlp_ratio,
    dropout=dropout,
    batch_first=True
)
self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=decoder_depth)
```

**å†…éƒ¨æœºåˆ¶** (æ¯ä¸€å±‚):
```
Query (æœªæ¥å‘é‡)
    â†“
Self-Attention (æœªæ¥æ­¥ä¹‹é—´çš„äº¤äº’)
    â†“
Cross-Attention (æŸ¥è¯¢å†å²ä¿¡æ¯)
    â†“ 
    Query â†â†’ Memory (encoder output)
    â†“
FeedForward (ç‰¹å¾å˜æ¢)
    â†“
è¾“å‡º
```

**Cross-Attention è¯¦è§£**:
```python
# ä¼ªä»£ç 
Q = future_queries  # (B*N, pred_len, D) - æœªæ¥çš„æŸ¥è¯¢
K = encoder_output  # (B*N, 12, D) - å†å²çš„é”®
V = encoder_output  # (B*N, 12, D) - å†å²çš„å€¼

# è®¡ç®—æ³¨æ„åŠ›: æœªæ¥æ¯ä¸€æ­¥å…³æ³¨å†å²çš„å“ªäº›éƒ¨åˆ†
attention_scores = softmax(Q @ K^T / sqrt(D))  # (B*N, pred_len, 12)
output = attention_scores @ V  # (B*N, pred_len, D)
```

**å…³é”®ä¼˜åŠ¿**:
- âœ… æ¯ä¸ªæœªæ¥æ—¶é—´æ­¥å¯ä»¥ **é€‰æ‹©æ€§åœ°å…³æ³¨** ä¸åŒçš„å†å²æ—¶é—´æ­¥
- âœ… è‡ªåŠ¨å­¦ä¹ "é¢„æµ‹æœªæ¥ç¬¬tæ­¥æ—¶ï¼Œåº”è¯¥é‡ç‚¹çœ‹å†å²çš„å“ªäº›éƒ¨åˆ†"
- âœ… æ¯”å›ºå®šçš„"åªçœ‹æœ€åä¸€æ­¥"æ›´çµæ´»

---

### 4. è¾“å‡ºæŠ•å½±å±‚

**ä½œç”¨**: å°†è§£ç å™¨ç‰¹å¾æ˜ å°„ä¸ºæœ€ç»ˆé¢„æµ‹å€¼

```python
self.output_projection = nn.Sequential(
    nn.Linear(embed_dim, embed_dim // 2),
    nn.ReLU(),
    nn.Dropout(dropout),
    nn.Linear(embed_dim // 2, 1)  # æœ€ç»ˆé¢„æµ‹å€¼
)
```

---

## ğŸ†š ä¸æ—§æ¶æ„å¯¹æ¯”

### æ—§æ¶æ„ (å•ç¼–ç å™¨ + MLP)

```python
# ç¼–ç å™¨
encoder_output = encoder(history)  # (B*N, 12, D)

# åªä½¿ç”¨æœ€åä¸€æ­¥
last_step = encoder_output[:, -1, :]  # (B*N, D)

# MLP ç›´æ¥é¢„æµ‹æ‰€æœ‰æœªæ¥æ­¥
prediction = MLP(last_step)  # (B*N, 12)
```

**é—®é¢˜**:
- âŒ åªç”¨æœ€åä¸€æ­¥ï¼Œä¸¢å¤±äº†å…¶ä»–å†å²ä¿¡æ¯
- âŒ æ‰€æœ‰æœªæ¥æ­¥å…±äº«åŒä¸€ä¸ªè¾“å…¥ç‰¹å¾
- âŒ æ— æ³•å­¦ä¹ "ä¸åŒæœªæ¥æ­¥éœ€è¦ä¸åŒå†å²ä¿¡æ¯"

### æ–°æ¶æ„ (Encoder-Decoder)

```python
# ç¼–ç å™¨
encoder_memory = encoder(history)  # (B*N, 12, D) - ä¿ç•™æ‰€æœ‰å†å²

# è§£ç å™¨
future_queries = learned_queries  # (B*N, pred_len, D) - æ¯æ­¥ç‹¬ç«‹æŸ¥è¯¢

# äº¤å‰æ³¨æ„åŠ›: æ¯ä¸ªæœªæ¥æ­¥ä»æ‰€æœ‰å†å²ä¸­æå–ä¿¡æ¯
decoder_output = decoder(future_queries, encoder_memory)  # (B*N, pred_len, D)

# æŠ•å½±
prediction = projection(decoder_output)  # (B*N, pred_len, 1)
```

**ä¼˜åŠ¿**:
- âœ… ä½¿ç”¨æ‰€æœ‰å†å²ä¿¡æ¯ï¼Œä¸ä¸¢å¤±ç»†èŠ‚
- âœ… æ¯ä¸ªæœªæ¥æ­¥æœ‰ç‹¬ç«‹çš„æŸ¥è¯¢å‘é‡
- âœ… è‡ªåŠ¨å­¦ä¹ æœ€ä¼˜çš„å†å²-æœªæ¥å…³è”æ¨¡å¼

---

## ğŸ’¡ è®¾è®¡æ€æƒ³

### 1. ä¸ºä»€ä¹ˆéœ€è¦ Encoder-Decoderï¼Ÿ

**åœºæ™¯**: é¢„æµ‹æœªæ¥ 12 æ­¥äº¤é€šæµé‡

**ç›´è§‰**:
- é¢„æµ‹"æœªæ¥ç¬¬1æ­¥"å¯èƒ½éœ€è¦å…³æ³¨"å†å²æœ€å2æ­¥"çš„è¶‹åŠ¿
- é¢„æµ‹"æœªæ¥ç¬¬6æ­¥"å¯èƒ½éœ€è¦å…³æ³¨"å†å²ä¸­æœŸ"çš„å‘¨æœŸæ¨¡å¼
- é¢„æµ‹"æœªæ¥ç¬¬12æ­¥"å¯èƒ½éœ€è¦ç»¼åˆè€ƒè™‘"æ•´ä¸ªå†å²åºåˆ—"çš„è¶‹åŠ¿

**Encoder-Decoder å¦‚ä½•å®ç°**:
```python
# ç¼–ç å™¨: æå–æ‰€æœ‰å†å²ä¿¡æ¯
encoder_memory[0:12] = [hâ‚€, hâ‚, ..., hâ‚â‚]  # 12 æ­¥å†å²çš„è¡¨ç¤º

# è§£ç å™¨: æ¯ä¸ªæœªæ¥æ­¥ç‹¬ç«‹æŸ¥è¯¢
for t in range(pred_len):
    # å¯å­¦ä¹ çš„æŸ¥è¯¢å‘é‡
    q_t = future_queries[t]  
    
    # äº¤å‰æ³¨æ„åŠ›: æŸ¥è¯¢å†å²
    # "é¢„æµ‹ç¬¬tæ­¥æ—¶ï¼Œåº”è¯¥é‡ç‚¹çœ‹å†å²çš„å“ªäº›éƒ¨åˆ†ï¼Ÿ"
    attention_t = softmax(q_t @ encoder_memory^T)
    
    # åŠ æƒæå–å†å²ä¿¡æ¯
    context_t = attention_t @ encoder_memory
    
    # ç”Ÿæˆé¢„æµ‹
    pred_t = projection(context_t)
```

### 2. æœªæ¥æŸ¥è¯¢å‘é‡çš„å­¦ä¹ è¿‡ç¨‹

**åˆå§‹åŒ–**:
```python
# éšæœºåˆå§‹åŒ–
future_queries = torch.randn(pred_len, embed_dim)
# ä¾‹å¦‚: pred_len=12, åˆ™æœ‰ 12 ä¸ªä¸åŒçš„æŸ¥è¯¢å‘é‡
```

**è®­ç»ƒç¬¬1è½®**:
```python
# å‡è®¾ future_queries[0] æ˜¯é¢„æµ‹ç¬¬1æ­¥çš„æŸ¥è¯¢
# åˆå§‹çŠ¶æ€: éšæœºå‘é‡ï¼Œä¸çŸ¥é“è¯¥æŸ¥ä»€ä¹ˆ

# å‰å‘ä¼ æ’­
q_0 = future_queries[0]
attention_weights = softmax(q_0 @ encoder_memory^T)
# å¯èƒ½: [0.08, 0.09, 0.07, ..., 0.11, 0.10]  # å‡ ä¹å‡åŒ€

# åå‘ä¼ æ’­
loss = MSE(prediction[0], target[0])
# æ¢¯åº¦å‘Šè¯‰ q_0: "ä½ åº”è¯¥æ›´å…³æ³¨æœ€åå‡ æ­¥ï¼Œè€Œä¸æ˜¯å‡åŒ€åˆ†å¸ƒ"
```

**è®­ç»ƒç¬¬Nè½®å**:
```python
# future_queries[0] å·²ç»å­¦ä¹ åˆ°:
attention_weights = softmax(q_0 @ encoder_memory^T)
# å˜æˆ: [0.02, 0.03, 0.04, ..., 0.25, 0.35, 0.20]  # é›†ä¸­åœ¨æœ€åå‡ æ­¥

# future_queries[11] (é¢„æµ‹ç¬¬12æ­¥) å¯èƒ½å­¦åˆ°:
attention_weights = softmax(q_11 @ encoder_memory^T)
# å˜æˆ: [0.12, 0.10, 0.11, ..., 0.05, 0.04, 0.03]  # æ›´å…³æ³¨æ•´ä½“è¶‹åŠ¿
```

**æœ€ç»ˆç»“æœ**:
- `future_queries[0]` å­¦ä¼šäº†"çŸ­æœŸé¢„æµ‹æ¨¡å¼" (å…³æ³¨æœ€è¿‘)
- `future_queries[5]` å­¦ä¼šäº†"ä¸­æœŸé¢„æµ‹æ¨¡å¼" (å¹³è¡¡å…³æ³¨)
- `future_queries[11]` å­¦ä¼šäº†"é•¿æœŸé¢„æµ‹æ¨¡å¼" (å…³æ³¨æ•´ä½“)

---

## ğŸ“Š æ•°æ®æµç¤ºä¾‹

### å®Œæ•´å‰å‘ä¼ æ’­

```python
# è¾“å…¥
history_data = (B=32, T=12, N=358, C=1)  # 32æ‰¹æ¬¡, 12æ­¥å†å², 358èŠ‚ç‚¹, 1ç‰¹å¾

# ============ ENCODER ============
# å»å™ª
denoised = denoise(history_data)  # (32, 12, 358, 1)

# è½¬æ¢ + åµŒå…¥
x = time_embedding(denoised)  # (32, 358, 12, 96)

# ä½ç½®ç¼–ç 
x = x + encoder_pos_embed  # (32, 358, 12, 96)

# å›¾å­¦ä¹  + å›¾å·ç§¯
x = dynamic_graph_conv(x)  # (32, 358, 12, 96)

# Transformer ç¼–ç å™¨
x_flat = x.reshape(32*358, 12, 96)  # (11456, 12, 96)
encoder_output = encoder(x_flat)    # (11456, 12, 96)

# ============ DECODER ============
# æœªæ¥æŸ¥è¯¢
queries = future_queries.expand(11456, 12, 96)  # (11456, 12, 96)

# ä½ç½®ç¼–ç 
queries = queries + decoder_pos_embed  # (11456, 12, 96)

# Transformer è§£ç å™¨ (äº¤å‰æ³¨æ„åŠ›)
decoder_output = decoder(queries, encoder_output)  # (11456, 12, 96)

# è¾“å‡ºæŠ•å½±
prediction = output_projection(decoder_output)  # (11456, 12, 1)

# é‡å¡‘
prediction = prediction.reshape(32, 358, 12, 1)  # (32, 358, 12, 1)
prediction = prediction.permute(0, 2, 1, 3)      # (32, 12, 358, 1)
```

---

## ğŸ“ ç†è®ºæ”¯æ’‘

### 1. Transformer åŸå§‹è®ºæ–‡
- **"Attention is All You Need" (Vaswani et al., 2017)**
- Encoder-Decoder æ¶æ„ç”¨äºæœºå™¨ç¿»è¯‘
- å†å²åºåˆ— â†’ ç¼–ç å™¨ â†’ è®°å¿†
- æœªæ¥åºåˆ— â†’ è§£ç å™¨ â†’ é€æ­¥ç”Ÿæˆ

### 2. æ—¶åºé¢„æµ‹åº”ç”¨
- **Informer (Zhou et al., 2021)** - é•¿åºåˆ—æ—¶åºé¢„æµ‹
- **Autoformer (Wu et al., 2021)** - è¶‹åŠ¿-å­£èŠ‚åˆ†è§£
- **FEDformer (Zhou et al., 2022)** - é¢‘åŸŸå¢å¼º

### 3. äº¤é€šé¢„æµ‹åº”ç”¨
- **STTN (Xu et al., 2020)** - æ—¶ç©º Transformer
- **GMAN (Zheng et al., 2020)** - å›¾å¤šæ³¨æ„åŠ›ç½‘ç»œ
- éƒ½é‡‡ç”¨ Encoder-Decoder æ¶æ„å¤„ç†å†å²-æœªæ¥å»ºæ¨¡

---

## âš™ï¸ é…ç½®å‚æ•°

### æ–°å¢å‚æ•°

```python
AGPSTModel(
    # ... åŸæœ‰å‚æ•° ...
    
    # æ–°å¢: è§£ç å™¨æ·±åº¦
    decoder_depth=2,  # è§£ç å™¨å±‚æ•° (æ¨è 2-4)
    
    # é¢„æµ‹é•¿åº¦ (å·²æœ‰)
    pred_len=12,  # é¢„æµ‹æœªæ¥ 12 æ­¥
)
```

### å‚æ•°å»ºè®®

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| `encoder_depth` | 4-6 | ç¼–ç å™¨å±‚æ•°ï¼Œå¤„ç†å†å²å¤æ‚æ¨¡å¼ |
| `decoder_depth` | 2-3 | è§£ç å™¨å±‚æ•°ï¼Œç”Ÿæˆæœªæ¥é¢„æµ‹ |
| `num_heads` | 4-8 | æ³¨æ„åŠ›å¤´æ•°ï¼Œå¤šè§†è§’å…³æ³¨ |
| `embed_dim` | 96 | éšè—ç»´åº¦ï¼Œå¹³è¡¡æ€§èƒ½ä¸æ•ˆç‡ |
| `pred_len` | 12 | é¢„æµ‹æ­¥æ•°ï¼Œæ ¹æ®ä»»åŠ¡è°ƒæ•´ |

**ç»éªŒæ³•åˆ™**:
- ç¼–ç å™¨é€šå¸¸æ¯”è§£ç å™¨æ·± (ç¼–ç å™¨è´Ÿè´£æ›´å¤æ‚çš„ç‰¹å¾æå–)
- è§£ç å™¨ 2-3 å±‚é€šå¸¸è¶³å¤Ÿ (ä¸»è¦åšä¿¡æ¯æ•´åˆ)

---

## ğŸš€ é¢„æœŸæ”¹è¿›

### 1. æ€§èƒ½æå‡
- âœ… **æ›´å¥½çš„é•¿æœŸé¢„æµ‹** - è§£ç å™¨å¯ä»¥æŒç»­å…³æ³¨å†å²ä¿¡æ¯
- âœ… **æ›´çµæ´»çš„æ¨¡å¼** - æ¯ä¸ªæœªæ¥æ­¥å­¦ä¹ ç‹¬ç«‹çš„æŸ¥è¯¢ç­–ç•¥
- âœ… **æ›´å¼ºçš„æ³›åŒ–** - Encoder-Decoder æ˜¯æ›´ç»å…¸ã€æ›´æˆç†Ÿçš„æ¶æ„

### 2. å¯è§£é‡Šæ€§
- âœ… **æ³¨æ„åŠ›å¯è§†åŒ–** - å¯ä»¥çœ‹åˆ°"é¢„æµ‹ç¬¬tæ­¥æ—¶å…³æ³¨å†å²çš„å“ªäº›éƒ¨åˆ†"
- âœ… **æŸ¥è¯¢å‘é‡åˆ†æ** - å¯ä»¥åˆ†æä¸åŒæœªæ¥æ­¥çš„æŸ¥è¯¢æ¨¡å¼å·®å¼‚

### 3. æ‰©å±•æ€§
- âœ… **æ˜“äºæ‰©å±•é¢„æµ‹é•¿åº¦** - åªéœ€è°ƒæ•´ `pred_len` å’Œ `future_queries`
- âœ… **æ”¯æŒå¤šæ­¥è§£ç ** - å¯ä»¥æ”¹ä¸ºè‡ªå›å½’è§£ç  (é€æ­¥ç”Ÿæˆ)

---

## ğŸ“ˆ ä¸å…¶ä»–æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | æ¶æ„ | ä¼˜åŠ¿ | åŠ£åŠ¿ |
|------|------|------|------|
| **å•ç¼–ç å™¨ + MLP** | Encoder â†’ Last Step â†’ MLP | ç®€å•å¿«é€Ÿ | ä¿¡æ¯ä¸¢å¤±ï¼Œç¼ºä¹çµæ´»æ€§ |
| **å•ç¼–ç å™¨ + å…¨è¿æ¥** | Encoder â†’ Flatten â†’ FC | ä½¿ç”¨æ‰€æœ‰å†å² | å‚æ•°é‡å¤§ï¼Œè¿‡æ‹Ÿåˆé£é™© |
| **Encoder-Decoder** â­ | Encoder â†’ Decoder (Cross-Attn) | çµæ´»ã€å¼ºå¤§ã€å¯è§£é‡Š | ç¨å¤æ‚ï¼Œè®¡ç®—ç¨æ…¢ |
| **è‡ªå›å½’è§£ç ** | Encoder â†’ Decoder (é€æ­¥ç”Ÿæˆ) | æœ€çµæ´» | é€Ÿåº¦æ…¢ï¼Œè¯¯å·®ç´¯ç§¯ |

---

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨

```python
from basicts.mask.model import AGPSTModel

model = AGPSTModel(
    num_nodes=358,
    dim=40,
    topK=10,
    in_channel=1,
    embed_dim=96,
    num_heads=4,
    mlp_ratio=4,
    dropout=0.1,
    encoder_depth=4,      # ç¼–ç å™¨ 4 å±‚
    decoder_depth=2,      # è§£ç å™¨ 2 å±‚ â­ æ–°å¢
    use_denoising=True,
    denoise_type='conv',
    use_advanced_graph=True,
    graph_heads=4,
    pred_len=12           # é¢„æµ‹ 12 æ­¥
)

# å‰å‘ä¼ æ’­
history = torch.randn(32, 12, 358, 1)  # (B, T, N, C)
prediction = model(history)            # (32, 12, 358, 1)
```

### é…ç½®æ–‡ä»¶ä¿®æ”¹

```yaml
# parameters/PEMS03.yaml

MODEL:
  NAME: ForecastingWithAdaptiveGraph
  ARCH: AGPSTModel
  PARAM:
    num_nodes: 358
    dim: 40
    topK: 10
    in_channel: 1
    embed_dim: 96
    num_heads: 4
    mlp_ratio: 4
    dropout: 0.1
    encoder_depth: 4
    decoder_depth: 2      # â­ æ–°å¢
    use_denoising: true
    denoise_type: "conv"
    use_advanced_graph: true
    graph_heads: 4
    pred_len: 12
```

---

## ğŸ”§ è°ƒè¯•å»ºè®®

### æ£€æŸ¥å„é˜¶æ®µå½¢çŠ¶

```python
# åœ¨ forward() ä¸­æ·»åŠ æ‰“å°
print(f"After encoder: {encoder_output.shape}")  # (B*N, 12, 96)
print(f"Future queries: {queries.shape}")        # (B*N, 12, 96)
print(f"After decoder: {decoder_output.shape}")  # (B*N, 12, 96)
print(f"Final prediction: {prediction.shape}")   # (B, 12, N, 1)
```

### å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡

```python
# ä¿®æ”¹ TransformerDecoder ä¿å­˜æ³¨æ„åŠ›æƒé‡
# ç„¶åç»˜åˆ¶çƒ­åŠ›å›¾æŸ¥çœ‹"æœªæ¥ç¬¬tæ­¥å…³æ³¨å†å²çš„å“ªäº›éƒ¨åˆ†"
import matplotlib.pyplot as plt
import seaborn as sns

# attention_weights: (pred_len, history_len)
sns.heatmap(attention_weights.detach().cpu().numpy(),
            xticklabels=[f'H{i}' for i in range(12)],
            yticklabels=[f'F{i}' for i in range(12)],
            cmap='viridis')
plt.xlabel('History Steps')
plt.ylabel('Future Steps')
plt.title('Cross-Attention Weights')
plt.show()
```

---

## âœ… æ€»ç»“

### æ ¸å¿ƒæ”¹è¿›
1. **ä»å•å‘ç¼–ç åˆ°åŒå‘å»ºæ¨¡** - Encoderå¤„ç†å†å²ï¼ŒDecoderç”Ÿæˆæœªæ¥
2. **ä»å•ç‚¹é¢„æµ‹åˆ°åºåˆ—è§£ç ** - æ¯ä¸ªæœªæ¥æ­¥ç‹¬ç«‹æŸ¥è¯¢å†å²
3. **ä»å›ºå®šæ¨¡å¼åˆ°å¯å­¦ä¹ ç­–ç•¥** - è‡ªåŠ¨å­¦ä¹ æœ€ä¼˜çš„å†å²-æœªæ¥å…³è”

### æŠ€æœ¯äº®ç‚¹
- ğŸŒŸ **å¯å­¦ä¹ çš„æœªæ¥æŸ¥è¯¢å‘é‡** - ä»£è¡¨ä¸åŒé¢„æµ‹æ­¥çš„è¯­ä¹‰
- ğŸŒŸ **äº¤å‰æ³¨æ„åŠ›æœºåˆ¶** - çµæ´»æå–å†å²ä¿¡æ¯
- ğŸŒŸ **ç«¯åˆ°ç«¯è®­ç»ƒ** - æ‰€æœ‰ç»„ä»¶è”åˆä¼˜åŒ–

### é€‚ç”¨åœºæ™¯
- âœ… å¤šæ­¥æ—¶åºé¢„æµ‹ (traffic, energy, weather)
- âœ… éœ€è¦çµæ´»å†å²-æœªæ¥å»ºæ¨¡çš„ä»»åŠ¡
- âœ… éœ€è¦å¯è§£é‡Šæ€§çš„åº”ç”¨åœºæ™¯

**æ–°æ¶æ„å®Œå…¨å…¼å®¹æ—§ä»£ç ï¼Œåªéœ€æ·»åŠ  `decoder_depth` å‚æ•°å³å¯ä½¿ç”¨ï¼** ğŸš€
