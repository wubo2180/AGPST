# ============================================================================
# Alternating Spatio-Temporal Architecture - Phase 1 Optimized
# Dataset: METR-LA (207 nodes)
# Model: AlternatingSTModel
# 
# üéØ Refined Configuration Based on Experimental Insights üéØ
# Phase 2/3 failed (MAE 6.0-6.9), proving Phase 1 architecture is optimal
# This config focuses on hyperparameter tuning rather than architectural changes
# ============================================================================

# --------------------- Basic Information ---------------------
description: 'Phase 1 Optimized with Asymmetric Depth and Better Scheduling'
model_name: 'AlternatingSTModel'
dataset_name: "METR-LA"
dataset_type: "Traffic speed"
mode: train

# --------------------- Device Configuration ---------------------
device: 'cuda'
gpu_num: 1

# --------------------- Dataset Configuration ---------------------
dataset_dir: 'datasets/'
adj_dir: 'datasets/METR-LA/adj_mx.pkl'
input_len: 12
output_len: 12
num_nodes: 207
use_timestamps: False
norm_each_channel: True
rescale: False
memmap: False
local: True

# --------------------- Training Configuration ---------------------
epochs: 150
batch_size: 32
lr: 0.0005  # Proven optimal from diagnostic experiments
weight_decay: 0.0003  # üî• INCREASED from 0.0001 for better regularization
eps: 0.001
evaluation_horizons: 12

# Gradient clipping
train:
    clip_grad_param:
        max_norm: 5.0
    null_val: 0.0

# --------------------- Architecture Configuration ---------------------
# Feature configuration
in_channel: 1
froward_features: [0]
target_features: [0]

# Core architecture parameters
embed_dim: 96
dropout: 0.12  # üî• INCREASED from 0.1 to prevent overfitting
num_heads: 4
mlp_ratio: 4

# üî• OPTIMIZATION 1: Asymmetric Depth
# Rationale: Stage 1 for quick feature extraction, Stage 2 for refinement
temporal_depth_1: 1  # ‚¨áÔ∏è REDUCED: Stage 1 shallow (fast extraction)
spatial_depth_1: 1   # ‚¨áÔ∏è REDUCED
temporal_depth_2: 3  # ‚¨ÜÔ∏è INCREASED: Stage 2 deep (fine-grained modeling)
spatial_depth_2: 3   # ‚¨ÜÔ∏è INCREASED

# Decoder configuration
decoder_depth: 2
num_decoder_queries: 24

# üî• OPTIMIZATION 2: Enhanced Fusion
fusion_type: 'cross_attn'  # CHANGED from 'gated' to 'cross_attn'
                           # Cross-attention provides richer feature interaction

# Advanced features
use_positional_encoding: True
use_layer_norm: True

# --------------------- Denoise Settings ---------------------
use_denoising: False  # Keep disabled for speed and simplicity

# --------------------- Adaptive Graph Learning ---------------------
use_advanced_graph: True
graph_heads: 4
dim: 10
topK: 10

# --------------------- Evaluation Metrics ---------------------
metrics:
    MAE: "masked_mae"
    RMSE: "masked_rmse"
    MAPE: "masked_mape"

# --------------------- Optimization ---------------------
use_amp: False  # Can enable for speed, but may affect stability

# --------------------- Model Saving ---------------------
save_model: True
model_save_path: 'checkpoints/METR-LA_AlternatingST_Optimized/'

# --------------------- Logging Configuration ---------------------
test_mode: False
swanlab_mode: 'online'
log_dir: 'logs/METR-LA_AlternatingST_Optimized/'

# --------------------- Optimization Rationale ---------------------
# Experimental Evidence:
#   Phase 1 (baseline): Initial MAE 5.4 ‚úÖ
#   Phase 2 (all opts): Initial MAE 6.0-6.9 ‚ùå
#   Phase 3 (multi-stage): Initial MAE 6.0-6.9 ‚ùå
#
# Key Insights:
#   1. Simple architecture works best
#   2. 2 stages are sufficient
#   3. Further complexity hurts performance
#
# Optimization Strategy:
#   1. Asymmetric depth: 1-1-3-3 (shallow-deep pattern)
#   2. Cross-attention fusion (richer than gating)
#   3. Enhanced regularization (weight_decay, dropout)
#
# Expected Improvements:
#   Baseline (Phase 1 original): MAE ~4.5 @ 10 epochs
#   Optimized (this config):     MAE ~4.0 @ 10 epochs (10% improvement)
#   Target converged MAE:        < 3.8 @ 150 epochs
#
# If this fails (MAE > 4.5 @ 10 epochs):
#   ‚Üí Revert to original Phase 1 configuration
#   ‚Üí Accept MAE ~4.5 as the architecture's limit
#   ‚Üí Focus on ensemble or post-processing
# ============================================================================
