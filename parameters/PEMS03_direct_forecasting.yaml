# ============ Direct Forecasting Configuration ============ #
# 直接端到端forecasting，不需要预训练，使用自适应图学习

description: 'Direct Forecasting with Adaptive Graph Learning'
model_name: 'AGPST_DirectForecasting'
dataset_name: "PEMS03"
dataset_type: "Traffic speed"
dataset_input_len: 12      # 短期历史：(B, 12, N, C) 格式
dataset_output_len: 12     # 预测长度
seq_len: 12                # 不再使用patch embedding，直接使用12步历史数据

gpu_num: 1
device: 'cuda'
test_mode: False
swanlab_mode: 'online'
mode: train

# 数据路径
dataset_dir: 'datasets/PEMS03/data_in12_out12.pkl'
dataset_index_dir: 'datasets/PEMS03/index_in12_out12.pkl'
scaler_dir: 'datasets/PEMS03/scaler_in12_out12.pkl'
adj_dir: "datasets/PEMS03/adj_mx.pkl"

# 模型保存
save_model: True
model_save_path: 'checkpoints/PEMS03_DirectForecasting/'

# 训练参数
epochs: 100  # 训练轮数
num_nodes: 358        # 节点数 N=358
n_vertex: 358
froward_features: [0]  # 长期历史数据特征索引
target_features: [0]   # 短期历史和预测目标特征索引
lr: 0.001  # 学习率（可以尝试0.001-0.01）
evaluation_horizons: 12
batch_size: 16

# Adaptive Graph 参数
dim: 10  # 节点嵌入维度
topK: 10  # Top-K稀疏化参数

# Embedding 参数（简化版本，不再使用patch embedding）
in_channel: 1
embed_dim: 96  # 时间特征嵌入维度
num_heads: 4  # Transformer的注意力头数
mlp_ratio: 4
dropout: 0.1

# Transformer Encoder 参数
encoder_depth: 4  # Transformer编码器层数

# 已移除的参数（新模型不再使用）:
# patch_size: 不再使用patch embedding
# graph_heads: 简化为单一图学习
# contrastive_weight: 不再使用对比学习

# GraphWaveNet Backend 参数
backend_args: {
    "num_nodes": 358,
    "supports": '',  # 将在代码中填充
    "dropout": 0.3,
    "gcn_bool": True,
    "addaptadj": True,
    "aptinit": null,
    "in_dim": 1,
    "out_dim": 12,
    "residual_channels": 32,
    "dilation_channels": 32,
    "skip_channels": 256,
    "end_channels": 512,
    "kernel_size": 2,
    "blocks": 4,
    "layers": 2
}

# 训练设置
train:
    clip_grad_param:
        max_norm: 5.0
    null_val: 0.0

# Metrics
metrics: {"MAE": "masked_mae", "RMSE": "masked_rmse", "MAPE": "masked_mape"}
