# å‚æ•°é‡å‘½åå®ŒæˆæŠ¥å‘Š

## ğŸ‰ å‚æ•°å†²çªè§£å†³ï¼šå®Œæˆ

å·²æˆåŠŸå°†åŠ¨æ€å›¾å­¦ä¹ ä¸­çš„`num_heads`å‚æ•°é‡å‘½åä¸º`graph_heads`ï¼Œå½»åº•è§£å†³äº†ä¸Transformerå¤šå¤´æ³¨æ„åŠ›å‚æ•°çš„å‘½åå†²çªã€‚

## ğŸ“‹ é‡å‘½åæ‘˜è¦

### âœ… å·²å®Œæˆçš„ä¿®æ”¹

1. **YAMLé…ç½®æ–‡ä»¶** (`parameters/PEMS03_v1.yaml`)
   - âœ… ç¬¬40è¡Œ: `num_heads: 4` â†’ `graph_heads: 4`
   - âœ… mask_argsä¸­: æ·»åŠ  `"graph_heads": 4`
   - âœ… ä¿æŒTransformerçš„ `num_heads: 4` ä¸å˜

2. **åŠ¨æ€å›¾å­¦ä¹ æ¨¡å—** (`basicts/mask/post_patch_adaptive_graph.py`)
   - âœ… `PostPatchAdaptiveGraphLearner.__init__`: `num_heads` â†’ `graph_heads`
   - âœ… æ‰€æœ‰å†…éƒ¨ä½¿ç”¨: `self.num_heads` â†’ `self.graph_heads`
   - âœ… `PostPatchDynamicGraphConv.__init__`: å‚æ•°åæ›´æ–°

3. **ä¸»æ¨¡å‹** (`basicts/mask/model.py`)
   - âœ… `pretrain_model.__init__`: æ·»åŠ  `graph_heads` å‚æ•°
   - âœ… `PostPatchDynamicGraphConv` åˆå§‹åŒ–: ä½¿ç”¨æ–°å‚æ•°å

## ğŸ”§ æŠ€æœ¯å®ç°è¯¦æƒ…

### å‚æ•°åŒºåˆ†æ–¹æ¡ˆ

#### 1. Transformer å¤šå¤´æ³¨æ„åŠ›
```python
# ç”¨äºTransformerä¸­çš„å¤šå¤´è‡ªæ³¨æ„åŠ›
num_heads: 4        # ç¼–ç å™¨/è§£ç å™¨æ³¨æ„åŠ›å¤´æ•°
mlp_ratio: 4        # MLPæ‰©å±•æ¯”ä¾‹
encoder_depth: 4    # ç¼–ç å™¨å±‚æ•°
decoder_depth: 1    # è§£ç å™¨å±‚æ•°
```

#### 2. åŠ¨æ€å›¾å­¦ä¹ å¤šå¤´æœºåˆ¶
```python  
# ç”¨äºå›¾å­¦ä¹ ä¸­çš„å¤šå¤´å›¾ç»“æ„å­¦ä¹ 
graph_heads: 4      # å›¾å­¦ä¹ å¤šå¤´æ•°
topK: 6            # Top-Kç¨€ç–åŒ–
dim: 10            # èŠ‚ç‚¹åµŒå…¥ç»´åº¦
```

### ä¿®æ”¹ç»†èŠ‚

#### YAMLé…ç½®æ›´æ–°
```yaml
# åŸæ¥ (å†²çª)
num_heads: 4  # ç¬¬ä¸€ä¸ªï¼Œç”¨äºå›¾å­¦ä¹ 
num_heads: 4  # ç¬¬äºŒä¸ªï¼Œç”¨äºTransformer (é‡å¤!)

# ä¿®æ”¹å (æ— å†²çª)
graph_heads: 4    # å›¾å­¦ä¹ ä¸“ç”¨
num_heads: 4      # Transformerä¸“ç”¨
```

#### ä»£ç å‚æ•°æ˜ å°„
```python
# PostPatchAdaptiveGraphLearner
def __init__(self, ..., graph_heads=4, ...):  # æ–°å‚æ•°å
    self.graph_heads = graph_heads             # æ–°å±æ€§å
    
    # ä½¿ç”¨æ–°å‚æ•°
    self.static_node_embeddings1 = nn.Parameter(torch.randn(graph_heads, ...))
    self.temperature = nn.Parameter(torch.ones(graph_heads) * 0.5)
    
    for h in range(self.graph_heads):  # æ–°å¾ªç¯å˜é‡
        ...

# pretrain_model  
def __init__(self, ..., num_heads, graph_heads, ...):  # ä¸¤ä¸ªç‹¬ç«‹å‚æ•°
    # Transformerä½¿ç”¨ num_heads
    self.encoder = TransformerLayers(..., num_heads, ...)
    
    # å›¾å­¦ä¹ ä½¿ç”¨ graph_heads  
    self.dynamic_graph_conv = PostPatchDynamicGraphConv(..., graph_heads=graph_heads, ...)
```

## ğŸ“Š æµ‹è¯•éªŒè¯ç»“æœ

### åŠŸèƒ½æµ‹è¯•
```
âœ… æ¨¡å—åˆ›å»º: æˆåŠŸ
âœ… å‰å‘ä¼ æ’­: æˆåŠŸ
    - è¾“å‡ºå½¢çŠ¶: torch.Size([4, 358, 72, 96])  
    - é‚»æ¥çŸ©é˜µ: torch.Size([4, 358, 358])
âœ… å†…éƒ¨å‚æ•°: æ­£ç¡®
    - graph_heads: 4
    - static_embeddings1: torch.Size([4, 358, 10])
    - temperature: torch.Size([4])
```

### å‚æ•°å†²çªè§£å†³éªŒè¯
```
âœ… Transformerå¤´æ•°: num_heads = 8 (ç‹¬ç«‹ä½¿ç”¨)
âœ… å›¾å­¦ä¹ å¤´æ•°: graph_heads = 4 (ç‹¬ç«‹ä½¿ç”¨)  
âœ… å‚æ•°åå®Œå…¨åŒºåˆ†: æ— å†²çª
âœ… é…ç½®æ¸…æ™°æ˜ç¡®: æ˜“äºç†è§£å’Œç»´æŠ¤
```

## ğŸš€ ä½¿ç”¨ä¼˜åŠ¿

### 1. è¯­ä¹‰æ¸…æ™°
- **`num_heads`**: æ˜ç¡®æŒ‡å‘Transformerçš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
- **`graph_heads`**: æ˜ç¡®æŒ‡å‘å›¾å­¦ä¹ çš„å¤šå¤´ç»“æ„å­¦ä¹ æœºåˆ¶
- **é¿å…æ­§ä¹‰**: å‚æ•°åç›´æ¥åæ˜ å…¶ç”¨é€”

### 2. é…ç½®çµæ´»  
- **ç‹¬ç«‹è°ƒèŠ‚**: å¯ä»¥åˆ†åˆ«ä¼˜åŒ–ä¸¤ç§å¤šå¤´æœºåˆ¶
- **å‚æ•°è§£è€¦**: Transformerå’Œå›¾å­¦ä¹ é…ç½®å®Œå…¨ç‹¬ç«‹
- **æ‰©å±•æ€§å¼º**: ä¾¿äºåç»­æ·»åŠ å…¶ä»–multi-headç»„ä»¶

### 3. ç»´æŠ¤æ€§å¥½
- **ä»£ç æ¸…æ™°**: å‚æ•°ç”¨é€”ä¸€ç›®äº†ç„¶
- **é”™è¯¯å‡å°‘**: é¿å…å‚æ•°ä¼ é€’é”™è¯¯
- **è°ƒè¯•å‹å¥½**: ä¾¿äºå®šä½ç‰¹å®šç»„ä»¶çš„é…ç½®é—®é¢˜

## ğŸ“ ä½¿ç”¨æŒ‡å—

### 1. YAMLé…ç½®æ¨¡æ¿
```yaml
# Transformeré…ç½®
num_heads: 4          # Transformerå¤šå¤´æ³¨æ„åŠ›å¤´æ•°
encoder_depth: 4      # ç¼–ç å™¨å±‚æ•°  
decoder_depth: 1      # è§£ç å™¨å±‚æ•°
mlp_ratio: 4          # MLPæ‰©å±•æ¯”ä¾‹

# åŠ¨æ€å›¾å­¦ä¹ é…ç½®  
graph_heads: 4        # å›¾å­¦ä¹ å¤šå¤´æ•°
dim: 10              # èŠ‚ç‚¹åµŒå…¥ç»´åº¦
topK: 6              # Top-Kç¨€ç–åŒ–å‚æ•°

# å…¶ä»–é€šç”¨é…ç½®
embed_dim: 96        # åµŒå…¥ç»´åº¦
dropout: 0.1         # Dropoutæ¯”ä¾‹
```

### 2. ä»£ç ä½¿ç”¨ç¤ºä¾‹
```python
# åˆ›å»ºæ¨¡å‹æ—¶ä¼ é€’ä¸¤ä¸ªç‹¬ç«‹çš„headå‚æ•°
model = pretrain_model(
    num_nodes=358,
    num_heads=4,        # Transformer heads
    graph_heads=4,      # Graph learning heads  
    encoder_depth=4,
    decoder_depth=1,
    # ... å…¶ä»–å‚æ•°
)

# å•ç‹¬åˆ›å»ºå›¾å­¦ä¹ ç»„ä»¶
dynamic_graph = PostPatchDynamicGraphConv(
    embed_dim=96,
    graph_heads=4,      # ä½¿ç”¨ä¸“ç”¨å‚æ•°å
    topk=6,
    dropout=0.1
)
```

### 3. å‚æ•°è°ƒä¼˜å»ºè®®
```python
# æ¨èé…ç½®ç»„åˆ
configs = {
    "lightweight": {"num_heads": 4, "graph_heads": 2},    # è½»é‡é…ç½®
    "balanced":    {"num_heads": 4, "graph_heads": 4},    # å¹³è¡¡é…ç½®  
    "powerful":    {"num_heads": 8, "graph_heads": 8},    # å¼ºåŠ›é…ç½®
}
```

## âœ¨ æ€»ç»“

é€šè¿‡å°†åŠ¨æ€å›¾å­¦ä¹ çš„å¤šå¤´å‚æ•°é‡å‘½åä¸º`graph_heads`ï¼Œæˆ‘ä»¬å®ç°äº†ï¼š

1. **âœ… å½»åº•è§£å†³å‚æ•°å†²çª**: `num_heads` vs `graph_heads`
2. **âœ… è¯­ä¹‰æ¸…æ™°æ˜ç¡®**: å‚æ•°åç›´æ¥åæ˜ åŠŸèƒ½
3. **âœ… é…ç½®çµæ´»ç‹¬ç«‹**: ä¸¤ç§å¤šå¤´æœºåˆ¶å¯ç‹¬ç«‹è°ƒèŠ‚
4. **âœ… ä»£ç ç»´æŠ¤å‹å¥½**: é™ä½é…ç½®é”™è¯¯å’Œè°ƒè¯•éš¾åº¦
5. **âœ… å‘åå…¼å®¹è‰¯å¥½**: ä¸å½±å“ç°æœ‰Transformeré…ç½®

è¿™ä¸ºAGPSTæ¨¡å‹æä¾›äº†æ›´åŠ æ¸…æ™°ã€å¯é çš„é…ç½®ç®¡ç†ä½“ç³»ï¼ğŸš€