# AGPST ç®€åŒ–ç‰ˆä½¿ç”¨æŒ‡å—

## ğŸ“Œ ä¸»è¦å˜åŒ–

### âœ… ç®€åŒ–åçš„æ¶æ„
- **ç§»é™¤**: é¢„è®­ç»ƒï¼ˆpretrainï¼‰å’Œå¾®è°ƒï¼ˆfinetuneï¼‰ä¸¤é˜¶æ®µè®­ç»ƒ
- **æ–°å¢**: ç«¯åˆ°ç«¯è®­ç»ƒï¼ˆtrainï¼‰ï¼Œè‡ªé€‚åº”å›¾å­¦ä¹ ç›´æ¥é›†æˆåœ¨forecastingä¸­
- **æ¨¡å¼**: åªæœ‰ train / val / test ä¸‰ä¸ªé˜¶æ®µ

### ğŸ”§ ä»£ç æ¸…ç†
- ç§»é™¤æ‰€æœ‰è°ƒè¯•æ‰“å°è¯­å¥
- ç§»é™¤ pretrain_model å’Œ finetune_model ç±»
- ç§»é™¤ pretrain()ã€finetune()ã€preTrain_test() å‡½æ•°
- ç®€åŒ–ä¸ºå•ä¸€ train() å‡½æ•°

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç›´æ¥è¿è¡Œï¼ˆæ¨èï¼‰

```bash
# Windows
run_direct_forecasting.bat

# Linux/Mac
bash run_experiments_swanlab.sh
```

### 2. å‘½ä»¤è¡Œè¿è¡Œ

```bash
python main.py \
    --config parameters/PEMS03_direct_forecasting.yaml \
    --mode train \
    --device cuda \
    --swanlab_mode online
```

### 3. å‚æ•°è¯´æ˜

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--config` | `parameters/PEMS03_direct_forecasting.yaml` | é…ç½®æ–‡ä»¶è·¯å¾„ |
| `--mode` | `train` | è®­ç»ƒæ¨¡å¼ï¼ˆä»…æ”¯æŒ trainï¼‰ |
| `--device` | `cuda` | è®¾å¤‡ï¼ˆcuda/cpuï¼‰ |
| `--swanlab_mode` | `disabled` | SwanLabæ¨¡å¼ï¼ˆonline/disabledï¼‰ |
| `--test_mode` | `0` | æµ‹è¯•æ¨¡å¼ï¼ˆ1=åªå¤„ç†ä¸€ä¸ªbatchï¼‰ |

---

## ğŸ“‚ æ ¸å¿ƒæ–‡ä»¶

### æ¨¡å‹æ–‡ä»¶
```
basicts/mask/
â”œâ”€â”€ forecasting_with_adaptive_graph.py  # ä¸»æ¨¡å‹ï¼ˆç«¯åˆ°ç«¯è®­ç»ƒï¼‰
â”œâ”€â”€ post_patch_adaptive_graph.py        # è‡ªé€‚åº”å›¾å­¦ä¹ æ¨¡å—
â”œâ”€â”€ patch.py                             # Patch embedding
â””â”€â”€ transformer_layers.py                # Transformerç¼–ç å™¨
```

### é…ç½®æ–‡ä»¶
```
parameters/
â””â”€â”€ PEMS03_direct_forecasting.yaml      # ç›´æ¥forecastingé…ç½®
```

### è®­ç»ƒå…¥å£
```
main.py                                  # ä¸»ç¨‹åºï¼ˆå·²ç®€åŒ–ï¼‰
run_direct_forecasting.bat              # Windowså¯åŠ¨è„šæœ¬
```

---

## ğŸ¯ æ¨¡å‹æ¶æ„

```
è¾“å…¥æ•°æ® (B, T, N, C)
    â”œâ”€ short_data:  (B, 12, 358, 1)   # çŸ­æœŸå†å²
    â””â”€ long_data:   (B, 864, 358, 1)  # é•¿æœŸå†å²
           â†“
    [1] PatchEmbedding
        long_data â†’ patches (B, N, 72, 96)
           â†“
    [2] PostPatchDynamicGraphConv
        â”œâ”€ Multi-scale adaptive graph learning
        â”œâ”€ Local graph (node-level)
        â”œâ”€ Global graph (patch-level)
        â”œâ”€ Adaptive fusion
        â””â”€ InfoNCE contrastive loss
           â†“
    [3] Transformer Encoder (4 layers)
        Temporal modeling
           â†“
    [4] GraphWaveNet Backend
        Final prediction
           â†“
è¾“å‡ºé¢„æµ‹ (B, 12, 358, 1)
```

---

## âš™ï¸ å…³é”®å‚æ•°

### æ•°æ®æ ¼å¼
```yaml
dataset_input_len: 12      # çŸ­æœŸå†å²é•¿åº¦
dataset_output_len: 12     # é¢„æµ‹é•¿åº¦
seq_len: 864               # é•¿æœŸå†å²é•¿åº¦
num_nodes: 358             # èŠ‚ç‚¹æ•°
```

### è®­ç»ƒå‚æ•°
```yaml
epochs: 100                # è®­ç»ƒè½®æ•°
batch_size: 16             # æ‰¹æ¬¡å¤§å°
lr: 0.001                  # å­¦ä¹ ç‡
```

### æ¨¡å‹å‚æ•°
```yaml
patch_size: 12             # 864/12 = 72ä¸ªpatches
embed_dim: 96              # Patch embeddingç»´åº¦
encoder_depth: 4           # Transformerå±‚æ•°
topK: 10                   # å›¾ç¨€ç–åŒ–Top-K
graph_heads: 4             # å›¾å­¦ä¹ å¤šå¤´æ•°é‡
contrastive_weight: 0.05   # å¯¹æ¯”å­¦ä¹ æƒé‡
```

---

## ğŸ“Š è®­ç»ƒæµç¨‹

### 1. æ•°æ®åŠ è½½
```python
train_dataset = ForecastingDataset(...)
val_dataset = ForecastingDataset(...)
test_dataset = ForecastingDataset(...)
```

### 2. æ¨¡å‹è®­ç»ƒ
```python
for epoch in range(epochs):
    # è®­ç»ƒé˜¶æ®µ
    model.train()
    for batch in train_loader:
        preds = model(short_data, long_data, ...)
        loss = MAE(preds, labels)
        loss += contrastive_weight * contrastive_loss
        loss.backward()
        optimizer.step()
    
    # éªŒè¯é˜¶æ®µ
    val_loss = validate(val_loader, model, ...)
    
    # æµ‹è¯•é˜¶æ®µ
    test(test_loader, model, ...)
```

### 3. ä¿å­˜æœ€ä½³æ¨¡å‹
```python
if val_loss < best_val_loss:
    torch.save(model.state_dict(), "best_model.pt")
```

---

## ğŸ“ˆ ç›‘æ§æŒ‡æ ‡

### SwanLab è®°å½•
```python
swanlab.log({
    "train/loss": train_loss,
    "train/contrastive_loss": contrastive_loss,
    "train/lr": learning_rate,
    "val/MAE": val_mae,
    "val/RMSE": val_rmse,
    "val/MAPE": val_mape,
    "test/MAE": test_mae,
    "test/RMSE": test_rmse,
    "test/MAPE": test_mape
})
```

### æ§åˆ¶å°è¾“å‡º
```
============ Epoch 0/100 ============
ğŸ“Š Data Shape Verification:
  history_data (short-term):     torch.Size([16, 12, 358, 1])
  long_history_data (long-term): torch.Size([16, 864, 358, 1])
  future_data (labels):          torch.Size([16, 12, 358, 1])
  predictions (model output):    torch.Size([16, 12, 358, 1])
============================================
Epoch 0 - Train Loss: 3.5421, Contrastive Loss: 0.1234
============ Validation ============
Val MAE: 2.8765, Val RMSE: 4.3210, Val MAPE: 0.1234
âœ… Best model saved with val loss: 2.8765
============ Test ============
Test MAE: 2.9123, Test RMSE: 4.3876, Test MAPE: 0.1289
```

---

## ğŸ” è°ƒè¯•æ¨¡å¼

### æµ‹è¯•æ¨¡å¼ï¼ˆåªå¤„ç†ä¸€ä¸ªbatchï¼‰
```bash
python main.py \
    --config parameters/PEMS03_direct_forecasting.yaml \
    --mode train \
    --test_mode 1
```

### æ•°æ®æ ¼å¼éªŒè¯
- ç¬¬ä¸€ä¸ªepochçš„ç¬¬ä¸€ä¸ªbatchä¼šè‡ªåŠ¨æ‰“å°æ‰€æœ‰æ•°æ®å½¢çŠ¶
- ç¡®ä¿æ‰€æœ‰tensoréƒ½æ˜¯ (B, T, N, C) æ ¼å¼

---

## â“ å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•ä¿®æ”¹æ•°æ®é›†ï¼Ÿ
A: ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„ï¼š
```yaml
dataset_dir: 'datasets/PEMS04/data_in12_out12.pkl'
dataset_index_dir: 'datasets/PEMS04/index_in12_out12.pkl'
scaler_dir: 'datasets/PEMS04/scaler_in12_out12.pkl'
adj_dir: "datasets/PEMS04/adj_mx.pkl"
num_nodes: 307  # PEMS04æœ‰307ä¸ªèŠ‚ç‚¹
```

### Q2: å¦‚ä½•è°ƒæ•´patchå¤§å°ï¼Ÿ
A: ä¿®æ”¹ `patch_size` å‚æ•°ï¼š
```yaml
patch_size: 24  # 864/24 = 36ä¸ªpatches
```
æ³¨æ„ï¼š864 å¿…é¡»èƒ½è¢« patch_size æ•´é™¤

### Q3: å¦‚ä½•å…³é—­SwanLabè®°å½•ï¼Ÿ
A: ä½¿ç”¨ `--swanlab_mode disabled`ï¼š
```bash
python main.py --swanlab_mode disabled
```

### Q4: å¦‚ä½•è°ƒæ•´å¯¹æ¯”å­¦ä¹ æƒé‡ï¼Ÿ
A: ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼š
```yaml
contrastive_weight: 0.1  # èŒƒå›´ï¼š0.01-0.5
```

### Q5: è®­ç»ƒé€Ÿåº¦æ…¢æ€ä¹ˆåŠï¼Ÿ
A: 
1. å‡å°batch_size
2. å‡å°‘encoder_depth
3. å‡å°embed_dim
4. ä½¿ç”¨æ›´å°‘çš„num_heads

---

## ğŸ“ ç‰ˆæœ¬å†å²

### v2.0 (å½“å‰ç‰ˆæœ¬)
- âœ… ç§»é™¤é¢„è®­ç»ƒæœºåˆ¶
- âœ… ç«¯åˆ°ç«¯è®­ç»ƒ
- âœ… ç®€åŒ–ä»£ç ç»“æ„
- âœ… ç§»é™¤æ‰€æœ‰è°ƒè¯•è¯­å¥
- âœ… ç»Ÿä¸€æ•°æ®æ ¼å¼ä¸º (B, T, N, C)

### v1.0 (æ—§ç‰ˆæœ¬)
- âŒ é¢„è®­ç»ƒ + å¾®è°ƒä¸¤é˜¶æ®µ
- âŒ å¤æ‚çš„æ¨¡å‹åˆ‡æ¢é€»è¾‘
- âŒ å¤§é‡è°ƒè¯•è¾“å‡º

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [å®Œæ•´README](DIRECT_FORECASTING_README.md)
- [æ•°æ®æ ¼å¼æŒ‡å—](DATA_FORMAT_GUIDE.md)
- [è‡ªé€‚åº”å›¾æŒ‡å—](ADAPTIVE_GRAPH_GUIDE.md)
- [å¿«é€Ÿå¼€å§‹](ADAPTIVE_GRAPH_QUICKSTART.md)

---

## ğŸ’¡ æœ€ä½³å®è·µ

1. **é¦–æ¬¡è¿è¡Œ**: ä½¿ç”¨é»˜è®¤é…ç½®æµ‹è¯•
2. **æ•°æ®éªŒè¯**: æ£€æŸ¥ç¬¬ä¸€ä¸ªepochçš„æ•°æ®å½¢çŠ¶è¾“å‡º
3. **æ€§èƒ½è°ƒä¼˜**: æ ¹æ®éªŒè¯é›†è°ƒæ•´å­¦ä¹ ç‡å’Œæƒé‡
4. **æ¨¡å‹ä¿å­˜**: å¯ç”¨ `save_model: True` ä¿å­˜æœ€ä½³æ¨¡å‹
5. **ç›‘æ§è®­ç»ƒ**: ä½¿ç”¨ SwanLab åœ¨çº¿ç›‘æ§å®éªŒ

---

**Last Updated**: 2024
**Author**: AGPST Team
